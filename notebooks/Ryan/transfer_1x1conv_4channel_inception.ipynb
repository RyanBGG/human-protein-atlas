{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "import keras\n",
    "import warnings\n",
    "from keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 299\n",
    "SEED = 777\n",
    "THRESHOLD = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info\n",
    "DIR = '../input/'\n",
    "data = pd.read_csv('../input/train.csv')\n",
    "\n",
    "# train_dataset_info = []\n",
    "# for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "#     train_dataset_info.append({\n",
    "#         'path':os.path.join(path_to_train, name),\n",
    "#         'labels':np.array([int(label) for label in labels])})\n",
    "# train_dataset_info = np.array(train_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainDataset():\n",
    "    \n",
    "    path_to_train = DIR + '/train/'\n",
    "    data = pd.read_csv(DIR + '/train.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name, lbl in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "        y = np.zeros(28)\n",
    "        for key in lbl:\n",
    "            y[int(key)] = 1\n",
    "        paths.append(os.path.join(path_to_train, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)\n",
    "\n",
    "def getTestDataset():\n",
    "    \n",
    "    path_to_test = DIR + '/test/'\n",
    "    data = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name in data['Id']:\n",
    "        y = np.ones(28)\n",
    "        paths.append(os.path.join(path_to_test, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)\n",
    "paths, labels = getTrainDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credits: https://github.com/keras-team/keras/blob/master/keras/utils/data_utils.py#L302\n",
    "# credits: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "from random import randint\n",
    "class ProteinDataGenerator(keras.utils.Sequence):\n",
    "            \n",
    "    def __init__(self, paths, labels, batch_size, shape, channels = [], shuffle = False, use_cache = False, augmentor = False):\n",
    "        self.paths, self.labels = paths, labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shape = shape\n",
    "        self.shuffle = shuffle\n",
    "        self.use_cache = use_cache\n",
    "        self.channels = channels\n",
    "        self.augmentor = augmentor\n",
    "        self.clahe = cv2.createCLAHE()\n",
    "        if use_cache == True:\n",
    "            self.cache = np.zeros((paths.shape[0], shape[0], shape[1], len(channels)))\n",
    "            self.is_cached = np.zeros((paths.shape[0]))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "\n",
    "        paths = self.paths[indexes]\n",
    "        X = np.zeros((paths.shape[0], self.shape[0], self.shape[1], self.shape[2]))\n",
    "        # Generate data\n",
    "        if self.use_cache == True:\n",
    "            X = self.cache[indexes]\n",
    "            for i, path in enumerate(paths[np.where(self.is_cached[indexes] == 0)]):\n",
    "                image = self.__load_image(path)\n",
    "                self.is_cached[indexes[i]] = 1\n",
    "                self.cache[indexes[i]] = image\n",
    "                X[i] = image\n",
    "        else:\n",
    "            for i, path in enumerate(paths):\n",
    "                X[i] = self.__load_image(path)\n",
    "        if self.augmentor == True:\n",
    "            for i, item in enumerate(X):\n",
    "                X[i] = self.augment(item)\n",
    "        y = self.labels[indexes]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        # Updates indexes after each epoch\n",
    "        self.indexes = np.arange(len(self.paths))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "        for item in (self[i] for i in range(len(self))):\n",
    "            yield item\n",
    "            \n",
    "    def __load_image(self, path):\n",
    "        images = []\n",
    "        for channel in self.channels:\n",
    "            im = np.array(Image.open(path + '_' + channel + '.png'))\n",
    "            \n",
    "#             im = clahe.apply(im)\n",
    "            images.append(im)\n",
    "            \n",
    "        if len(self.channels) >= 2:\n",
    "            im = np.stack((\n",
    "                images\n",
    "            ), -1)\n",
    "            im = cv2.resize(im, (SIZE,SIZE))\n",
    "            im = np.divide(im, 255)\n",
    "\n",
    "        else:\n",
    "            im = images[0]\n",
    "            im = cv2.resize(im, (SIZE,SIZE))\n",
    "            im = np.divide(im, 255)\n",
    "            im = np.expand_dims(im, 2)\n",
    "        return im\n",
    "    def augment(self, image):\n",
    "        if randint(0,1) == 1:\n",
    "            augment_img = iaa.Sequential([\n",
    "                iaa.OneOf([\n",
    "                    iaa.Fliplr(0.5), # horizontal flips\n",
    "                    iaa.Flipud(0.5), # horizontal flips\n",
    "                    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "                    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "                    # But we only blur about 50% of all images.\n",
    "                    iaa.Sometimes(0.5,\n",
    "                        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "                    ),\n",
    "                    # Make some images brighter and some darker.\n",
    "                    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "                    # which can end up changing the color of the images.\n",
    "                    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "                    # Apply affine transformations to each image.\n",
    "                    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "                    iaa.Affine(\n",
    "                        scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)},\n",
    "                        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
    "                        rotate=(-180, 180),\n",
    "                        shear=(-4, 4)\n",
    "                    )\n",
    "                ])], random_order=True)\n",
    "\n",
    "\n",
    "            image_aug = augment_img.augment_image(image)\n",
    "            return image_aug\n",
    "        else:\n",
    "            return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPE = (299, 299, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n"
     ]
    }
   ],
   "source": [
    "channels = [\"red\"]\n",
    "for path in paths[0:10]:\n",
    "    images = []\n",
    "    for channel in channels:\n",
    "        print(channel)\n",
    "        images.append(np.array(Image.open(path + '_' + channel + '.png')))\n",
    "\n",
    "    if len(channels) >= 2:\n",
    "        im = np.stack((\n",
    "            images\n",
    "        ), -1)\n",
    "        im = cv2.resize(im, (SIZE,SIZE))\n",
    "        im = np.divide(im, 255)\n",
    "        \n",
    "    else:\n",
    "        im = images[0]\n",
    "        im = cv2.resize(im, (SIZE,SIZE))\n",
    "        im = np.divide(im, 255)\n",
    "        im = np.expand_dims(im, 2)\n",
    "    print(im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class data_generator:\n",
    "    \n",
    "#     def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "#         assert shape[2] == 3\n",
    "#         while True:\n",
    "#             dataset_info = shuffle(dataset_info)\n",
    "#             for start in range(0, len(dataset_info), batch_size):\n",
    "#                 end = min(start + batch_size, len(dataset_info))\n",
    "#                 batch_images = []\n",
    "#                 X_train_batch = dataset_info[start:end]\n",
    "#                 batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "#                 for i in range(len(X_train_batch)):\n",
    "#                     image = data_generator.load_image(\n",
    "#                         X_train_batch[i]['path'], shape)   \n",
    "#                     if augument:\n",
    "#                         image = data_generator.augment(image)\n",
    "#                     batch_images.append(image/255.)\n",
    "#                     batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "#                 yield np.array(batch_images, np.float32), batch_labels\n",
    "\n",
    "#     def load_image(path, shape):\n",
    "#         image_red_ch = Image.open(path+'_red.png')\n",
    "#         image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "#         image_green_ch = Image.open(path+'_green.png')\n",
    "#         image_blue_ch = Image.open(path+'_blue.png')\n",
    "#         image = np.stack((\n",
    "#         np.array(image_red_ch), \n",
    "#         np.array(image_green_ch), \n",
    "#         np.array(image_blue_ch)), -1)\n",
    "#         image = cv2.resize(image, (shape[0], shape[1]))\n",
    "#         return image\n",
    "\n",
    "#     def augment(image):\n",
    "#         augment_img = iaa.Sequential([\n",
    "#             iaa.OneOf([\n",
    "#                 iaa.Affine(rotate=0),\n",
    "#                 iaa.Affine(rotate=90),\n",
    "#                 iaa.Affine(rotate=180),\n",
    "#                 iaa.Affine(rotate=270),\n",
    "#                 iaa.Fliplr(0.5),\n",
    "#                 iaa.Flipud(0.5),\n",
    "#             ])], random_order=True)\n",
    "\n",
    "#         image_aug = augment_img.augment_image(image)\n",
    "#         return image_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D, MaxPooling2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    #y_pred = K.round(y_pred)\n",
    "    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    #y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1-K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# f = h5py.File(\"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\", 'r')\n",
    "# dset = f[\"conv2d_1\"]\n",
    "# list(dset.keys())\n",
    "# group = dset[\"conv2d_1\"]\n",
    "# list(group.keys())\n",
    "# first_lay = group['kernel:0'].value\n",
    "# print(first_lay.shape)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros = np.zeros(shape = (3,3,1,32))\n",
    "# print(zeros.shape)\n",
    "# first_lay1 = np.concatenate([zeros, first_lay], axis = 2)\n",
    "# first_lay.shape\n",
    "# f = h5py.File(\"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\", 'r+')\n",
    "# dset = f[\"conv2d_1\"]\n",
    "# list(dset.keys())\n",
    "# group = dset[\"conv2d_1\"]\n",
    "# list(group.keys())\n",
    "# del group['kernel:0']\n",
    "# group['kernel:0'] = first_lay1\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.clear_session()\n",
    "# # -*- coding: utf-8 -*-\n",
    "# \"\"\"Inception V3 model for Keras.\n",
    "# Note that the input image format for this model is different than for\n",
    "# the VGG16 and ResNet models (299x299 instead of 224x224),\n",
    "# and that the input preprocessing function is also different (same as Xception).\n",
    "# # Reference\n",
    "# - [Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567)\n",
    "# \"\"\"\n",
    "# from __future__ import print_function\n",
    "# from __future__ import absolute_import\n",
    "\n",
    "# import warnings\n",
    "# import numpy as np\n",
    "\n",
    "# from keras.models import Model\n",
    "# from keras import layers\n",
    "# from keras.layers import Activation\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Input\n",
    "# from keras.layers import BatchNormalization\n",
    "# from keras.layers import Conv2D\n",
    "# from keras.layers import MaxPooling2D\n",
    "# from keras.layers import AveragePooling2D\n",
    "# from keras.layers import GlobalAveragePooling2D\n",
    "# from keras.layers import GlobalMaxPooling2D\n",
    "# from keras.engine.topology import get_source_inputs\n",
    "# from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "# from keras.utils.data_utils import get_file\n",
    "# from keras import backend as K\n",
    "# from keras.applications.imagenet_utils import decode_predictions\n",
    "# from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "# from keras.preprocessing import image\n",
    "\n",
    "\n",
    "# WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "# WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "\n",
    "# def conv2d_bn(x,\n",
    "#               filters,\n",
    "#               num_row,\n",
    "#               num_col,\n",
    "#               padding='same',\n",
    "#               strides=(1, 1),\n",
    "#               name=None):\n",
    "#     \"\"\"Utility function to apply conv + BN.\n",
    "#     Arguments:\n",
    "#         x: input tensor.\n",
    "#         filters: filters in `Conv2D`.\n",
    "#         num_row: height of the convolution kernel.\n",
    "#         num_col: width of the convolution kernel.\n",
    "#         padding: padding mode in `Conv2D`.\n",
    "#         strides: strides in `Conv2D`.\n",
    "#         name: name of the ops; will become `name + '_conv'`\n",
    "#             for the convolution and `name + '_bn'` for the\n",
    "#             batch norm layer.\n",
    "#     Returns:\n",
    "#         Output tensor after applying `Conv2D` and `BatchNormalization`.\n",
    "#     \"\"\"\n",
    "#     if name is not None:\n",
    "#         bn_name = name + '_bn'\n",
    "#         conv_name = name + '_conv'\n",
    "#     else:\n",
    "#         bn_name = None\n",
    "#         conv_name = None\n",
    "#     if K.image_data_format() == 'channels_first':\n",
    "#         bn_axis = 1\n",
    "#     else:\n",
    "#         bn_axis = 3\n",
    "#     x = Conv2D(\n",
    "#         filters, (num_row, num_col),\n",
    "#         strides=strides,\n",
    "#         padding=padding,\n",
    "#         use_bias=False,\n",
    "#         name=conv_name)(x)\n",
    "#     x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n",
    "#     x = Activation('relu', name=name)(x)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def InceptionV3(include_top=True,\n",
    "#                 weights='imagenet',\n",
    "#                 input_tensor=None,\n",
    "#                 input_shape=None,\n",
    "#                 pooling=None,\n",
    "#                 classes=1000):\n",
    "#     \"\"\"Instantiates the Inception v3 architecture.\n",
    "#     Optionally loads weights pre-trained\n",
    "#     on ImageNet. Note that when using TensorFlow,\n",
    "#     for best performance you should set\n",
    "#     `image_data_format=\"channels_last\"` in your Keras config\n",
    "#     at ~/.keras/keras.json.\n",
    "#     The model and the weights are compatible with both\n",
    "#     TensorFlow and Theano. The data format\n",
    "#     convention used by the model is the one\n",
    "#     specified in your Keras config file.\n",
    "#     Note that the default input image size for this model is 299x299.\n",
    "#     Arguments:\n",
    "#         include_top: whether to include the fully-connected\n",
    "#             layer at the top of the network.\n",
    "#         weights: one of `None` (random initialization)\n",
    "#             or \"imagenet\" (pre-training on ImageNet).\n",
    "#         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "#             to use as image input for the model.\n",
    "#         input_shape: optional shape tuple, only to be specified\n",
    "#             if `include_top` is False (otherwise the input shape\n",
    "#             has to be `(299, 299, 3)` (with `channels_last` data format)\n",
    "#             or `(3, 299, 299)` (with `channels_first` data format).\n",
    "#             It should have exactly 3 inputs channels,\n",
    "#             and width and height should be no smaller than 139.\n",
    "#             E.g. `(150, 150, 3)` would be one valid value.\n",
    "#         pooling: Optional pooling mode for feature extraction\n",
    "#             when `include_top` is `False`.\n",
    "#             - `None` means that the output of the model will be\n",
    "#                 the 4D tensor output of the\n",
    "#                 last convolutional layer.\n",
    "#             - `avg` means that global average pooling\n",
    "#                 will be applied to the output of the\n",
    "#                 last convolutional layer, and thus\n",
    "#                 the output of the model will be a 2D tensor.\n",
    "#             - `max` means that global max pooling will\n",
    "#                 be applied.\n",
    "#         classes: optional number of classes to classify images\n",
    "#             into, only to be specified if `include_top` is True, and\n",
    "#             if no `weights` argument is specified.\n",
    "#     Returns:\n",
    "#         A Keras model instance.\n",
    "#     Raises:\n",
    "#         ValueError: in case of invalid argument for `weights`,\n",
    "#             or invalid input shape.\n",
    "#     \"\"\"\n",
    "#     if weights not in {'imagenet', None}:\n",
    "#         raise ValueError('The `weights` argument should be either '\n",
    "#                          '`None` (random initialization) or `imagenet` '\n",
    "#                          '(pre-training on ImageNet).')\n",
    "\n",
    "#     if weights == 'imagenet' and include_top and classes != 1000:\n",
    "#         raise ValueError('If using `weights` as imagenet with `include_top`'\n",
    "#                          ' as true, `classes` should be 1000')\n",
    "\n",
    "#     # Determine proper input shape\n",
    "#     input_shape = (299,299,4)\n",
    "#     if input_tensor is None:\n",
    "#         img_input = Input(shape=input_shape)\n",
    "#     else:\n",
    "#         img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "\n",
    "#     if K.image_data_format() == 'channels_first':\n",
    "#         channel_axis = 1\n",
    "#     else:\n",
    "#         channel_axis = 3\n",
    "\n",
    "#     x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding='valid')\n",
    "#     x = conv2d_bn(x, 32, 3, 3, padding='valid')\n",
    "#     x = conv2d_bn(x, 64, 3, 3)\n",
    "#     x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "#     x = conv2d_bn(x, 80, 1, 1, padding='valid')\n",
    "#     x = conv2d_bn(x, 192, 3, 3, padding='valid')\n",
    "#     x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "#     # mixed 0, 1, 2: 35 x 35 x 256\n",
    "#     branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "#     branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "#     branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "#     branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "#     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "#     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "#     branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "#     branch_pool = conv2d_bn(branch_pool, 32, 1, 1)\n",
    "#     x = layers.concatenate(\n",
    "#         [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "#         axis=channel_axis,\n",
    "#         name='mixed0')\n",
    "\n",
    "#     # mixed 1: 35 x 35 x 256\n",
    "#     branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "#     branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "#     branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "#     branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "#     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "#     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "#     branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "#     branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n",
    "#     x = layers.concatenate(\n",
    "#         [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "#         axis=channel_axis,\n",
    "#         name='mixed1')\n",
    "\n",
    "#     # mixed 2: 35 x 35 x 256\n",
    "#     branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "#     branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "#     branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "#     branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "#     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "#     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "#     branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "#     branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n",
    "#     x = layers.concatenate(\n",
    "#         [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "#         axis=channel_axis,\n",
    "#         name='mixed2')\n",
    "\n",
    "#     # mixed 3: 17 x 17 x 768\n",
    "#     branch3x3 = conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "#     branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "#     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "#     branch3x3dbl = conv2d_bn(\n",
    "#         branch3x3dbl, 96, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "#     branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "#     x = layers.concatenate(\n",
    "#         [branch3x3, branch3x3dbl, branch_pool], axis=channel_axis, name='mixed3')\n",
    "\n",
    "#     # mixed 4: 17 x 17 x 768\n",
    "#     branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "#     branch7x7 = conv2d_bn(x, 128, 1, 1)\n",
    "#     branch7x7 = conv2d_bn(branch7x7, 128, 1, 7)\n",
    "#     branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "#     branch7x7dbl = conv2d_bn(x, 128, 1, 1)\n",
    "#     branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "#     branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 1, 7)\n",
    "#     branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "#     branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "#     branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "#     branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "#     x = layers.concatenate(\n",
    "#         [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "#         axis=channel_axis,\n",
    "#         name='mixed4')\n",
    "\n",
    "#     # mixed 5, 6: 17 x 17 x 768\n",
    "#     for i in range(2):\n",
    "#         branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "#         branch7x7 = conv2d_bn(x, 160, 1, 1)\n",
    "#         branch7x7 = conv2d_bn(branch7x7, 160, 1, 7)\n",
    "#         branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "#         branch7x7dbl = conv2d_bn(x, 160, 1, 1)\n",
    "#         branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "#         branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 1, 7)\n",
    "#         branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "#         branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "#         branch_pool = AveragePooling2D(\n",
    "#             (3, 3), strides=(1, 1), padding='same')(x)\n",
    "#         branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "#         x = layers.concatenate(\n",
    "#             [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "#             axis=channel_axis,\n",
    "#             name='mixed' + str(5 + i))\n",
    "\n",
    "#     # mixed 7: 17 x 17 x 768\n",
    "#     branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "#     branch7x7 = conv2d_bn(x, 192, 1, 1)\n",
    "#     branch7x7 = conv2d_bn(branch7x7, 192, 1, 7)\n",
    "#     branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "#     branch7x7dbl = conv2d_bn(x, 192, 1, 1)\n",
    "#     branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "#     branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "#     branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "#     branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "#     branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "#     branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "#     x = layers.concatenate(\n",
    "#         [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "#         axis=channel_axis,\n",
    "#         name='mixed7')\n",
    "\n",
    "#     # mixed 8: 8 x 8 x 1280\n",
    "#     branch3x3 = conv2d_bn(x, 192, 1, 1)\n",
    "#     branch3x3 = conv2d_bn(branch3x3, 320, 3, 3,\n",
    "#                           strides=(2, 2), padding='valid')\n",
    "\n",
    "#     branch7x7x3 = conv2d_bn(x, 192, 1, 1)\n",
    "#     branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)\n",
    "#     branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)\n",
    "#     branch7x7x3 = conv2d_bn(\n",
    "#         branch7x7x3, 192, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "#     branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "#     x = layers.concatenate(\n",
    "#         [branch3x3, branch7x7x3, branch_pool], axis=channel_axis, name='mixed8')\n",
    "\n",
    "#     # mixed 9: 8 x 8 x 2048\n",
    "#     for i in range(2):\n",
    "#         branch1x1 = conv2d_bn(x, 320, 1, 1)\n",
    "\n",
    "#         branch3x3 = conv2d_bn(x, 384, 1, 1)\n",
    "#         branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)\n",
    "#         branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)\n",
    "#         branch3x3 = layers.concatenate(\n",
    "#             [branch3x3_1, branch3x3_2], axis=channel_axis, name='mixed9_' + str(i))\n",
    "\n",
    "#         branch3x3dbl = conv2d_bn(x, 448, 1, 1)\n",
    "#         branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)\n",
    "#         branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)\n",
    "#         branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)\n",
    "#         branch3x3dbl = layers.concatenate(\n",
    "#             [branch3x3dbl_1, branch3x3dbl_2], axis=channel_axis)\n",
    "\n",
    "#         branch_pool = AveragePooling2D(\n",
    "#             (3, 3), strides=(1, 1), padding='same')(x)\n",
    "#         branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "#         x = layers.concatenate(\n",
    "#             [branch1x1, branch3x3, branch3x3dbl, branch_pool],\n",
    "#             axis=channel_axis,\n",
    "#             name='mixed' + str(9 + i))\n",
    "#     if include_top:\n",
    "#         # Classification block\n",
    "#         x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "#         x = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "#     else:\n",
    "#         if pooling == 'avg':\n",
    "#             x = GlobalAveragePooling2D()(x)\n",
    "#         elif pooling == 'max':\n",
    "#             x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "#     # Ensure that the model takes into account\n",
    "#     # any potential predecessors of `input_tensor`.\n",
    "#     if input_tensor is not None:\n",
    "#         inputs = get_source_inputs(input_tensor)\n",
    "#     else:\n",
    "#         inputs = img_input\n",
    "#     # Create model.\n",
    "#     model = Model(inputs, x, name='inception_v3')\n",
    "\n",
    "#     # load weights\n",
    "#     if weights == 'imagenet':\n",
    "#         if K.image_data_format() == 'channels_first':\n",
    "#             if K.backend() == 'tensorflow':\n",
    "#                 warnings.warn('You are using the TensorFlow backend, yet you '\n",
    "#                               'are using the Theano '\n",
    "#                               'image data format convention '\n",
    "#                               '(`image_data_format=\"channels_first\"`). '\n",
    "#                               'For best performance, set '\n",
    "#                               '`image_data_format=\"channels_last\"` in '\n",
    "#                               'your Keras config '\n",
    "#                               'at ~/.keras/keras.json.')\n",
    "#         if include_top:\n",
    "#             weights_path = get_file(\n",
    "#                 'inception_v3_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "#                 WEIGHTS_PATH,\n",
    "#                 cache_subdir='models',\n",
    "#                 md5_hash='9a0d58056eeedaa3f26cb7ebd46da564')\n",
    "#         else:\n",
    "#             weights_path = get_file(\n",
    "#                 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "#                 WEIGHTS_PATH_NO_TOP,\n",
    "#                 cache_subdir='models',\n",
    "#                 md5_hash='bcbd6486424b2319ff4ef7d526e38f63')\n",
    "#             weights_path = \"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "#         model.load_weights(weights_path)\n",
    "#         if K.backend() == 'theano':\n",
    "#             convert_all_kernels_in_model(model)\n",
    "#     return model\n",
    "# if __name__ == '__main__':\n",
    "#     model = InceptionV3(include_top=False, weights='imagenet')\n",
    "#     model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out, channels):\n",
    "    input_tensor = Input(shape=(299,299,len(channels)))\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    base_model = InceptionV3(include_top=False, weights='imagenet')\n",
    "    x = Conv2D(3, kernel_size = (1,1), activation = 'relu')(bn)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = base_model(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Conv2D(128, kernel_size=(3,3), activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 299, 299, 4)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_96 (Batc (None, 299, 299, 4)       16        \n",
      "_________________________________________________________________\n",
      "conv2d_189 (Conv2D)          (None, 299, 299, 3)       15        \n",
      "_________________________________________________________________\n",
      "batch_normalization_191 (Bat (None, 299, 299, 3)       12        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         multiple                  21802784  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 2048)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_190 (Conv2D)          (None, 6, 6, 128)         2359424   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 28)                28700     \n",
      "=================================================================\n",
      "Total params: 28,910,567\n",
      "Trainable params: 28,876,121\n",
      "Non-trainable params: 34,446\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "channels = [\"green\", \"blue\", \"red\", \"yellow\"]\n",
    "# with tf.device('/cpu:0'):\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,4), \n",
    "    n_out=28, channels = channels)\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=Adam(1e-04),\n",
    "    metrics=['acc', f1])\n",
    "model.summary()\n",
    "\n",
    "# model.layers[3].layers[1] = Conv2D(32, kernel_size = (7,7), strides = (2,2), padding = \"same\", input_shape = (299, 299, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31072,) (31072, 28)\n",
      "(27964,) (27964, 28) (3108,) (3108, 28)\n"
     ]
    }
   ],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "epochs = 10; batch_size = 32;VAL_RATIO = .1;DEBUG = False\n",
    "# split data into train, valid\n",
    "paths, labels = getTrainDataset()\n",
    "\n",
    "# divide to \n",
    "keys = np.arange(paths.shape[0], dtype=np.int)  \n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(keys)\n",
    "lastTrainIndex = int((1-VAL_RATIO) * paths.shape[0])\n",
    "\n",
    "if DEBUG == True:  # use only small subset for debugging, Kaggle's RAM is limited\n",
    "    pathsTrain = paths[0:256]\n",
    "    labelsTrain = labels[0:256]\n",
    "    pathsVal = paths[lastTrainIndex:lastTrainIndex+256]\n",
    "    labelsVal = labels[lastTrainIndex:lastTrainIndex+256]\n",
    "    use_cache = True\n",
    "else:\n",
    "    pathsTrain = paths[0:lastTrainIndex]\n",
    "    labelsTrain = labels[0:lastTrainIndex]\n",
    "    pathsVal = paths[lastTrainIndex:]\n",
    "    labelsVal = labels[lastTrainIndex:]\n",
    "    use_cache = False\n",
    "\n",
    "print(paths.shape, labels.shape)\n",
    "print(pathsTrain.shape, labelsTrain.shape, pathsVal.shape, labelsVal.shape)\n",
    "use_cache = True\n",
    "tg = ProteinDataGenerator(pathsTrain, labelsTrain, batch_size, SHAPE, channels, use_cache=use_cache)\n",
    "vg = ProteinDataGenerator(pathsVal, labelsVal, batch_size, SHAPE, channels, use_cache=use_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and valid datagens\n",
    "# train_generator = data_generator.create_train(\n",
    "#     train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "# validation_generator = data_generator.create_train(\n",
    "#     train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('../working/InceptionV3+1x1.h5', monitor='val_f1', verbose=1, \n",
    "                             save_best_only=True, mode='max', save_weights_only = False)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_f1', factor=0.5, patience=10, \n",
    "                                   verbose=1, mode='max', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_f1\", \n",
    "                      mode=\"max\", \n",
    "                      patience=20)\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 299, 299, 4)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_192 (Bat (None, 299, 299, 4)       16        \n",
      "_________________________________________________________________\n",
      "conv2d_285 (Conv2D)          (None, 299, 299, 3)       15        \n",
      "_________________________________________________________________\n",
      "batch_normalization_287 (Bat (None, 299, 299, 3)       12        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         multiple                  21802784  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 2048)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_286 (Conv2D)          (None, 6, 6, 128)         2359424   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 28)                28700     \n",
      "=================================================================\n",
      "Total params: 28,910,567\n",
      "Trainable params: 7,107,763\n",
      "Non-trainable params: 21,802,804\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# warm up model\n",
    "import tensorflow as tf\n",
    "channels = [\"green\", \"blue\", \"red\", \"yellow\"]\n",
    "# with tf.device('/cpu:0'):\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,4), \n",
    "    n_out=28, channels = channels)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "model.layers[0].trainable = True\n",
    "model.layers[1].trainable = True\n",
    "model.layers[2].trainable = True\n",
    "model.layers[-1].trainable = True\n",
    "model.layers[-2].trainable = True\n",
    "model.layers[-3].trainable = True\n",
    "model.layers[-4].trainable = True\n",
    "model.layers[-5].trainable = True\n",
    "model.layers[-6].trainable = True\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tg = ProteinDataGenerator(pathsTrain, labelsTrain, batch_size, SHAPE, channels, use_cache=use_cache)\n",
    "vg = ProteinDataGenerator(pathsVal, labelsVal, batch_size, SHAPE, channels, use_cache=use_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "437/437 [==============================] - 352s 805ms/step - loss: 0.2100 - acc: 0.9333 - f1: 0.0639 - val_loss: 0.2037 - val_acc: 0.9383 - val_f1: 0.0635\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.06348, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 2/20\n",
      "437/437 [==============================] - 406s 928ms/step - loss: 0.1825 - acc: 0.9428 - f1: 0.0651 - val_loss: 0.2080 - val_acc: 0.9222 - val_f1: 0.0568\n",
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.06348\n",
      "Epoch 3/20\n",
      "437/437 [==============================] - 150s 344ms/step - loss: 0.1768 - acc: 0.9437 - f1: 0.0700 - val_loss: 0.2138 - val_acc: 0.9239 - val_f1: 0.0784\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.06348 to 0.07837, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 4/20\n",
      "437/437 [==============================] - 166s 381ms/step - loss: 0.1726 - acc: 0.9447 - f1: 0.0716 - val_loss: 0.2084 - val_acc: 0.9254 - val_f1: 0.0734\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.07837\n",
      "Epoch 5/20\n",
      "437/437 [==============================] - 144s 330ms/step - loss: 0.1702 - acc: 0.9454 - f1: 0.0773 - val_loss: 0.2110 - val_acc: 0.9204 - val_f1: 0.0656\n",
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.07837\n",
      "Epoch 6/20\n",
      "437/437 [==============================] - 221s 506ms/step - loss: 0.1675 - acc: 0.9455 - f1: 0.0782 - val_loss: 0.1966 - val_acc: 0.9253 - val_f1: 0.0585\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.07837\n",
      "Epoch 7/20\n",
      "437/437 [==============================] - 140s 320ms/step - loss: 0.1659 - acc: 0.9462 - f1: 0.0834 - val_loss: 0.2186 - val_acc: 0.9249 - val_f1: 0.0826\n",
      "\n",
      "Epoch 00007: val_f1 improved from 0.07837 to 0.08263, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 8/20\n",
      "437/437 [==============================] - 142s 325ms/step - loss: 0.1651 - acc: 0.9461 - f1: 0.0851 - val_loss: 0.2394 - val_acc: 0.9331 - val_f1: 0.0981\n",
      "\n",
      "Epoch 00008: val_f1 improved from 0.08263 to 0.09807, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 9/20\n",
      "437/437 [==============================] - 184s 420ms/step - loss: 0.1630 - acc: 0.9468 - f1: 0.0904 - val_loss: 0.2130 - val_acc: 0.9306 - val_f1: 0.0887\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.09807\n",
      "Epoch 10/20\n",
      "437/437 [==============================] - 140s 320ms/step - loss: 0.1617 - acc: 0.9468 - f1: 0.0936 - val_loss: 0.1947 - val_acc: 0.9320 - val_f1: 0.0685\n",
      "\n",
      "Epoch 00010: val_f1 did not improve from 0.09807\n",
      "Epoch 11/20\n",
      "437/437 [==============================] - 146s 335ms/step - loss: 0.1590 - acc: 0.9475 - f1: 0.1003 - val_loss: 0.2199 - val_acc: 0.9297 - val_f1: 0.0887\n",
      "\n",
      "Epoch 00011: val_f1 did not improve from 0.09807\n",
      "Epoch 12/20\n",
      "437/437 [==============================] - 183s 419ms/step - loss: 0.1598 - acc: 0.9474 - f1: 0.0991 - val_loss: 0.2006 - val_acc: 0.9273 - val_f1: 0.0612\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.09807\n",
      "Epoch 13/20\n",
      "437/437 [==============================] - 164s 375ms/step - loss: 0.1569 - acc: 0.9478 - f1: 0.1048 - val_loss: 0.2147 - val_acc: 0.9287 - val_f1: 0.0868\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.09807\n",
      "Epoch 14/20\n",
      "437/437 [==============================] - 143s 327ms/step - loss: 0.1574 - acc: 0.9479 - f1: 0.1054 - val_loss: 0.2195 - val_acc: 0.9236 - val_f1: 0.0834\n",
      "\n",
      "Epoch 00014: val_f1 did not improve from 0.09807\n",
      "Epoch 15/20\n",
      "437/437 [==============================] - 172s 393ms/step - loss: 0.1550 - acc: 0.9478 - f1: 0.1115 - val_loss: 0.2143 - val_acc: 0.9259 - val_f1: 0.0816\n",
      "\n",
      "Epoch 00015: val_f1 did not improve from 0.09807\n",
      "Epoch 16/20\n",
      "437/437 [==============================] - 156s 358ms/step - loss: 0.1549 - acc: 0.9484 - f1: 0.1129 - val_loss: 0.2172 - val_acc: 0.9280 - val_f1: 0.0832\n",
      "\n",
      "Epoch 00016: val_f1 did not improve from 0.09807\n",
      "Epoch 17/20\n",
      "437/437 [==============================] - 143s 328ms/step - loss: 0.1517 - acc: 0.9491 - f1: 0.1181 - val_loss: 0.2126 - val_acc: 0.9286 - val_f1: 0.0804\n",
      "\n",
      "Epoch 00017: val_f1 did not improve from 0.09807\n",
      "Epoch 18/20\n",
      "437/437 [==============================] - 188s 430ms/step - loss: 0.1538 - acc: 0.9481 - f1: 0.1176 - val_loss: 0.2134 - val_acc: 0.9244 - val_f1: 0.0843\n",
      "\n",
      "Epoch 00018: val_f1 did not improve from 0.09807\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 19/20\n",
      "437/437 [==============================] - 150s 343ms/step - loss: 0.1478 - acc: 0.9498 - f1: 0.1281 - val_loss: 0.2140 - val_acc: 0.9268 - val_f1: 0.0835\n",
      "\n",
      "Epoch 00019: val_f1 did not improve from 0.09807\n",
      "Epoch 20/20\n",
      "437/437 [==============================] - 154s 352ms/step - loss: 0.1483 - acc: 0.9497 - f1: 0.1287 - val_loss: 0.2172 - val_acc: 0.9251 - val_f1: 0.0773\n",
      "\n",
      "Epoch 00020: val_f1 did not improve from 0.09807\n"
     ]
    }
   ],
   "source": [
    "hist =  model.fit_generator(\n",
    "        tg,\n",
    "        steps_per_epoch=np.ceil(float(len(pathsTrain)) / float(batch_size))/2,\n",
    "        validation_data=vg,\n",
    "        validation_steps=np.ceil(float(len(pathsVal)) / float(batch_size))/2,\n",
    "        epochs=20, \n",
    "        verbose=1,\n",
    "        callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x000001E08480ED30>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001E08480EA58>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001F549719C88>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001F54975DE48>\n",
      "<keras.engine.training.Model object at 0x000001F549702588>\n",
      "<keras.layers.core.Dropout object at 0x000001F54F105E80>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001F54E818C18>\n",
      "<keras.layers.core.Flatten object at 0x000001F54F4AECF8>\n",
      "<keras.layers.core.Dropout object at 0x000001F54D218860>\n",
      "<keras.layers.core.Dense object at 0x000001F5639A7B70>\n",
      "<keras.layers.core.Dropout object at 0x000001F563ADCB00>\n",
      "<keras.layers.core.Dense object at 0x000001F563ADCE10>\n"
     ]
    }
   ],
   "source": [
    "# train all layers\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    layer.trainable = True\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1166/1165 [==============================] - 621s 533ms/step - loss: 0.1530 - acc: 0.9495 - f1: 0.1261 - val_loss: 0.1518 - val_acc: 0.9506 - val_f1: 0.1736\n",
      "\n",
      "Epoch 00001: val_f1 improved from 0.09807 to 0.17361, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 2/100\n",
      "1166/1165 [==============================] - 531s 456ms/step - loss: 0.1350 - acc: 0.9543 - f1: 0.1766 - val_loss: 0.1400 - val_acc: 0.9516 - val_f1: 0.1889\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.17361 to 0.18892, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 3/100\n",
      "1166/1165 [==============================] - 779s 668ms/step - loss: 0.1206 - acc: 0.9585 - f1: 0.2244 - val_loss: 0.1345 - val_acc: 0.9538 - val_f1: 0.2171\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.18892 to 0.21710, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 4/100\n",
      "1166/1165 [==============================] - 573s 491ms/step - loss: 0.0987 - acc: 0.9658 - f1: 0.2831 - val_loss: 0.1394 - val_acc: 0.9523 - val_f1: 0.2234\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.21710 to 0.22339, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 5/100\n",
      "1166/1165 [==============================] - 535s 459ms/step - loss: 0.0818 - acc: 0.9714 - f1: 0.3311 - val_loss: 0.1523 - val_acc: 0.9511 - val_f1: 0.2172\n",
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.22339\n",
      "Epoch 6/100\n",
      "1166/1165 [==============================] - 542s 465ms/step - loss: 0.0666 - acc: 0.9768 - f1: 0.3714 - val_loss: 0.1501 - val_acc: 0.9518 - val_f1: 0.2348\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.22339 to 0.23479, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 7/100\n",
      "1166/1165 [==============================] - 556s 477ms/step - loss: 0.0499 - acc: 0.9825 - f1: 0.4170 - val_loss: 0.1726 - val_acc: 0.9485 - val_f1: 0.2302\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.23479\n",
      "Epoch 8/100\n",
      "1166/1165 [==============================] - 557s 478ms/step - loss: 0.0418 - acc: 0.9853 - f1: 0.4424 - val_loss: 0.1719 - val_acc: 0.9493 - val_f1: 0.2312\n",
      "\n",
      "Epoch 00008: val_f1 did not improve from 0.23479\n",
      "Epoch 9/100\n",
      "1166/1165 [==============================] - 584s 501ms/step - loss: 0.0353 - acc: 0.9876 - f1: 0.4655 - val_loss: 0.1927 - val_acc: 0.9508 - val_f1: 0.2408\n",
      "\n",
      "Epoch 00009: val_f1 improved from 0.23479 to 0.24076, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 10/100\n",
      "1166/1165 [==============================] - 571s 490ms/step - loss: 0.0291 - acc: 0.9900 - f1: 0.4834 - val_loss: 0.1841 - val_acc: 0.9513 - val_f1: 0.2408\n",
      "\n",
      "Epoch 00010: val_f1 did not improve from 0.24076\n",
      "Epoch 11/100\n",
      "1166/1165 [==============================] - 549s 471ms/step - loss: 0.0263 - acc: 0.9910 - f1: 0.4913 - val_loss: 0.2048 - val_acc: 0.9478 - val_f1: 0.2291\n",
      "\n",
      "Epoch 00011: val_f1 did not improve from 0.24076\n",
      "Epoch 12/100\n",
      "1166/1165 [==============================] - 542s 465ms/step - loss: 0.0231 - acc: 0.9921 - f1: 0.5026 - val_loss: 0.1984 - val_acc: 0.9515 - val_f1: 0.2366\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.24076\n",
      "Epoch 13/100\n",
      "1166/1165 [==============================] - 557s 477ms/step - loss: 0.0201 - acc: 0.9932 - f1: 0.5110 - val_loss: 0.2309 - val_acc: 0.9497 - val_f1: 0.2252\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.24076\n",
      "Epoch 14/100\n",
      "1166/1165 [==============================] - 531s 456ms/step - loss: 0.0185 - acc: 0.9937 - f1: 0.5154 - val_loss: 0.2205 - val_acc: 0.9501 - val_f1: 0.2338\n",
      "\n",
      "Epoch 00014: val_f1 did not improve from 0.24076\n",
      "Epoch 15/100\n",
      "1166/1165 [==============================] - 599s 514ms/step - loss: 0.0182 - acc: 0.9939 - f1: 0.5149 - val_loss: 0.2073 - val_acc: 0.9492 - val_f1: 0.2321\n",
      "\n",
      "Epoch 00015: val_f1 did not improve from 0.24076\n",
      "Epoch 16/100\n",
      "1166/1165 [==============================] - 562s 482ms/step - loss: 0.0159 - acc: 0.9947 - f1: 0.5229 - val_loss: 0.2360 - val_acc: 0.9520 - val_f1: 0.2296\n",
      "\n",
      "Epoch 00016: val_f1 did not improve from 0.24076\n",
      "Epoch 17/100\n",
      "1166/1165 [==============================] - 531s 455ms/step - loss: 0.0151 - acc: 0.9950 - f1: 0.5226 - val_loss: 0.2439 - val_acc: 0.9480 - val_f1: 0.2276\n",
      "\n",
      "Epoch 00017: val_f1 did not improve from 0.24076\n",
      "Epoch 18/100\n",
      "1166/1165 [==============================] - 555s 476ms/step - loss: 0.0148 - acc: 0.9951 - f1: 0.5256 - val_loss: 0.2212 - val_acc: 0.9523 - val_f1: 0.2459\n",
      "\n",
      "Epoch 00018: val_f1 improved from 0.24076 to 0.24586, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 19/100\n",
      "1166/1165 [==============================] - 534s 458ms/step - loss: 0.0131 - acc: 0.9956 - f1: 0.5306 - val_loss: 0.2350 - val_acc: 0.9505 - val_f1: 0.2308\n",
      "\n",
      "Epoch 00019: val_f1 did not improve from 0.24586\n",
      "Epoch 20/100\n",
      "1166/1165 [==============================] - 542s 465ms/step - loss: 0.0126 - acc: 0.9958 - f1: 0.5299 - val_loss: 0.2485 - val_acc: 0.9516 - val_f1: 0.2336\n",
      "\n",
      "Epoch 00020: val_f1 did not improve from 0.24586\n",
      "Epoch 21/100\n",
      "1166/1165 [==============================] - 544s 466ms/step - loss: 0.0128 - acc: 0.9958 - f1: 0.5297 - val_loss: 0.2399 - val_acc: 0.9505 - val_f1: 0.2283\n",
      "\n",
      "Epoch 00021: val_f1 did not improve from 0.24586\n",
      "Epoch 22/100\n",
      "1166/1165 [==============================] - 576s 494ms/step - loss: 0.0114 - acc: 0.9963 - f1: 0.5340 - val_loss: 0.2325 - val_acc: 0.9531 - val_f1: 0.2449\n",
      "\n",
      "Epoch 00022: val_f1 did not improve from 0.24586\n",
      "Epoch 23/100\n",
      "1166/1165 [==============================] - 530s 455ms/step - loss: 0.0112 - acc: 0.9964 - f1: 0.5342 - val_loss: 0.2449 - val_acc: 0.9530 - val_f1: 0.2263\n",
      "\n",
      "Epoch 00023: val_f1 did not improve from 0.24586\n",
      "Epoch 24/100\n",
      "1166/1165 [==============================] - 558s 479ms/step - loss: 0.0109 - acc: 0.9964 - f1: 0.5363 - val_loss: 0.2329 - val_acc: 0.9517 - val_f1: 0.2489\n",
      "\n",
      "Epoch 00024: val_f1 improved from 0.24586 to 0.24892, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 25/100\n",
      "1166/1165 [==============================] - 548s 470ms/step - loss: 0.0101 - acc: 0.9967 - f1: 0.5376 - val_loss: 0.2586 - val_acc: 0.9509 - val_f1: 0.2354\n",
      "\n",
      "Epoch 00025: val_f1 did not improve from 0.24892\n",
      "Epoch 26/100\n",
      "1166/1165 [==============================] - 554s 475ms/step - loss: 0.0101 - acc: 0.9966 - f1: 0.5379 - val_loss: 0.2325 - val_acc: 0.9538 - val_f1: 0.2444\n",
      "\n",
      "Epoch 00026: val_f1 did not improve from 0.24892\n",
      "Epoch 27/100\n",
      "1166/1165 [==============================] - 592s 507ms/step - loss: 0.0093 - acc: 0.9971 - f1: 0.5405 - val_loss: 0.2403 - val_acc: 0.9491 - val_f1: 0.2314\n",
      "\n",
      "Epoch 00027: val_f1 did not improve from 0.24892\n",
      "Epoch 28/100\n",
      "1166/1165 [==============================] - 546s 468ms/step - loss: 0.0092 - acc: 0.9971 - f1: 0.5397 - val_loss: 0.2406 - val_acc: 0.9535 - val_f1: 0.2379\n",
      "\n",
      "Epoch 00028: val_f1 did not improve from 0.24892\n",
      "Epoch 29/100\n",
      "1166/1165 [==============================] - 548s 470ms/step - loss: 0.0089 - acc: 0.9972 - f1: 0.5398 - val_loss: 0.2617 - val_acc: 0.9518 - val_f1: 0.2266\n",
      "\n",
      "Epoch 00029: val_f1 did not improve from 0.24892\n",
      "Epoch 30/100\n",
      "1166/1165 [==============================] - 552s 474ms/step - loss: 0.0085 - acc: 0.9972 - f1: 0.5417 - val_loss: 0.2426 - val_acc: 0.9523 - val_f1: 0.2366\n",
      "\n",
      "Epoch 00030: val_f1 did not improve from 0.24892\n",
      "Epoch 31/100\n",
      "1166/1165 [==============================] - 567s 487ms/step - loss: 0.0084 - acc: 0.9973 - f1: 0.5408 - val_loss: 0.2420 - val_acc: 0.9539 - val_f1: 0.2494\n",
      "\n",
      "Epoch 00031: val_f1 improved from 0.24892 to 0.24938, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 32/100\n",
      "1166/1165 [==============================] - 543s 465ms/step - loss: 0.0080 - acc: 0.9975 - f1: 0.5419 - val_loss: 0.2554 - val_acc: 0.9518 - val_f1: 0.2318\n",
      "\n",
      "Epoch 00032: val_f1 did not improve from 0.24938\n",
      "Epoch 33/100\n",
      "1166/1165 [==============================] - 605s 519ms/step - loss: 0.0082 - acc: 0.9974 - f1: 0.5426 - val_loss: 0.2295 - val_acc: 0.9503 - val_f1: 0.2473\n",
      "\n",
      "Epoch 00033: val_f1 did not improve from 0.24938\n",
      "Epoch 34/100\n",
      "1166/1165 [==============================] - 550s 472ms/step - loss: 0.0079 - acc: 0.9975 - f1: 0.5434 - val_loss: 0.2505 - val_acc: 0.9539 - val_f1: 0.2425\n",
      "\n",
      "Epoch 00034: val_f1 did not improve from 0.24938\n",
      "Epoch 35/100\n",
      "1166/1165 [==============================] - 541s 464ms/step - loss: 0.0070 - acc: 0.9977 - f1: 0.5447 - val_loss: 0.2410 - val_acc: 0.9530 - val_f1: 0.2414\n",
      "\n",
      "Epoch 00035: val_f1 did not improve from 0.24938\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1165 [==============================] - 568s 487ms/step - loss: 0.0076 - acc: 0.9975 - f1: 0.5442 - val_loss: 0.2408 - val_acc: 0.9524 - val_f1: 0.2432\n",
      "\n",
      "Epoch 00036: val_f1 did not improve from 0.24938\n",
      "Epoch 37/100\n",
      "1166/1165 [==============================] - 574s 492ms/step - loss: 0.0072 - acc: 0.9977 - f1: 0.5451 - val_loss: 0.2249 - val_acc: 0.9525 - val_f1: 0.2366\n",
      "\n",
      "Epoch 00037: val_f1 did not improve from 0.24938\n",
      "Epoch 38/100\n",
      "1166/1165 [==============================] - 554s 475ms/step - loss: 0.0069 - acc: 0.9978 - f1: 0.5455 - val_loss: 0.2442 - val_acc: 0.9530 - val_f1: 0.2401\n",
      "\n",
      "Epoch 00038: val_f1 did not improve from 0.24938\n",
      "Epoch 39/100\n",
      "1166/1165 [==============================] - 557s 478ms/step - loss: 0.0066 - acc: 0.9979 - f1: 0.5447 - val_loss: 0.2402 - val_acc: 0.9523 - val_f1: 0.2530\n",
      "\n",
      "Epoch 00039: val_f1 improved from 0.24938 to 0.25301, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 40/100\n",
      "1166/1165 [==============================] - 556s 477ms/step - loss: 0.0063 - acc: 0.9980 - f1: 0.5469 - val_loss: 0.2463 - val_acc: 0.9510 - val_f1: 0.2375\n",
      "\n",
      "Epoch 00040: val_f1 did not improve from 0.25301\n",
      "Epoch 41/100\n",
      "1166/1165 [==============================] - 523s 448ms/step - loss: 0.0066 - acc: 0.9979 - f1: 0.5466 - val_loss: 0.2454 - val_acc: 0.9487 - val_f1: 0.2480\n",
      "\n",
      "Epoch 00041: val_f1 did not improve from 0.25301\n",
      "Epoch 42/100\n",
      "1166/1165 [==============================] - 561s 481ms/step - loss: 0.0063 - acc: 0.9980 - f1: 0.5466 - val_loss: 0.2649 - val_acc: 0.9519 - val_f1: 0.2406\n",
      "\n",
      "Epoch 00042: val_f1 did not improve from 0.25301\n",
      "Epoch 43/100\n",
      "1166/1165 [==============================] - 557s 478ms/step - loss: 0.0060 - acc: 0.9982 - f1: 0.5484 - val_loss: 0.2814 - val_acc: 0.9457 - val_f1: 0.2163\n",
      "\n",
      "Epoch 00043: val_f1 did not improve from 0.25301\n",
      "Epoch 44/100\n",
      "1166/1165 [==============================] - 518s 445ms/step - loss: 0.0063 - acc: 0.9980 - f1: 0.5477 - val_loss: 0.2564 - val_acc: 0.9536 - val_f1: 0.2387\n",
      "\n",
      "Epoch 00044: val_f1 did not improve from 0.25301\n",
      "Epoch 45/100\n",
      "1166/1165 [==============================] - 814s 698ms/step - loss: 0.0060 - acc: 0.9982 - f1: 0.5460 - val_loss: 0.2599 - val_acc: 0.9542 - val_f1: 0.2425\n",
      "\n",
      "Epoch 00045: val_f1 did not improve from 0.25301\n",
      "Epoch 46/100\n",
      "1166/1165 [==============================] - 553s 474ms/step - loss: 0.0058 - acc: 0.9982 - f1: 0.5478 - val_loss: 0.2602 - val_acc: 0.9534 - val_f1: 0.2438\n",
      "\n",
      "Epoch 00046: val_f1 did not improve from 0.25301\n",
      "Epoch 47/100\n",
      "1166/1165 [==============================] - 520s 446ms/step - loss: 0.0061 - acc: 0.9981 - f1: 0.5474 - val_loss: 0.2388 - val_acc: 0.9529 - val_f1: 0.2411\n",
      "\n",
      "Epoch 00047: val_f1 did not improve from 0.25301\n",
      "Epoch 48/100\n",
      "1166/1165 [==============================] - 614s 527ms/step - loss: 0.0055 - acc: 0.9983 - f1: 0.5490 - val_loss: 0.2462 - val_acc: 0.9544 - val_f1: 0.2400\n",
      "\n",
      "Epoch 00048: val_f1 did not improve from 0.25301\n",
      "Epoch 49/100\n",
      "1166/1165 [==============================] - 535s 459ms/step - loss: 0.0056 - acc: 0.9983 - f1: 0.5473 - val_loss: 0.2497 - val_acc: 0.9516 - val_f1: 0.2313\n",
      "\n",
      "Epoch 00049: val_f1 did not improve from 0.25301\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 50/100\n",
      "1166/1165 [==============================] - 531s 456ms/step - loss: 0.0026 - acc: 0.9993 - f1: 0.5554 - val_loss: 0.2773 - val_acc: 0.9542 - val_f1: 0.2514\n",
      "\n",
      "Epoch 00050: val_f1 did not improve from 0.25301\n",
      "Epoch 51/100\n",
      "1166/1165 [==============================] - 566s 485ms/step - loss: 0.0015 - acc: 0.9996 - f1: 0.5578 - val_loss: 0.3055 - val_acc: 0.9554 - val_f1: 0.2427\n",
      "\n",
      "Epoch 00051: val_f1 did not improve from 0.25301\n",
      "Epoch 52/100\n",
      "1166/1165 [==============================] - 590s 506ms/step - loss: 0.0016 - acc: 0.9995 - f1: 0.5582 - val_loss: 0.2941 - val_acc: 0.9559 - val_f1: 0.2510\n",
      "\n",
      "Epoch 00052: val_f1 did not improve from 0.25301\n",
      "Epoch 53/100\n",
      "1166/1165 [==============================] - 525s 450ms/step - loss: 0.0018 - acc: 0.9995 - f1: 0.5574 - val_loss: 0.2831 - val_acc: 0.9561 - val_f1: 0.2566\n",
      "\n",
      "Epoch 00053: val_f1 improved from 0.25301 to 0.25661, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 54/100\n",
      "1166/1165 [==============================] - 622s 533ms/step - loss: 0.0017 - acc: 0.9995 - f1: 0.5554 - val_loss: 0.2949 - val_acc: 0.9558 - val_f1: 0.2492\n",
      "\n",
      "Epoch 00054: val_f1 did not improve from 0.25661\n",
      "Epoch 55/100\n",
      "1166/1165 [==============================] - 549s 471ms/step - loss: 0.0016 - acc: 0.9995 - f1: 0.5570 - val_loss: 0.2806 - val_acc: 0.9537 - val_f1: 0.2584\n",
      "\n",
      "Epoch 00055: val_f1 improved from 0.25661 to 0.25835, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 56/100\n",
      "1166/1165 [==============================] - 543s 466ms/step - loss: 0.0018 - acc: 0.9995 - f1: 0.5571 - val_loss: 0.2979 - val_acc: 0.9556 - val_f1: 0.2456\n",
      "\n",
      "Epoch 00056: val_f1 did not improve from 0.25835\n",
      "Epoch 57/100\n",
      "1166/1165 [==============================] - 565s 485ms/step - loss: 0.0018 - acc: 0.9995 - f1: 0.5569 - val_loss: 0.3037 - val_acc: 0.9564 - val_f1: 0.2461\n",
      "\n",
      "Epoch 00057: val_f1 did not improve from 0.25835\n",
      "Epoch 58/100\n",
      "1166/1165 [==============================] - 554s 475ms/step - loss: 0.0017 - acc: 0.9995 - f1: 0.5570 - val_loss: 0.2653 - val_acc: 0.9558 - val_f1: 0.2625\n",
      "\n",
      "Epoch 00058: val_f1 improved from 0.25835 to 0.26247, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 59/100\n",
      "1166/1165 [==============================] - 564s 484ms/step - loss: 0.0015 - acc: 0.9996 - f1: 0.5586 - val_loss: 0.2787 - val_acc: 0.9567 - val_f1: 0.2606\n",
      "\n",
      "Epoch 00059: val_f1 did not improve from 0.26247\n",
      "Epoch 60/100\n",
      "1166/1165 [==============================] - 558s 478ms/step - loss: 0.0015 - acc: 0.9995 - f1: 0.5573 - val_loss: 0.3083 - val_acc: 0.9568 - val_f1: 0.2463\n",
      "\n",
      "Epoch 00060: val_f1 did not improve from 0.26247\n",
      "Epoch 61/100\n",
      "1166/1165 [==============================] - 544s 467ms/step - loss: 0.0012 - acc: 0.9996 - f1: 0.5571 - val_loss: 0.2877 - val_acc: 0.9556 - val_f1: 0.2589\n",
      "\n",
      "Epoch 00061: val_f1 did not improve from 0.26247\n",
      "Epoch 62/100\n",
      "1166/1165 [==============================] - 531s 456ms/step - loss: 0.0018 - acc: 0.9995 - f1: 0.5566 - val_loss: 0.2812 - val_acc: 0.9562 - val_f1: 0.2513\n",
      "\n",
      "Epoch 00062: val_f1 did not improve from 0.26247\n",
      "Epoch 63/100\n",
      "1166/1165 [==============================] - 608s 521ms/step - loss: 0.0015 - acc: 0.9996 - f1: 0.5585 - val_loss: 0.2807 - val_acc: 0.9560 - val_f1: 0.2527\n",
      "\n",
      "Epoch 00063: val_f1 did not improve from 0.26247\n",
      "Epoch 64/100\n",
      "1166/1165 [==============================] - 556s 477ms/step - loss: 0.0014 - acc: 0.9996 - f1: 0.5570 - val_loss: 0.2966 - val_acc: 0.9529 - val_f1: 0.2531\n",
      "\n",
      "Epoch 00064: val_f1 did not improve from 0.26247\n",
      "Epoch 65/100\n",
      "1166/1165 [==============================] - 523s 449ms/step - loss: 0.0017 - acc: 0.9995 - f1: 0.5574 - val_loss: 0.2936 - val_acc: 0.9539 - val_f1: 0.2510\n",
      "\n",
      "Epoch 00065: val_f1 did not improve from 0.26247\n",
      "Epoch 66/100\n",
      "1166/1165 [==============================] - 862s 739ms/step - loss: 0.0017 - acc: 0.9995 - f1: 0.5577 - val_loss: 0.2968 - val_acc: 0.9568 - val_f1: 0.2537\n",
      "\n",
      "Epoch 00066: val_f1 did not improve from 0.26247\n",
      "Epoch 67/100\n",
      "1166/1165 [==============================] - 537s 460ms/step - loss: 0.0015 - acc: 0.9996 - f1: 0.5569 - val_loss: 0.2752 - val_acc: 0.9578 - val_f1: 0.2625\n",
      "\n",
      "Epoch 00067: val_f1 did not improve from 0.26247\n",
      "Epoch 68/100\n",
      "1166/1165 [==============================] - 527s 452ms/step - loss: 0.0013 - acc: 0.9997 - f1: 0.5580 - val_loss: 0.3158 - val_acc: 0.9558 - val_f1: 0.2505\n",
      "\n",
      "Epoch 00068: val_f1 did not improve from 0.26247\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 69/100\n",
      "1166/1165 [==============================] - 552s 473ms/step - loss: 6.6452e-04 - acc: 0.9998 - f1: 0.5595 - val_loss: 0.3050 - val_acc: 0.9573 - val_f1: 0.2553\n",
      "\n",
      "Epoch 00069: val_f1 did not improve from 0.26247\n",
      "Epoch 70/100\n",
      "1166/1165 [==============================] - 634s 544ms/step - loss: 3.4010e-04 - acc: 0.9999 - f1: 0.5617 - val_loss: 0.3258 - val_acc: 0.9571 - val_f1: 0.2564\n",
      "\n",
      "Epoch 00070: val_f1 did not improve from 0.26247\n",
      "Epoch 71/100\n",
      "1166/1165 [==============================] - 544s 466ms/step - loss: 4.2950e-04 - acc: 0.9999 - f1: 0.5566 - val_loss: 0.3291 - val_acc: 0.9567 - val_f1: 0.2584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00071: val_f1 did not improve from 0.26247\n",
      "Epoch 72/100\n",
      "1166/1165 [==============================] - 544s 467ms/step - loss: 4.8564e-04 - acc: 0.9999 - f1: 0.5607 - val_loss: 0.3312 - val_acc: 0.9554 - val_f1: 0.2538\n",
      "\n",
      "Epoch 00072: val_f1 did not improve from 0.26247\n",
      "Epoch 73/100\n",
      "1166/1165 [==============================] - 551s 472ms/step - loss: 4.8158e-04 - acc: 0.9999 - f1: 0.5594 - val_loss: 0.3275 - val_acc: 0.9564 - val_f1: 0.2559\n",
      "\n",
      "Epoch 00073: val_f1 did not improve from 0.26247\n",
      "Epoch 74/100\n",
      "1166/1165 [==============================] - 533s 457ms/step - loss: 5.8937e-04 - acc: 0.9999 - f1: 0.5588 - val_loss: 0.3111 - val_acc: 0.9567 - val_f1: 0.2620\n",
      "\n",
      "Epoch 00074: val_f1 did not improve from 0.26247\n",
      "Epoch 75/100\n",
      "1166/1165 [==============================] - 554s 475ms/step - loss: 4.2438e-04 - acc: 0.9999 - f1: 0.5612 - val_loss: 0.3308 - val_acc: 0.9568 - val_f1: 0.2616\n",
      "\n",
      "Epoch 00075: val_f1 did not improve from 0.26247\n",
      "Epoch 76/100\n",
      "1166/1165 [==============================] - 547s 469ms/step - loss: 4.8767e-04 - acc: 0.9999 - f1: 0.5597 - val_loss: 0.3352 - val_acc: 0.9568 - val_f1: 0.2598\n",
      "\n",
      "Epoch 00076: val_f1 did not improve from 0.26247\n",
      "Epoch 77/100\n",
      "1166/1165 [==============================] - 543s 466ms/step - loss: 5.1232e-04 - acc: 0.9999 - f1: 0.5587 - val_loss: 0.3280 - val_acc: 0.9570 - val_f1: 0.2594\n",
      "\n",
      "Epoch 00077: val_f1 did not improve from 0.26247\n",
      "Epoch 78/100\n",
      "1166/1165 [==============================] - 579s 497ms/step - loss: 4.7433e-04 - acc: 0.9999 - f1: 0.5596 - val_loss: 0.3219 - val_acc: 0.9572 - val_f1: 0.2626\n",
      "\n",
      "Epoch 00078: val_f1 improved from 0.26247 to 0.26264, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 79/100\n",
      "1166/1165 [==============================] - 576s 494ms/step - loss: 3.8055e-04 - acc: 0.9999 - f1: 0.5582 - val_loss: 0.3222 - val_acc: 0.9563 - val_f1: 0.2648\n",
      "\n",
      "Epoch 00079: val_f1 improved from 0.26264 to 0.26480, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 80/100\n",
      "1166/1165 [==============================] - 545s 467ms/step - loss: 5.0090e-04 - acc: 0.9999 - f1: 0.5616 - val_loss: 0.3385 - val_acc: 0.9574 - val_f1: 0.2593\n",
      "\n",
      "Epoch 00080: val_f1 did not improve from 0.26480\n",
      "Epoch 81/100\n",
      "1166/1165 [==============================] - 549s 471ms/step - loss: 4.4677e-04 - acc: 0.9999 - f1: 0.5590 - val_loss: 0.3155 - val_acc: 0.9579 - val_f1: 0.2659\n",
      "\n",
      "Epoch 00081: val_f1 improved from 0.26480 to 0.26589, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 82/100\n",
      "1166/1165 [==============================] - 545s 467ms/step - loss: 3.1797e-04 - acc: 0.9999 - f1: 0.5607 - val_loss: 0.3477 - val_acc: 0.9579 - val_f1: 0.2568\n",
      "\n",
      "Epoch 00082: val_f1 did not improve from 0.26589\n",
      "Epoch 83/100\n",
      "1166/1165 [==============================] - 552s 473ms/step - loss: 4.2462e-04 - acc: 0.9999 - f1: 0.5584 - val_loss: 0.3585 - val_acc: 0.9569 - val_f1: 0.2562\n",
      "\n",
      "Epoch 00083: val_f1 did not improve from 0.26589\n",
      "Epoch 84/100\n",
      "1166/1165 [==============================] - 579s 497ms/step - loss: 5.7269e-04 - acc: 0.9999 - f1: 0.5609 - val_loss: 0.3353 - val_acc: 0.9564 - val_f1: 0.2614\n",
      "\n",
      "Epoch 00084: val_f1 did not improve from 0.26589\n",
      "Epoch 85/100\n",
      "1166/1165 [==============================] - 545s 467ms/step - loss: 5.0099e-04 - acc: 0.9999 - f1: 0.5591 - val_loss: 0.3419 - val_acc: 0.9570 - val_f1: 0.2571\n",
      "\n",
      "Epoch 00085: val_f1 did not improve from 0.26589\n",
      "Epoch 86/100\n",
      "1166/1165 [==============================] - 540s 463ms/step - loss: 4.3523e-04 - acc: 0.9999 - f1: 0.5588 - val_loss: 0.3432 - val_acc: 0.9560 - val_f1: 0.2557\n",
      "\n",
      "Epoch 00086: val_f1 did not improve from 0.26589\n",
      "Epoch 87/100\n",
      "1166/1165 [==============================] - 571s 490ms/step - loss: 5.9569e-04 - acc: 0.9999 - f1: 0.5605 - val_loss: 0.3355 - val_acc: 0.9573 - val_f1: 0.2613\n",
      "\n",
      "Epoch 00087: val_f1 did not improve from 0.26589\n",
      "Epoch 88/100\n",
      "1166/1165 [==============================] - 557s 478ms/step - loss: 4.7431e-04 - acc: 0.9999 - f1: 0.5589 - val_loss: 0.3397 - val_acc: 0.9580 - val_f1: 0.2605\n",
      "\n",
      "Epoch 00088: val_f1 did not improve from 0.26589\n",
      "Epoch 89/100\n",
      "1166/1165 [==============================] - 572s 491ms/step - loss: 3.5769e-04 - acc: 0.9999 - f1: 0.5615 - val_loss: 0.3587 - val_acc: 0.9579 - val_f1: 0.2596\n",
      "\n",
      "Epoch 00089: val_f1 did not improve from 0.26589\n",
      "Epoch 90/100\n",
      "1166/1165 [==============================] - 592s 508ms/step - loss: 3.4860e-04 - acc: 0.9999 - f1: 0.5577 - val_loss: 0.3370 - val_acc: 0.9575 - val_f1: 0.2633\n",
      "\n",
      "Epoch 00090: val_f1 did not improve from 0.26589\n",
      "Epoch 91/100\n",
      "1166/1165 [==============================] - 538s 462ms/step - loss: 4.6123e-04 - acc: 0.9999 - f1: 0.5603 - val_loss: 0.3464 - val_acc: 0.9569 - val_f1: 0.2602\n",
      "\n",
      "Epoch 00091: val_f1 did not improve from 0.26589\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 92/100\n",
      "1166/1165 [==============================] - 548s 470ms/step - loss: 3.1951e-04 - acc: 0.9999 - f1: 0.5605 - val_loss: 0.3447 - val_acc: 0.9576 - val_f1: 0.2640\n",
      "\n",
      "Epoch 00092: val_f1 did not improve from 0.26589\n",
      "Epoch 93/100\n",
      "1166/1165 [==============================] - 604s 518ms/step - loss: 2.3921e-04 - acc: 0.9999 - f1: 0.5605 - val_loss: 0.3530 - val_acc: 0.9576 - val_f1: 0.2608\n",
      "\n",
      "Epoch 00093: val_f1 did not improve from 0.26589\n",
      "Epoch 94/100\n",
      "1166/1165 [==============================] - 587s 503ms/step - loss: 1.8156e-04 - acc: 1.0000 - f1: 0.5596 - val_loss: 0.3581 - val_acc: 0.9580 - val_f1: 0.2636\n",
      "\n",
      "Epoch 00094: val_f1 did not improve from 0.26589\n",
      "Epoch 95/100\n",
      "1166/1165 [==============================] - 573s 492ms/step - loss: 1.8103e-04 - acc: 1.0000 - f1: 0.5601 - val_loss: 0.3638 - val_acc: 0.9579 - val_f1: 0.2617\n",
      "\n",
      "Epoch 00095: val_f1 did not improve from 0.26589\n",
      "Epoch 96/100\n",
      "1166/1165 [==============================] - 1003s 860ms/step - loss: 2.4451e-04 - acc: 0.9999 - f1: 0.5600 - val_loss: 0.3567 - val_acc: 0.9575 - val_f1: 0.2609\n",
      "\n",
      "Epoch 00096: val_f1 did not improve from 0.26589\n",
      "Epoch 97/100\n",
      "1166/1165 [==============================] - 540s 463ms/step - loss: 1.9417e-04 - acc: 1.0000 - f1: 0.5586 - val_loss: 0.3510 - val_acc: 0.9580 - val_f1: 0.2649\n",
      "\n",
      "Epoch 00097: val_f1 did not improve from 0.26589\n",
      "Epoch 98/100\n",
      "1166/1165 [==============================] - 546s 468ms/step - loss: 1.2582e-04 - acc: 1.0000 - f1: 0.5629 - val_loss: 0.3585 - val_acc: 0.9579 - val_f1: 0.2673\n",
      "\n",
      "Epoch 00098: val_f1 improved from 0.26589 to 0.26725, saving model to ../working/InceptionV3+1x1.h5\n",
      "Epoch 99/100\n",
      "1166/1165 [==============================] - 540s 463ms/step - loss: 1.0176e-04 - acc: 1.0000 - f1: 0.5597 - val_loss: 0.3744 - val_acc: 0.9576 - val_f1: 0.2603\n",
      "\n",
      "Epoch 00099: val_f1 did not improve from 0.26725\n",
      "Epoch 100/100\n",
      "1166/1165 [==============================] - 579s 497ms/step - loss: 1.0217e-04 - acc: 1.0000 - f1: 0.5599 - val_loss: 0.3732 - val_acc: 0.9577 - val_f1: 0.2608\n",
      "\n",
      "Epoch 00100: val_f1 did not improve from 0.26725\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 12\n",
    "hist =  model.fit_generator(\n",
    "        tg,\n",
    "        steps_per_epoch=np.ceil(float(len(pathsTrain)) / float(batch_size))/2,\n",
    "        validation_data=vg,\n",
    "        validation_steps=np.ceil(float(len(pathsVal)) / float(batch_size))/2,\n",
    "        epochs=100, \n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=f1_loss,\n",
    "#             optimizer=Adam(lr=1e-4),\n",
    "#             metrics=['accuracy', f1])\n",
    "# hist =  model.fit_generator(\n",
    "#         tg,\n",
    "#         steps_per_epoch=np.ceil(float(len(pathsTrain)) / float(batch_size))/2,\n",
    "#         validation_data=vg,\n",
    "#         validation_steps=np.ceil(float(len(pathsVal)) / float(batch_size))/2,\n",
    "#         epochs=200, \n",
    "#         verbose=1,\n",
    "#         callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f54ee83780>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].set_title('loss')\n",
    "ax[0].plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\n",
    "ax[0].plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax[1].set_title('acc')\n",
    "ax[1].plot(hist.epoch, hist.history[\"f1\"], label=\"Train F1\")\n",
    "ax[1].plot(hist.epoch, hist.history[\"val_f1\"], label=\"Validation F1\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "bestModel = load_model('../working/InceptionV3.h5', custom_objects={'f1': f1, 'f1_loss': f1_loss, 'focal_loss_fixed':focal_loss()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "lastFullValPred = np.empty((0, 28))\n",
    "lastFullValLabels = np.empty((0, 28))\n",
    "for i in tqdm(range(len(vg))): \n",
    "    im, lbl = vg[i]\n",
    "    scores = bestModel.predict(im)\n",
    "    lastFullValPred = np.append(lastFullValPred, scores, axis=0)\n",
    "    lastFullValLabels = np.append(lastFullValLabels, lbl, axis=0)\n",
    "print(lastFullValPred.shape, lastFullValLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score as off1\n",
    "rng = np.arange(0, 1, 0.001)\n",
    "f1s = np.zeros((rng.shape[0], 28))\n",
    "for j,t in enumerate(tqdm(rng)):\n",
    "    for i in range(28):\n",
    "        p = np.array(lastFullValPred[:,i]>t, dtype=np.int8)\n",
    "        scoref1 = off1(lastFullValLabels[:,i], p, average='binary')\n",
    "        f1s[j,i] = scoref1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Individual F1-scores for each class:')\n",
    "print(np.max(f1s, axis=0))\n",
    "print('Macro F1-score CV =', np.mean(np.max(f1s, axis=0)))\n",
    "plt.plot(rng, f1s)\n",
    "T = np.empty(28)\n",
    "for i in range(28):\n",
    "    T[i] = rng[np.where(f1s[:,i] == np.max(f1s[:,i]))[0][0]]\n",
    "print('Probability threshold maximizing CV F1-score for each class:')\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathsTest, labelsTest = getTestDataset()\n",
    "\n",
    "testg = ProteinDataGenerator(pathsTest, labelsTest, batch_size, SHAPE, channels)\n",
    "submit = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "P = np.zeros((pathsTest.shape[0], 28))\n",
    "for i in tqdm(range(len(testg))):\n",
    "    images, labels = testg[i]\n",
    "    score = bestModel.predict(images)\n",
    "    P[i*batch_size:i*batch_size+score.shape[0]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP = np.array(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "\n",
    "for row in tqdm(range(submit.shape[0])):\n",
    "    \n",
    "    str_label = ''\n",
    "    \n",
    "    for col in range(PP.shape[1]):\n",
    "        if(PP[row, col] < T[col]):\n",
    "            str_label += ''\n",
    "        else:\n",
    "            str_label += str(col) + ' '\n",
    "    prediction.append(str_label.strip())\n",
    "    \n",
    "submit['Predicted'] = np.array(prediction)\n",
    "submit.to_csv('transfer_1x1conv_aug_focal_loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# pathsTest, labelsTest = getTestDataset()\n",
    "\n",
    "# testg = ProteinDataGenerator(pathsTest, labelsTest, batch_size, SHAPE)\n",
    "# submit = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "# P = np.zeros((pathsTest.shape[0], 28))\n",
    "# for i in tqdm(range(len(testg))):\n",
    "#     images, labels = testg[i]\n",
    "#     score = bestModel.predict(images)\n",
    "#     P[i*batch_size:i*batch_size+score.shape[0]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PP = np.array(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = []\n",
    "\n",
    "# for row in tqdm(range(submit.shape[0])):\n",
    "    \n",
    "#     str_label = ''\n",
    "    \n",
    "#     for col in range(PP.shape[1]):\n",
    "#         if(PP[row, col] < .2):   # to account for losing TP is more costly than decreasing FP\n",
    "#             #print(PP[row])\n",
    "#             str_label += ''\n",
    "#         else:\n",
    "#             str_label += str(col) + ' '\n",
    "#     prediction.append(str_label.strip())\n",
    "    \n",
    "# submit['Predicted'] = np.array(prediction)\n",
    "# submit.to_csv('datagenerator_model_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
