{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "import keras\n",
    "import warnings\n",
    "from keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 299\n",
    "SEED = 777\n",
    "THRESHOLD = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info\n",
    "DIR = '../input/'\n",
    "data = pd.read_csv('../input/train.csv')\n",
    "\n",
    "# train_dataset_info = []\n",
    "# for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "#     train_dataset_info.append({\n",
    "#         'path':os.path.join(path_to_train, name),\n",
    "#         'labels':np.array([int(label) for label in labels])})\n",
    "# train_dataset_info = np.array(train_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainDataset():\n",
    "    \n",
    "    path_to_train = DIR + '/train/'\n",
    "    data = pd.read_csv(DIR + '/train.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name, lbl in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "        y = np.zeros(28)\n",
    "        for key in lbl:\n",
    "            y[int(key)] = 1\n",
    "        paths.append(os.path.join(path_to_train, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)\n",
    "\n",
    "def getTestDataset():\n",
    "    \n",
    "    path_to_test = DIR + '/test/'\n",
    "    data = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name in data['Id']:\n",
    "        y = np.ones(28)\n",
    "        paths.append(os.path.join(path_to_test, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)\n",
    "paths, labels = getTrainDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credits: https://github.com/keras-team/keras/blob/master/keras/utils/data_utils.py#L302\n",
    "# credits: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "from random import randint\n",
    "class ProteinDataGenerator(keras.utils.Sequence):\n",
    "            \n",
    "    def __init__(self, paths, labels, batch_size, shape, channels = [], shuffle = False, use_cache = False, augmentor = False):\n",
    "        self.paths, self.labels = paths, labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shape = shape\n",
    "        self.shuffle = shuffle\n",
    "        self.use_cache = use_cache\n",
    "        self.channels = channels\n",
    "        self.augmentor = augmentor\n",
    "        self.clahe = cv2.createCLAHE()\n",
    "        if use_cache == True:\n",
    "            self.cache = np.zeros((paths.shape[0], shape[0], shape[1], len(channels)))\n",
    "            self.is_cached = np.zeros((paths.shape[0]))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "\n",
    "        paths = self.paths[indexes]\n",
    "        X = np.zeros((paths.shape[0], self.shape[0], self.shape[1], self.shape[2]))\n",
    "        # Generate data\n",
    "        if self.use_cache == True:\n",
    "            X = self.cache[indexes]\n",
    "            for i, path in enumerate(paths[np.where(self.is_cached[indexes] == 0)]):\n",
    "                image = self.__load_image(path)\n",
    "                self.is_cached[indexes[i]] = 1\n",
    "                self.cache[indexes[i]] = image\n",
    "                X[i] = image\n",
    "        else:\n",
    "            for i, path in enumerate(paths):\n",
    "                X[i] = self.__load_image(path)\n",
    "        if self.augmentor == True:\n",
    "            for i, item in enumerate(X):\n",
    "                X[i] = self.augment(item)\n",
    "        y = self.labels[indexes]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        # Updates indexes after each epoch\n",
    "        self.indexes = np.arange(len(self.paths))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "        for item in (self[i] for i in range(len(self))):\n",
    "            yield item\n",
    "            \n",
    "    def __load_image(self, path):\n",
    "        images = []\n",
    "        for channel in self.channels:\n",
    "            im = np.array(Image.open(path + '_' + channel + '.png'))\n",
    "            \n",
    "#             im = clahe.apply(im)\n",
    "            images.append(im)\n",
    "            \n",
    "        if len(self.channels) >= 2:\n",
    "            im = np.stack((\n",
    "                images\n",
    "            ), -1)\n",
    "            im = cv2.resize(im, (SIZE,SIZE))\n",
    "            im = np.divide(im, 255)\n",
    "\n",
    "        else:\n",
    "            im = images[0]\n",
    "            im = cv2.resize(im, (SIZE,SIZE))\n",
    "            im = np.divide(im, 255)\n",
    "            im = np.expand_dims(im, 2)\n",
    "        return im\n",
    "    def augment(self, image):\n",
    "        if randint(0,1) == 1:\n",
    "            augment_img = iaa.Sequential([\n",
    "                iaa.OneOf([\n",
    "                    iaa.Fliplr(0.5), # horizontal flips\n",
    "                    iaa.Flipud(0.5), # horizontal flips\n",
    "                    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "                    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "                    # But we only blur about 50% of all images.\n",
    "                    iaa.Sometimes(0.5,\n",
    "                        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "                    ),\n",
    "                    # Make some images brighter and some darker.\n",
    "                    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "                    # which can end up changing the color of the images.\n",
    "                    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "                    # Apply affine transformations to each image.\n",
    "                    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "                    iaa.Affine(\n",
    "                        scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)},\n",
    "                        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
    "                        rotate=(-180, 180),\n",
    "                        shear=(-4, 4)\n",
    "                    )\n",
    "                ])], random_order=True)\n",
    "\n",
    "\n",
    "            image_aug = augment_img.augment_image(image)\n",
    "            return image_aug\n",
    "        else:\n",
    "            return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPE = (299, 299, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n"
     ]
    }
   ],
   "source": [
    "channels = [\"red\"]\n",
    "for path in paths[0:10]:\n",
    "    images = []\n",
    "    for channel in channels:\n",
    "        print(channel)\n",
    "        images.append(np.array(Image.open(path + '_' + channel + '.png')))\n",
    "\n",
    "    if len(channels) >= 2:\n",
    "        im = np.stack((\n",
    "            images\n",
    "        ), -1)\n",
    "        im = cv2.resize(im, (SIZE,SIZE))\n",
    "        im = np.divide(im, 255)\n",
    "        \n",
    "    else:\n",
    "        im = images[0]\n",
    "        im = cv2.resize(im, (SIZE,SIZE))\n",
    "        im = np.divide(im, 255)\n",
    "        im = np.expand_dims(im, 2)\n",
    "    print(im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class data_generator:\n",
    "    \n",
    "#     def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "#         assert shape[2] == 3\n",
    "#         while True:\n",
    "#             dataset_info = shuffle(dataset_info)\n",
    "#             for start in range(0, len(dataset_info), batch_size):\n",
    "#                 end = min(start + batch_size, len(dataset_info))\n",
    "#                 batch_images = []\n",
    "#                 X_train_batch = dataset_info[start:end]\n",
    "#                 batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "#                 for i in range(len(X_train_batch)):\n",
    "#                     image = data_generator.load_image(\n",
    "#                         X_train_batch[i]['path'], shape)   \n",
    "#                     if augument:\n",
    "#                         image = data_generator.augment(image)\n",
    "#                     batch_images.append(image/255.)\n",
    "#                     batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "#                 yield np.array(batch_images, np.float32), batch_labels\n",
    "\n",
    "#     def load_image(path, shape):\n",
    "#         image_red_ch = Image.open(path+'_red.png')\n",
    "#         image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "#         image_green_ch = Image.open(path+'_green.png')\n",
    "#         image_blue_ch = Image.open(path+'_blue.png')\n",
    "#         image = np.stack((\n",
    "#         np.array(image_red_ch), \n",
    "#         np.array(image_green_ch), \n",
    "#         np.array(image_blue_ch)), -1)\n",
    "#         image = cv2.resize(image, (shape[0], shape[1]))\n",
    "#         return image\n",
    "\n",
    "#     def augment(image):\n",
    "#         augment_img = iaa.Sequential([\n",
    "#             iaa.OneOf([\n",
    "#                 iaa.Affine(rotate=0),\n",
    "#                 iaa.Affine(rotate=90),\n",
    "#                 iaa.Affine(rotate=180),\n",
    "#                 iaa.Affine(rotate=270),\n",
    "#                 iaa.Fliplr(0.5),\n",
    "#                 iaa.Flipud(0.5),\n",
    "#             ])], random_order=True)\n",
    "\n",
    "#         image_aug = augment_img.augment_image(image)\n",
    "#         return image_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D, MaxPooling2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    #y_pred = K.round(y_pred)\n",
    "    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    #y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1-K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# f = h5py.File(\"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\", 'r')\n",
    "# dset = f[\"conv2d_1\"]\n",
    "# list(dset.keys())\n",
    "# group = dset[\"conv2d_1\"]\n",
    "# list(group.keys())\n",
    "# first_lay = group['kernel:0'].value\n",
    "# print(first_lay.shape)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros = np.zeros(shape = (3,3,1,32))\n",
    "# print(zeros.shape)\n",
    "# first_lay1 = np.concatenate([zeros, first_lay], axis = 2)\n",
    "# first_lay.shape\n",
    "# f = h5py.File(\"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\", 'r+')\n",
    "# dset = f[\"conv2d_1\"]\n",
    "# list(dset.keys())\n",
    "# group = dset[\"conv2d_1\"]\n",
    "# list(group.keys())\n",
    "# del group['kernel:0']\n",
    "# group['kernel:0'] = first_lay1\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.clear_session()\n",
    "# # -*- coding: utf-8 -*-\n",
    "# \"\"\"Inception V3 model for Keras.\n",
    "# Note that the input image format for this model is different than for\n",
    "# the VGG16 and ResNet models (299x299 instead of 224x224),\n",
    "# and that the input preprocessing function is also different (same as Xception).\n",
    "# # Reference\n",
    "# - [Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567)\n",
    "# \"\"\"\n",
    "# from __future__ import print_function\n",
    "# from __future__ import absolute_import\n",
    "\n",
    "# import warnings\n",
    "# import numpy as np\n",
    "\n",
    "# from keras.models import Model\n",
    "# from keras import layers\n",
    "# from keras.layers import Activation\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Input\n",
    "# from keras.layers import BatchNormalization\n",
    "# from keras.layers import Conv2D\n",
    "# from keras.layers import MaxPooling2D\n",
    "# from keras.layers import AveragePooling2D\n",
    "# from keras.layers import GlobalAveragePooling2D\n",
    "# from keras.layers import GlobalMaxPooling2D\n",
    "# from keras.engine.topology import get_source_inputs\n",
    "# from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "# from keras.utils.data_utils import get_file\n",
    "# from keras import backend as K\n",
    "# from keras.applications.imagenet_utils import decode_predictions\n",
    "# from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "# from keras.preprocessing import image\n",
    "\n",
    "\n",
    "# WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "# WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "\n",
    "# def conv2d_bn(x,\n",
    "#               filters,\n",
    "#               num_row,\n",
    "#               num_col,\n",
    "#               padding='same',\n",
    "#               strides=(1, 1),\n",
    "#               name=None):\n",
    "#     \"\"\"Utility function to apply conv + BN.\n",
    "#     Arguments:\n",
    "#         x: input tensor.\n",
    "#         filters: filters in `Conv2D`.\n",
    "#         num_row: height of the convolution kernel.\n",
    "#         num_col: width of the convolution kernel.\n",
    "#         padding: padding mode in `Conv2D`.\n",
    "#         strides: strides in `Conv2D`.\n",
    "#         name: name of the ops; will become `name + '_conv'`\n",
    "#             for the convolution and `name + '_bn'` for the\n",
    "#             batch norm layer.\n",
    "#     Returns:\n",
    "#         Output tensor after applying `Conv2D` and `BatchNormalization`.\n",
    "#     \"\"\"\n",
    "#     if name is not None:\n",
    "#         bn_name = name + '_bn'\n",
    "#         conv_name = name + '_conv'\n",
    "#     else:\n",
    "#         bn_name = None\n",
    "#         conv_name = None\n",
    "#     if K.image_data_format() == 'channels_first':\n",
    "#         bn_axis = 1\n",
    "#     else:\n",
    "#         bn_axis = 3\n",
    "#     x = Conv2D(\n",
    "#         filters, (num_row, num_col),\n",
    "#         strides=strides,\n",
    "#         padding=padding,\n",
    "#         use_bias=False,\n",
    "#         name=conv_name)(x)\n",
    "#     x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n",
    "#     x = Activation('relu', name=name)(x)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def InceptionV3(include_top=True,\n",
    "#                 weights='imagenet',\n",
    "#                 input_tensor=None,\n",
    "#                 input_shape=None,\n",
    "#                 pooling=None,\n",
    "#                 classes=1000):\n",
    "#     \"\"\"Instantiates the Inception v3 architecture.\n",
    "#     Optionally loads weights pre-trained\n",
    "#     on ImageNet. Note that when using TensorFlow,\n",
    "#     for best performance you should set\n",
    "#     `image_data_format=\"channels_last\"` in your Keras config\n",
    "#     at ~/.keras/keras.json.\n",
    "#     The model and the weights are compatible with both\n",
    "#     TensorFlow and Theano. The data format\n",
    "#     convention used by the model is the one\n",
    "#     specified in your Keras config file.\n",
    "#     Note that the default input image size for this model is 299x299.\n",
    "#     Arguments:\n",
    "#         include_top: whether to include the fully-connected\n",
    "#             layer at the top of the network.\n",
    "#         weights: one of `None` (random initialization)\n",
    "#             or \"imagenet\" (pre-training on ImageNet).\n",
    "#         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "#             to use as image input for the model.\n",
    "#         input_shape: optional shape tuple, only to be specified\n",
    "#             if `include_top` is False (otherwise the input shape\n",
    "#             has to be `(299, 299, 3)` (with `channels_last` data format)\n",
    "#             or `(3, 299, 299)` (with `channels_first` data format).\n",
    "#             It should have exactly 3 inputs channels,\n",
    "#             and width and height should be no smaller than 139.\n",
    "#             E.g. `(150, 150, 3)` would be one valid value.\n",
    "#         pooling: Optional pooling mode for feature extraction\n",
    "#             when `include_top` is `False`.\n",
    "#             - `None` means that the output of the model will be\n",
    "#                 the 4D tensor output of the\n",
    "#                 last convolutional layer.\n",
    "#             - `avg` means that global average pooling\n",
    "#                 will be applied to the output of the\n",
    "#                 last convolutional layer, and thus\n",
    "#                 the output of the model will be a 2D tensor.\n",
    "#             - `max` means that global max pooling will\n",
    "#                 be applied.\n",
    "#         classes: optional number of classes to classify images\n",
    "#             into, only to be specified if `include_top` is True, and\n",
    "#             if no `weights` argument is specified.\n",
    "#     Returns:\n",
    "#         A Keras model instance.\n",
    "#     Raises:\n",
    "#         ValueError: in case of invalid argument for `weights`,\n",
    "#             or invalid input shape.\n",
    "#     \"\"\"\n",
    "#     if weights not in {'imagenet', None}:\n",
    "#         raise ValueError('The `weights` argument should be either '\n",
    "#                          '`None` (random initialization) or `imagenet` '\n",
    "#                          '(pre-training on ImageNet).')\n",
    "\n",
    "#     if weights == 'imagenet' and include_top and classes != 1000:\n",
    "#         raise ValueError('If using `weights` as imagenet with `include_top`'\n",
    "#                          ' as true, `classes` should be 1000')\n",
    "\n",
    "#     # Determine proper input shape\n",
    "#     input_shape = (299,299,4)\n",
    "#     if input_tensor is None:\n",
    "#         img_input = Input(shape=input_shape)\n",
    "#     else:\n",
    "#         img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "\n",
    "#     if K.image_data_format() == 'channels_first':\n",
    "#         channel_axis = 1\n",
    "#     else:\n",
    "#         channel_axis = 3\n",
    "\n",
    "#     x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding='valid')\n",
    "#     x = conv2d_bn(x, 32, 3, 3, padding='valid')\n",
    "#     x = conv2d_bn(x, 64, 3, 3)\n",
    "#     x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "#     x = conv2d_bn(x, 80, 1, 1, padding='valid')\n",
    "#     x = conv2d_bn(x, 192, 3, 3, padding='valid')\n",
    "#     x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "#     # mixed 0, 1, 2: 35 x 35 x 256\n",
    "#     branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "#     branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "#     branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "#     branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "#     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "#     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "#     branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "#     branch_pool = conv2d_bn(branch_pool, 32, 1, 1)\n",
    "#     x = layers.concatenate(\n",
    "#         [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "#         axis=channel_axis,\n",
    "#         name='mixed0')\n",
    "\n",
    "#     # mixed 1: 35 x 35 x 256\n",
    "#     branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "#     branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "#     branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "#     branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "#     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "#     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "#     branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "#     branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n",
    "#     x = layers.concatenate(\n",
    "#         [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "#         axis=channel_axis,\n",
    "#         name='mixed1')\n",
    "\n",
    "#     # mixed 2: 35 x 35 x 256\n",
    "#     branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "#     branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "#     branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "#     branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "#     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "#     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "#     branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "#     branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n",
    "#     x = layers.concatenate(\n",
    "#         [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "#         axis=channel_axis,\n",
    "#         name='mixed2')\n",
    "\n",
    "#     # mixed 3: 17 x 17 x 768\n",
    "#     branch3x3 = conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "#     branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "#     branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "#     branch3x3dbl = conv2d_bn(\n",
    "#         branch3x3dbl, 96, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "#     branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "#     x = layers.concatenate(\n",
    "#         [branch3x3, branch3x3dbl, branch_pool], axis=channel_axis, name='mixed3')\n",
    "\n",
    "#     # mixed 4: 17 x 17 x 768\n",
    "#     branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "#     branch7x7 = conv2d_bn(x, 128, 1, 1)\n",
    "#     branch7x7 = conv2d_bn(branch7x7, 128, 1, 7)\n",
    "#     branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "#     branch7x7dbl = conv2d_bn(x, 128, 1, 1)\n",
    "#     branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "#     branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 1, 7)\n",
    "#     branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "#     branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "#     branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "#     branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "#     x = layers.concatenate(\n",
    "#         [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "#         axis=channel_axis,\n",
    "#         name='mixed4')\n",
    "\n",
    "#     # mixed 5, 6: 17 x 17 x 768\n",
    "#     for i in range(2):\n",
    "#         branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "#         branch7x7 = conv2d_bn(x, 160, 1, 1)\n",
    "#         branch7x7 = conv2d_bn(branch7x7, 160, 1, 7)\n",
    "#         branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "#         branch7x7dbl = conv2d_bn(x, 160, 1, 1)\n",
    "#         branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "#         branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 1, 7)\n",
    "#         branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "#         branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "#         branch_pool = AveragePooling2D(\n",
    "#             (3, 3), strides=(1, 1), padding='same')(x)\n",
    "#         branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "#         x = layers.concatenate(\n",
    "#             [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "#             axis=channel_axis,\n",
    "#             name='mixed' + str(5 + i))\n",
    "\n",
    "#     # mixed 7: 17 x 17 x 768\n",
    "#     branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "#     branch7x7 = conv2d_bn(x, 192, 1, 1)\n",
    "#     branch7x7 = conv2d_bn(branch7x7, 192, 1, 7)\n",
    "#     branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "#     branch7x7dbl = conv2d_bn(x, 192, 1, 1)\n",
    "#     branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "#     branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "#     branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "#     branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "#     branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "#     branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "#     x = layers.concatenate(\n",
    "#         [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "#         axis=channel_axis,\n",
    "#         name='mixed7')\n",
    "\n",
    "#     # mixed 8: 8 x 8 x 1280\n",
    "#     branch3x3 = conv2d_bn(x, 192, 1, 1)\n",
    "#     branch3x3 = conv2d_bn(branch3x3, 320, 3, 3,\n",
    "#                           strides=(2, 2), padding='valid')\n",
    "\n",
    "#     branch7x7x3 = conv2d_bn(x, 192, 1, 1)\n",
    "#     branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)\n",
    "#     branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)\n",
    "#     branch7x7x3 = conv2d_bn(\n",
    "#         branch7x7x3, 192, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "#     branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "#     x = layers.concatenate(\n",
    "#         [branch3x3, branch7x7x3, branch_pool], axis=channel_axis, name='mixed8')\n",
    "\n",
    "#     # mixed 9: 8 x 8 x 2048\n",
    "#     for i in range(2):\n",
    "#         branch1x1 = conv2d_bn(x, 320, 1, 1)\n",
    "\n",
    "#         branch3x3 = conv2d_bn(x, 384, 1, 1)\n",
    "#         branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)\n",
    "#         branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)\n",
    "#         branch3x3 = layers.concatenate(\n",
    "#             [branch3x3_1, branch3x3_2], axis=channel_axis, name='mixed9_' + str(i))\n",
    "\n",
    "#         branch3x3dbl = conv2d_bn(x, 448, 1, 1)\n",
    "#         branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)\n",
    "#         branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)\n",
    "#         branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)\n",
    "#         branch3x3dbl = layers.concatenate(\n",
    "#             [branch3x3dbl_1, branch3x3dbl_2], axis=channel_axis)\n",
    "\n",
    "#         branch_pool = AveragePooling2D(\n",
    "#             (3, 3), strides=(1, 1), padding='same')(x)\n",
    "#         branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "#         x = layers.concatenate(\n",
    "#             [branch1x1, branch3x3, branch3x3dbl, branch_pool],\n",
    "#             axis=channel_axis,\n",
    "#             name='mixed' + str(9 + i))\n",
    "#     if include_top:\n",
    "#         # Classification block\n",
    "#         x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "#         x = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "#     else:\n",
    "#         if pooling == 'avg':\n",
    "#             x = GlobalAveragePooling2D()(x)\n",
    "#         elif pooling == 'max':\n",
    "#             x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "#     # Ensure that the model takes into account\n",
    "#     # any potential predecessors of `input_tensor`.\n",
    "#     if input_tensor is not None:\n",
    "#         inputs = get_source_inputs(input_tensor)\n",
    "#     else:\n",
    "#         inputs = img_input\n",
    "#     # Create model.\n",
    "#     model = Model(inputs, x, name='inception_v3')\n",
    "\n",
    "#     # load weights\n",
    "#     if weights == 'imagenet':\n",
    "#         if K.image_data_format() == 'channels_first':\n",
    "#             if K.backend() == 'tensorflow':\n",
    "#                 warnings.warn('You are using the TensorFlow backend, yet you '\n",
    "#                               'are using the Theano '\n",
    "#                               'image data format convention '\n",
    "#                               '(`image_data_format=\"channels_first\"`). '\n",
    "#                               'For best performance, set '\n",
    "#                               '`image_data_format=\"channels_last\"` in '\n",
    "#                               'your Keras config '\n",
    "#                               'at ~/.keras/keras.json.')\n",
    "#         if include_top:\n",
    "#             weights_path = get_file(\n",
    "#                 'inception_v3_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "#                 WEIGHTS_PATH,\n",
    "#                 cache_subdir='models',\n",
    "#                 md5_hash='9a0d58056eeedaa3f26cb7ebd46da564')\n",
    "#         else:\n",
    "#             weights_path = get_file(\n",
    "#                 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "#                 WEIGHTS_PATH_NO_TOP,\n",
    "#                 cache_subdir='models',\n",
    "#                 md5_hash='bcbd6486424b2319ff4ef7d526e38f63')\n",
    "#             weights_path = \"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "#         model.load_weights(weights_path)\n",
    "#         if K.backend() == 'theano':\n",
    "#             convert_all_kernels_in_model(model)\n",
    "#     return model\n",
    "# if __name__ == '__main__':\n",
    "#     model = InceptionV3(include_top=False, weights='imagenet')\n",
    "#     model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out, channels):\n",
    "    input_tensor = Input(shape=(299,299,len(channels)))\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    base_model = InceptionV3(include_top=False, weights='imagenet')\n",
    "    x = base_model(bn)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Conv2D(128, kernel_size=(3,3), activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 299, 299, 3)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_191 (Bat (None, 299, 299, 3)       12        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         multiple                  21802784  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 2048)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_284 (Conv2D)          (None, 6, 6, 128)         2359424   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 28)                28700     \n",
      "=================================================================\n",
      "Total params: 28,910,536\n",
      "Trainable params: 28,876,098\n",
      "Non-trainable params: 34,438\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "channels = [\"green\", \"blue\", \"red\"]\n",
    "# with tf.device('/cpu:0'):\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,3), \n",
    "    n_out=28, channels = channels)\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=Adam(1e-04),\n",
    "    metrics=['acc', f1])\n",
    "model.summary()\n",
    "\n",
    "# model.layers[3].layers[1] = Conv2D(32, kernel_size = (7,7), strides = (2,2), padding = \"same\", input_shape = (299, 299, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31072,) (31072, 28)\n",
      "(27964,) (27964, 28) (3108,) (3108, 28)\n"
     ]
    }
   ],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "epochs = 10; batch_size = 32;VAL_RATIO = .1;DEBUG = False\n",
    "# split data into train, valid\n",
    "paths, labels = getTrainDataset()\n",
    "\n",
    "# divide to \n",
    "keys = np.arange(paths.shape[0], dtype=np.int)  \n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(keys)\n",
    "lastTrainIndex = int((1-VAL_RATIO) * paths.shape[0])\n",
    "\n",
    "if DEBUG == True:  # use only small subset for debugging, Kaggle's RAM is limited\n",
    "    pathsTrain = paths[0:256]\n",
    "    labelsTrain = labels[0:256]\n",
    "    pathsVal = paths[lastTrainIndex:lastTrainIndex+256]\n",
    "    labelsVal = labels[lastTrainIndex:lastTrainIndex+256]\n",
    "    use_cache = True\n",
    "else:\n",
    "    pathsTrain = paths[0:lastTrainIndex]\n",
    "    labelsTrain = labels[0:lastTrainIndex]\n",
    "    pathsVal = paths[lastTrainIndex:]\n",
    "    labelsVal = labels[lastTrainIndex:]\n",
    "    use_cache = False\n",
    "\n",
    "print(paths.shape, labels.shape)\n",
    "print(pathsTrain.shape, labelsTrain.shape, pathsVal.shape, labelsVal.shape)\n",
    "use_cache = True\n",
    "tg = ProteinDataGenerator(pathsTrain, labelsTrain, batch_size, SHAPE, channels, use_cache=use_cache)\n",
    "vg = ProteinDataGenerator(pathsVal, labelsVal, batch_size, SHAPE, channels, use_cache=use_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and valid datagens\n",
    "# train_generator = data_generator.create_train(\n",
    "#     train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "# validation_generator = data_generator.create_train(\n",
    "#     train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('../working/InceptionV3_3chan.h5', monitor='val_f1', verbose=1, \n",
    "                             save_best_only=True, mode='max', save_weights_only = False)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_f1', factor=0.5, patience=10, \n",
    "                                   verbose=1, mode='max', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_f1\", \n",
    "                      mode=\"max\", \n",
    "                      patience=20)\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 299, 299, 3)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_286 (Bat (None, 299, 299, 3)       12        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         multiple                  21802784  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8, 8, 2048)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_379 (Conv2D)          (None, 6, 6, 128)         2359424   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 28)                28700     \n",
      "=================================================================\n",
      "Total params: 28,910,536\n",
      "Trainable params: 7,107,746\n",
      "Non-trainable params: 21,802,790\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# warm up model\n",
    "import tensorflow as tf\n",
    "channels = [\"green\", \"blue\", \"red\"]\n",
    "# with tf.device('/cpu:0'):\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,4), \n",
    "    n_out=28, channels = channels)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "model.layers[0].trainable = True\n",
    "model.layers[1].trainable = True\n",
    "model.layers[-1].trainable = True\n",
    "model.layers[-2].trainable = True\n",
    "model.layers[-3].trainable = True\n",
    "model.layers[-4].trainable = True\n",
    "model.layers[-5].trainable = True\n",
    "model.layers[-6].trainable = True\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tg = ProteinDataGenerator(pathsTrain, labelsTrain, batch_size, SHAPE, channels, use_cache=use_cache)\n",
    "vg = ProteinDataGenerator(pathsVal, labelsVal, batch_size, SHAPE, channels, use_cache=use_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "437/437 [==============================] - 287s 657ms/step - loss: 0.2093 - acc: 0.9329 - f1: 0.0664 - val_loss: 0.2109 - val_acc: 0.9392 - val_f1: 0.0688\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.06883, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 2/20\n",
      "437/437 [==============================] - 298s 683ms/step - loss: 0.1808 - acc: 0.9413 - f1: 0.0719 - val_loss: 0.2188 - val_acc: 0.9393 - val_f1: 0.0820\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.06883 to 0.08197, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 3/20\n",
      "437/437 [==============================] - 135s 310ms/step - loss: 0.1729 - acc: 0.9432 - f1: 0.0856 - val_loss: 0.2179 - val_acc: 0.9410 - val_f1: 0.0887\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.08197 to 0.08874, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 4/20\n",
      "437/437 [==============================] - 136s 312ms/step - loss: 0.1711 - acc: 0.9433 - f1: 0.0909 - val_loss: 0.2313 - val_acc: 0.9394 - val_f1: 0.0968\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.08874 to 0.09676, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 5/20\n",
      "437/437 [==============================] - 136s 311ms/step - loss: 0.1656 - acc: 0.9451 - f1: 0.0973 - val_loss: 0.2395 - val_acc: 0.9382 - val_f1: 0.1011\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.09676 to 0.10108, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 6/20\n",
      "437/437 [==============================] - 135s 309ms/step - loss: 0.1651 - acc: 0.9447 - f1: 0.1020 - val_loss: 0.2252 - val_acc: 0.9331 - val_f1: 0.0954\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.10108\n",
      "Epoch 7/20\n",
      "437/437 [==============================] - 135s 308ms/step - loss: 0.1617 - acc: 0.9460 - f1: 0.1078 - val_loss: 0.2014 - val_acc: 0.9367 - val_f1: 0.0851\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.10108\n",
      "Epoch 8/20\n",
      "437/437 [==============================] - 130s 297ms/step - loss: 0.1612 - acc: 0.9458 - f1: 0.1122 - val_loss: 0.2331 - val_acc: 0.9335 - val_f1: 0.0983\n",
      "\n",
      "Epoch 00008: val_f1 did not improve from 0.10108\n",
      "Epoch 9/20\n",
      "437/437 [==============================] - 124s 283ms/step - loss: 0.1573 - acc: 0.9469 - f1: 0.1197 - val_loss: 0.2107 - val_acc: 0.9299 - val_f1: 0.0825\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.10108\n",
      "Epoch 10/20\n",
      " 26/437 [>.............................] - ETA: 1:49 - loss: 0.1596 - acc: 0.9462 - f1: 0.1176"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-21c35fc7265d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         callbacks = callbacks_list)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1426\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1220\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1221\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist =  model.fit_generator(\n",
    "        tg,\n",
    "        steps_per_epoch=np.ceil(float(len(pathsTrain)) / float(batch_size))/2,\n",
    "        validation_data=vg,\n",
    "        validation_steps=np.ceil(float(len(pathsVal)) / float(batch_size))/2,\n",
    "        epochs=20, \n",
    "        verbose=1,\n",
    "        callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x0000025B9020A0B8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000025B9020AB38>\n",
      "<keras.engine.training.Model object at 0x0000024D9414DA90>\n",
      "<keras.layers.core.Dropout object at 0x0000024D9414D748>\n",
      "<keras.layers.convolutional.Conv2D object at 0x0000024D99D8C898>\n",
      "<keras.layers.core.Flatten object at 0x0000024D94D6C828>\n",
      "<keras.layers.core.Dropout object at 0x0000024D99E2FA58>\n",
      "<keras.layers.core.Dense object at 0x0000024D989C3048>\n",
      "<keras.layers.core.Dropout object at 0x0000024D989355C0>\n",
      "<keras.layers.core.Dense object at 0x0000024D98935198>\n"
     ]
    }
   ],
   "source": [
    "# train all layers\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    layer.trainable = True\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1166/1165 [==============================] - 504s 432ms/step - loss: 0.1403 - acc: 0.9532 - f1: 0.1796 - val_loss: 0.1327 - val_acc: 0.9558 - val_f1: 0.2322\n",
      "\n",
      "Epoch 00001: val_f1 improved from 0.10108 to 0.23221, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 2/100\n",
      "1166/1165 [==============================] - 464s 398ms/step - loss: 0.1127 - acc: 0.9616 - f1: 0.2657 - val_loss: 0.1228 - val_acc: 0.9573 - val_f1: 0.2618\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.23221 to 0.26179, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 3/100\n",
      "1166/1165 [==============================] - 463s 397ms/step - loss: 0.0945 - acc: 0.9675 - f1: 0.3159 - val_loss: 0.1248 - val_acc: 0.9574 - val_f1: 0.2696\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.26179 to 0.26959, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 4/100\n",
      "1166/1165 [==============================] - 462s 396ms/step - loss: 0.0708 - acc: 0.9757 - f1: 0.3728 - val_loss: 0.1190 - val_acc: 0.9612 - val_f1: 0.2983\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.26959 to 0.29833, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 5/100\n",
      "1166/1165 [==============================] - 461s 396ms/step - loss: 0.0570 - acc: 0.9803 - f1: 0.4088 - val_loss: 0.1426 - val_acc: 0.9574 - val_f1: 0.2821\n",
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.29833\n",
      "Epoch 6/100\n",
      "1166/1165 [==============================] - 462s 396ms/step - loss: 0.0447 - acc: 0.9845 - f1: 0.4411 - val_loss: 0.1408 - val_acc: 0.9621 - val_f1: 0.2960\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.29833\n",
      "Epoch 7/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 0.0340 - acc: 0.9882 - f1: 0.4745 - val_loss: 0.1681 - val_acc: 0.9597 - val_f1: 0.2995\n",
      "\n",
      "Epoch 00007: val_f1 improved from 0.29833 to 0.29954, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 8/100\n",
      "1166/1165 [==============================] - 458s 393ms/step - loss: 0.0283 - acc: 0.9901 - f1: 0.4851 - val_loss: 0.1459 - val_acc: 0.9613 - val_f1: 0.3062\n",
      "\n",
      "Epoch 00008: val_f1 improved from 0.29954 to 0.30621, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 9/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 0.0248 - acc: 0.9916 - f1: 0.4993 - val_loss: 0.1799 - val_acc: 0.9598 - val_f1: 0.2892\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.30621\n",
      "Epoch 10/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 0.0207 - acc: 0.9930 - f1: 0.5081 - val_loss: 0.1610 - val_acc: 0.9581 - val_f1: 0.3128\n",
      "\n",
      "Epoch 00010: val_f1 improved from 0.30621 to 0.31279, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 11/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 0.0184 - acc: 0.9939 - f1: 0.5159 - val_loss: 0.1837 - val_acc: 0.9595 - val_f1: 0.2905\n",
      "\n",
      "Epoch 00011: val_f1 did not improve from 0.31279\n",
      "Epoch 12/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 0.0177 - acc: 0.9941 - f1: 0.5173 - val_loss: 0.1844 - val_acc: 0.9569 - val_f1: 0.3041\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.31279\n",
      "Epoch 13/100\n",
      "1166/1165 [==============================] - 458s 393ms/step - loss: 0.0147 - acc: 0.9951 - f1: 0.5269 - val_loss: 0.1946 - val_acc: 0.9626 - val_f1: 0.3143\n",
      "\n",
      "Epoch 00013: val_f1 improved from 0.31279 to 0.31430, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 14/100\n",
      "1166/1165 [==============================] - 458s 393ms/step - loss: 0.0144 - acc: 0.9952 - f1: 0.5239 - val_loss: 0.1819 - val_acc: 0.9628 - val_f1: 0.3056\n",
      "\n",
      "Epoch 00014: val_f1 did not improve from 0.31430\n",
      "Epoch 15/100\n",
      "1166/1165 [==============================] - 459s 393ms/step - loss: 0.0136 - acc: 0.9955 - f1: 0.5311 - val_loss: 0.1781 - val_acc: 0.9591 - val_f1: 0.3111\n",
      "\n",
      "Epoch 00015: val_f1 did not improve from 0.31430\n",
      "Epoch 16/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 0.0126 - acc: 0.9959 - f1: 0.5311 - val_loss: 0.1829 - val_acc: 0.9592 - val_f1: 0.3009\n",
      "\n",
      "Epoch 00016: val_f1 did not improve from 0.31430\n",
      "Epoch 17/100\n",
      "1166/1165 [==============================] - 458s 393ms/step - loss: 0.0115 - acc: 0.9963 - f1: 0.5347 - val_loss: 0.1995 - val_acc: 0.9589 - val_f1: 0.3097\n",
      "\n",
      "Epoch 00017: val_f1 did not improve from 0.31430\n",
      "Epoch 18/100\n",
      "1166/1165 [==============================] - 459s 393ms/step - loss: 0.0112 - acc: 0.9963 - f1: 0.5348 - val_loss: 0.1695 - val_acc: 0.9607 - val_f1: 0.3132\n",
      "\n",
      "Epoch 00018: val_f1 did not improve from 0.31430\n",
      "Epoch 19/100\n",
      "1166/1165 [==============================] - 458s 393ms/step - loss: 0.0107 - acc: 0.9965 - f1: 0.5364 - val_loss: 0.2081 - val_acc: 0.9618 - val_f1: 0.2994\n",
      "\n",
      "Epoch 00019: val_f1 did not improve from 0.31430\n",
      "Epoch 20/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 0.0099 - acc: 0.9968 - f1: 0.5378 - val_loss: 0.1737 - val_acc: 0.9621 - val_f1: 0.3235\n",
      "\n",
      "Epoch 00020: val_f1 improved from 0.31430 to 0.32353, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 21/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 0.0101 - acc: 0.9968 - f1: 0.5385 - val_loss: 0.2019 - val_acc: 0.9605 - val_f1: 0.3046\n",
      "\n",
      "Epoch 00021: val_f1 did not improve from 0.32353\n",
      "Epoch 22/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 0.0092 - acc: 0.9971 - f1: 0.5418 - val_loss: 0.2253 - val_acc: 0.9607 - val_f1: 0.2961\n",
      "\n",
      "Epoch 00022: val_f1 did not improve from 0.32353\n",
      "Epoch 23/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 0.0089 - acc: 0.9971 - f1: 0.5389 - val_loss: 0.1921 - val_acc: 0.9624 - val_f1: 0.3195\n",
      "\n",
      "Epoch 00023: val_f1 did not improve from 0.32353\n",
      "Epoch 24/100\n",
      "1166/1165 [==============================] - 460s 394ms/step - loss: 0.0084 - acc: 0.9973 - f1: 0.5430 - val_loss: 0.2045 - val_acc: 0.9609 - val_f1: 0.3087\n",
      "\n",
      "Epoch 00024: val_f1 did not improve from 0.32353\n",
      "Epoch 25/100\n",
      "1166/1165 [==============================] - 460s 394ms/step - loss: 0.0082 - acc: 0.9974 - f1: 0.5423 - val_loss: 0.1793 - val_acc: 0.9616 - val_f1: 0.3187\n",
      "\n",
      "Epoch 00025: val_f1 did not improve from 0.32353\n",
      "Epoch 26/100\n",
      "1166/1165 [==============================] - 459s 393ms/step - loss: 0.0082 - acc: 0.9974 - f1: 0.5432 - val_loss: 0.1962 - val_acc: 0.9607 - val_f1: 0.3166\n",
      "\n",
      "Epoch 00026: val_f1 did not improve from 0.32353\n",
      "Epoch 27/100\n",
      "1166/1165 [==============================] - 459s 393ms/step - loss: 0.0077 - acc: 0.9975 - f1: 0.5442 - val_loss: 0.2158 - val_acc: 0.9607 - val_f1: 0.3147\n",
      "\n",
      "Epoch 00027: val_f1 did not improve from 0.32353\n",
      "Epoch 28/100\n",
      "1166/1165 [==============================] - 459s 393ms/step - loss: 0.0075 - acc: 0.9976 - f1: 0.5438 - val_loss: 0.1976 - val_acc: 0.9580 - val_f1: 0.2905\n",
      "\n",
      "Epoch 00028: val_f1 did not improve from 0.32353\n",
      "Epoch 29/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 0.0071 - acc: 0.9977 - f1: 0.5451 - val_loss: 0.2020 - val_acc: 0.9632 - val_f1: 0.3176\n",
      "\n",
      "Epoch 00029: val_f1 did not improve from 0.32353\n",
      "Epoch 30/100\n",
      "1166/1165 [==============================] - 459s 393ms/step - loss: 0.0073 - acc: 0.9977 - f1: 0.5457 - val_loss: 0.1972 - val_acc: 0.9614 - val_f1: 0.3204\n",
      "\n",
      "Epoch 00030: val_f1 did not improve from 0.32353\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 31/100\n",
      "1166/1165 [==============================] - 461s 395ms/step - loss: 0.0032 - acc: 0.9990 - f1: 0.5541 - val_loss: 0.2079 - val_acc: 0.9653 - val_f1: 0.3378\n",
      "\n",
      "Epoch 00031: val_f1 improved from 0.32353 to 0.33780, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 32/100\n",
      "1166/1165 [==============================] - 461s 395ms/step - loss: 0.0016 - acc: 0.9995 - f1: 0.5580 - val_loss: 0.2467 - val_acc: 0.9642 - val_f1: 0.3228\n",
      "\n",
      "Epoch 00032: val_f1 did not improve from 0.33780\n",
      "Epoch 33/100\n",
      "1166/1165 [==============================] - 461s 396ms/step - loss: 0.0023 - acc: 0.9993 - f1: 0.5541 - val_loss: 0.2236 - val_acc: 0.9642 - val_f1: 0.3361\n",
      "\n",
      "Epoch 00033: val_f1 did not improve from 0.33780\n",
      "Epoch 34/100\n",
      "1166/1165 [==============================] - 460s 395ms/step - loss: 0.0022 - acc: 0.9994 - f1: 0.5565 - val_loss: 0.2187 - val_acc: 0.9640 - val_f1: 0.3335\n",
      "\n",
      "Epoch 00034: val_f1 did not improve from 0.33780\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1165 [==============================] - 462s 396ms/step - loss: 0.0022 - acc: 0.9993 - f1: 0.5553 - val_loss: 0.2174 - val_acc: 0.9645 - val_f1: 0.3387\n",
      "\n",
      "Epoch 00035: val_f1 improved from 0.33780 to 0.33870, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 36/100\n",
      "1166/1165 [==============================] - 462s 396ms/step - loss: 0.0018 - acc: 0.9995 - f1: 0.5571 - val_loss: 0.2269 - val_acc: 0.9642 - val_f1: 0.3308\n",
      "\n",
      "Epoch 00036: val_f1 did not improve from 0.33870\n",
      "Epoch 37/100\n",
      "1166/1165 [==============================] - 462s 396ms/step - loss: 0.0018 - acc: 0.9995 - f1: 0.5572 - val_loss: 0.2363 - val_acc: 0.9639 - val_f1: 0.3316\n",
      "\n",
      "Epoch 00037: val_f1 did not improve from 0.33870\n",
      "Epoch 38/100\n",
      "1166/1165 [==============================] - 461s 396ms/step - loss: 0.0020 - acc: 0.9994 - f1: 0.5559 - val_loss: 0.2274 - val_acc: 0.9639 - val_f1: 0.3272\n",
      "\n",
      "Epoch 00038: val_f1 did not improve from 0.33870\n",
      "Epoch 39/100\n",
      "1166/1165 [==============================] - 462s 396ms/step - loss: 0.0019 - acc: 0.9995 - f1: 0.5570 - val_loss: 0.2362 - val_acc: 0.9634 - val_f1: 0.3264\n",
      "\n",
      "Epoch 00039: val_f1 did not improve from 0.33870\n",
      "Epoch 40/100\n",
      "1166/1165 [==============================] - 461s 395ms/step - loss: 0.0021 - acc: 0.9994 - f1: 0.5566 - val_loss: 0.2457 - val_acc: 0.9652 - val_f1: 0.3243\n",
      "\n",
      "Epoch 00040: val_f1 did not improve from 0.33870\n",
      "Epoch 41/100\n",
      "1166/1165 [==============================] - 462s 396ms/step - loss: 0.0018 - acc: 0.9995 - f1: 0.5576 - val_loss: 0.2359 - val_acc: 0.9654 - val_f1: 0.3305\n",
      "\n",
      "Epoch 00041: val_f1 did not improve from 0.33870\n",
      "Epoch 42/100\n",
      "1166/1165 [==============================] - 462s 396ms/step - loss: 0.0022 - acc: 0.9994 - f1: 0.5563 - val_loss: 0.2384 - val_acc: 0.9632 - val_f1: 0.3233\n",
      "\n",
      "Epoch 00042: val_f1 did not improve from 0.33870\n",
      "Epoch 43/100\n",
      "1166/1165 [==============================] - 458s 393ms/step - loss: 0.0018 - acc: 0.9995 - f1: 0.5568 - val_loss: 0.2412 - val_acc: 0.9644 - val_f1: 0.3328\n",
      "\n",
      "Epoch 00043: val_f1 did not improve from 0.33870\n",
      "Epoch 44/100\n",
      "1166/1165 [==============================] - 458s 393ms/step - loss: 0.0019 - acc: 0.9994 - f1: 0.5555 - val_loss: 0.2209 - val_acc: 0.9648 - val_f1: 0.3347\n",
      "\n",
      "Epoch 00044: val_f1 did not improve from 0.33870\n",
      "Epoch 45/100\n",
      "1166/1165 [==============================] - 462s 396ms/step - loss: 0.0017 - acc: 0.9995 - f1: 0.5580 - val_loss: 0.2251 - val_acc: 0.9649 - val_f1: 0.3322\n",
      "\n",
      "Epoch 00045: val_f1 did not improve from 0.33870\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 46/100\n",
      "1166/1165 [==============================] - 462s 396ms/step - loss: 0.0010 - acc: 0.9997 - f1: 0.5577 - val_loss: 0.2430 - val_acc: 0.9667 - val_f1: 0.3352\n",
      "\n",
      "Epoch 00046: val_f1 did not improve from 0.33870\n",
      "Epoch 47/100\n",
      "1166/1165 [==============================] - 461s 395ms/step - loss: 5.5933e-04 - acc: 0.9998 - f1: 0.5597 - val_loss: 0.2574 - val_acc: 0.9663 - val_f1: 0.3353\n",
      "\n",
      "Epoch 00047: val_f1 did not improve from 0.33870\n",
      "Epoch 48/100\n",
      "1166/1165 [==============================] - 462s 396ms/step - loss: 6.0512e-04 - acc: 0.9998 - f1: 0.5599 - val_loss: 0.2610 - val_acc: 0.9669 - val_f1: 0.3398\n",
      "\n",
      "Epoch 00048: val_f1 improved from 0.33870 to 0.33980, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 49/100\n",
      "1166/1165 [==============================] - 463s 397ms/step - loss: 4.6605e-04 - acc: 0.9999 - f1: 0.5598 - val_loss: 0.2734 - val_acc: 0.9660 - val_f1: 0.3356\n",
      "\n",
      "Epoch 00049: val_f1 did not improve from 0.33980\n",
      "Epoch 50/100\n",
      "1166/1165 [==============================] - 463s 397ms/step - loss: 7.6611e-04 - acc: 0.9998 - f1: 0.5578 - val_loss: 0.2550 - val_acc: 0.9659 - val_f1: 0.3361\n",
      "\n",
      "Epoch 00050: val_f1 did not improve from 0.33980\n",
      "Epoch 51/100\n",
      "1166/1165 [==============================] - 461s 395ms/step - loss: 5.6929e-04 - acc: 0.9999 - f1: 0.5600 - val_loss: 0.2687 - val_acc: 0.9661 - val_f1: 0.3357\n",
      "\n",
      "Epoch 00051: val_f1 did not improve from 0.33980\n",
      "Epoch 52/100\n",
      "1166/1165 [==============================] - 460s 394ms/step - loss: 7.2536e-04 - acc: 0.9998 - f1: 0.5590 - val_loss: 0.2559 - val_acc: 0.9652 - val_f1: 0.3340\n",
      "\n",
      "Epoch 00052: val_f1 did not improve from 0.33980\n",
      "Epoch 53/100\n",
      "1166/1165 [==============================] - 460s 395ms/step - loss: 6.3665e-04 - acc: 0.9998 - f1: 0.5589 - val_loss: 0.2622 - val_acc: 0.9651 - val_f1: 0.3326\n",
      "\n",
      "Epoch 00053: val_f1 did not improve from 0.33980\n",
      "Epoch 54/100\n",
      "1166/1165 [==============================] - 461s 395ms/step - loss: 5.2064e-04 - acc: 0.9999 - f1: 0.5602 - val_loss: 0.2621 - val_acc: 0.9659 - val_f1: 0.3389\n",
      "\n",
      "Epoch 00054: val_f1 did not improve from 0.33980\n",
      "Epoch 55/100\n",
      "1166/1165 [==============================] - 460s 394ms/step - loss: 5.9204e-04 - acc: 0.9999 - f1: 0.5601 - val_loss: 0.2575 - val_acc: 0.9661 - val_f1: 0.3366\n",
      "\n",
      "Epoch 00055: val_f1 did not improve from 0.33980\n",
      "Epoch 56/100\n",
      "1166/1165 [==============================] - 460s 394ms/step - loss: 6.2334e-04 - acc: 0.9998 - f1: 0.5573 - val_loss: 0.2584 - val_acc: 0.9662 - val_f1: 0.3402\n",
      "\n",
      "Epoch 00056: val_f1 improved from 0.33980 to 0.34017, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 57/100\n",
      "1166/1165 [==============================] - 460s 394ms/step - loss: 6.5425e-04 - acc: 0.9999 - f1: 0.5607 - val_loss: 0.2556 - val_acc: 0.9662 - val_f1: 0.3406\n",
      "\n",
      "Epoch 00057: val_f1 improved from 0.34017 to 0.34057, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 58/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 6.7817e-04 - acc: 0.9998 - f1: 0.5598 - val_loss: 0.2634 - val_acc: 0.9659 - val_f1: 0.3361\n",
      "\n",
      "Epoch 00058: val_f1 did not improve from 0.34057\n",
      "Epoch 59/100\n",
      "1166/1165 [==============================] - 459s 393ms/step - loss: 6.3066e-04 - acc: 0.9998 - f1: 0.5592 - val_loss: 0.2710 - val_acc: 0.9660 - val_f1: 0.3390\n",
      "\n",
      "Epoch 00059: val_f1 did not improve from 0.34057\n",
      "Epoch 60/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 4.7673e-04 - acc: 0.9999 - f1: 0.5596 - val_loss: 0.2625 - val_acc: 0.9665 - val_f1: 0.3425\n",
      "\n",
      "Epoch 00060: val_f1 improved from 0.34057 to 0.34251, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 61/100\n",
      "1166/1165 [==============================] - 460s 394ms/step - loss: 4.8517e-04 - acc: 0.9999 - f1: 0.5599 - val_loss: 0.2711 - val_acc: 0.9662 - val_f1: 0.3396\n",
      "\n",
      "Epoch 00061: val_f1 did not improve from 0.34251\n",
      "Epoch 62/100\n",
      "1166/1165 [==============================] - 460s 394ms/step - loss: 5.2488e-04 - acc: 0.9999 - f1: 0.5593 - val_loss: 0.2638 - val_acc: 0.9667 - val_f1: 0.3462\n",
      "\n",
      "Epoch 00062: val_f1 improved from 0.34251 to 0.34618, saving model to ../working/InceptionV3_3chan.h5\n",
      "Epoch 63/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 4.6883e-04 - acc: 0.9999 - f1: 0.5598 - val_loss: 0.2694 - val_acc: 0.9658 - val_f1: 0.3411\n",
      "\n",
      "Epoch 00063: val_f1 did not improve from 0.34618\n",
      "Epoch 64/100\n",
      "1166/1165 [==============================] - 460s 394ms/step - loss: 6.5028e-04 - acc: 0.9999 - f1: 0.5582 - val_loss: 0.2703 - val_acc: 0.9653 - val_f1: 0.3367\n",
      "\n",
      "Epoch 00064: val_f1 did not improve from 0.34618\n",
      "Epoch 65/100\n",
      "1166/1165 [==============================] - 460s 394ms/step - loss: 5.5346e-04 - acc: 0.9999 - f1: 0.5612 - val_loss: 0.2745 - val_acc: 0.9651 - val_f1: 0.3344\n",
      "\n",
      "Epoch 00065: val_f1 did not improve from 0.34618\n",
      "Epoch 66/100\n",
      "1166/1165 [==============================] - 460s 395ms/step - loss: 7.0599e-04 - acc: 0.9998 - f1: 0.5592 - val_loss: 0.2592 - val_acc: 0.9650 - val_f1: 0.3385\n",
      "\n",
      "Epoch 00066: val_f1 did not improve from 0.34618\n",
      "Epoch 67/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 6.3090e-04 - acc: 0.9998 - f1: 0.5594 - val_loss: 0.2579 - val_acc: 0.9659 - val_f1: 0.3401\n",
      "\n",
      "Epoch 00067: val_f1 did not improve from 0.34618\n",
      "Epoch 68/100\n",
      "1166/1165 [==============================] - 461s 396ms/step - loss: 5.4619e-04 - acc: 0.9999 - f1: 0.5591 - val_loss: 0.2574 - val_acc: 0.9659 - val_f1: 0.3429\n",
      "\n",
      "Epoch 00068: val_f1 did not improve from 0.34618\n",
      "Epoch 69/100\n",
      "1166/1165 [==============================] - 460s 394ms/step - loss: 4.9247e-04 - acc: 0.9999 - f1: 0.5595 - val_loss: 0.2709 - val_acc: 0.9662 - val_f1: 0.3344\n",
      "\n",
      "Epoch 00069: val_f1 did not improve from 0.34618\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1165 [==============================] - 460s 395ms/step - loss: 6.2817e-04 - acc: 0.9998 - f1: 0.5603 - val_loss: 0.2681 - val_acc: 0.9652 - val_f1: 0.3365\n",
      "\n",
      "Epoch 00070: val_f1 did not improve from 0.34618\n",
      "Epoch 71/100\n",
      "1166/1165 [==============================] - 459s 393ms/step - loss: 5.8649e-04 - acc: 0.9999 - f1: 0.5599 - val_loss: 0.2713 - val_acc: 0.9662 - val_f1: 0.3395\n",
      "\n",
      "Epoch 00071: val_f1 did not improve from 0.34618\n",
      "Epoch 72/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 6.0479e-04 - acc: 0.9999 - f1: 0.5587 - val_loss: 0.2680 - val_acc: 0.9654 - val_f1: 0.3422\n",
      "\n",
      "Epoch 00072: val_f1 did not improve from 0.34618\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 73/100\n",
      "1166/1165 [==============================] - 460s 394ms/step - loss: 4.0526e-04 - acc: 0.9999 - f1: 0.5603 - val_loss: 0.2681 - val_acc: 0.9668 - val_f1: 0.3432\n",
      "\n",
      "Epoch 00073: val_f1 did not improve from 0.34618\n",
      "Epoch 74/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 2.8431e-04 - acc: 1.0000 - f1: 0.5602 - val_loss: 0.2758 - val_acc: 0.9664 - val_f1: 0.3460\n",
      "\n",
      "Epoch 00074: val_f1 did not improve from 0.34618\n",
      "Epoch 75/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 2.3863e-04 - acc: 1.0000 - f1: 0.5591 - val_loss: 0.2835 - val_acc: 0.9666 - val_f1: 0.3455\n",
      "\n",
      "Epoch 00075: val_f1 did not improve from 0.34618\n",
      "Epoch 76/100\n",
      "1166/1165 [==============================] - 459s 393ms/step - loss: 3.7880e-04 - acc: 0.9999 - f1: 0.5587 - val_loss: 0.2868 - val_acc: 0.9661 - val_f1: 0.3433\n",
      "\n",
      "Epoch 00076: val_f1 did not improve from 0.34618\n",
      "Epoch 77/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 2.4969e-04 - acc: 1.0000 - f1: 0.5614 - val_loss: 0.2863 - val_acc: 0.9659 - val_f1: 0.3410\n",
      "\n",
      "Epoch 00077: val_f1 did not improve from 0.34618\n",
      "Epoch 78/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 2.5417e-04 - acc: 1.0000 - f1: 0.5601 - val_loss: 0.2916 - val_acc: 0.9657 - val_f1: 0.3372\n",
      "\n",
      "Epoch 00078: val_f1 did not improve from 0.34618\n",
      "Epoch 79/100\n",
      "1166/1165 [==============================] - 458s 393ms/step - loss: 2.5885e-04 - acc: 1.0000 - f1: 0.5598 - val_loss: 0.2958 - val_acc: 0.9657 - val_f1: 0.3349\n",
      "\n",
      "Epoch 00079: val_f1 did not improve from 0.34618\n",
      "Epoch 80/100\n",
      "1166/1165 [==============================] - 458s 393ms/step - loss: 2.9712e-04 - acc: 0.9999 - f1: 0.5586 - val_loss: 0.2829 - val_acc: 0.9665 - val_f1: 0.3397\n",
      "\n",
      "Epoch 00080: val_f1 did not improve from 0.34618\n",
      "Epoch 81/100\n",
      "1166/1165 [==============================] - 459s 393ms/step - loss: 1.8988e-04 - acc: 1.0000 - f1: 0.5615 - val_loss: 0.2819 - val_acc: 0.9659 - val_f1: 0.3368\n",
      "\n",
      "Epoch 00081: val_f1 did not improve from 0.34618\n",
      "Epoch 82/100\n",
      "1166/1165 [==============================] - 459s 394ms/step - loss: 2.2281e-04 - acc: 1.0000 - f1: 0.5614 - val_loss: 0.2909 - val_acc: 0.9661 - val_f1: 0.3371\n",
      "\n",
      "Epoch 00082: val_f1 did not improve from 0.34618\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 12\n",
    "hist =  model.fit_generator(\n",
    "        tg,\n",
    "        steps_per_epoch=np.ceil(float(len(pathsTrain)) / float(batch_size))/2,\n",
    "        validation_data=vg,\n",
    "        validation_steps=np.ceil(float(len(pathsVal)) / float(batch_size))/2,\n",
    "        epochs=100, \n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=f1_loss,\n",
    "#             optimizer=Adam(lr=1e-4),\n",
    "#             metrics=['accuracy', f1])\n",
    "# hist =  model.fit_generator(\n",
    "#         tg,\n",
    "#         steps_per_epoch=np.ceil(float(len(pathsTrain)) / float(batch_size))/2,\n",
    "#         validation_data=vg,\n",
    "#         validation_steps=np.ceil(float(len(pathsVal)) / float(batch_size))/2,\n",
    "#         epochs=200, \n",
    "#         verbose=1,\n",
    "#         callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x24c19b7e0b8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].set_title('loss')\n",
    "ax[0].plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\n",
    "ax[0].plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax[1].set_title('acc')\n",
    "ax[1].plot(hist.epoch, hist.history[\"f1\"], label=\"Train F1\")\n",
    "ax[1].plot(hist.epoch, hist.history[\"val_f1\"], label=\"Validation F1\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "bestModel = load_model('../working/InceptionV3.h5', custom_objects={'f1': f1, 'f1_loss': f1_loss, 'focal_loss_fixed':focal_loss()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "lastFullValPred = np.empty((0, 28))\n",
    "lastFullValLabels = np.empty((0, 28))\n",
    "for i in tqdm(range(len(vg))): \n",
    "    im, lbl = vg[i]\n",
    "    scores = bestModel.predict(im)\n",
    "    lastFullValPred = np.append(lastFullValPred, scores, axis=0)\n",
    "    lastFullValLabels = np.append(lastFullValLabels, lbl, axis=0)\n",
    "print(lastFullValPred.shape, lastFullValLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score as off1\n",
    "rng = np.arange(0, 1, 0.001)\n",
    "f1s = np.zeros((rng.shape[0], 28))\n",
    "for j,t in enumerate(tqdm(rng)):\n",
    "    for i in range(28):\n",
    "        p = np.array(lastFullValPred[:,i]>t, dtype=np.int8)\n",
    "        scoref1 = off1(lastFullValLabels[:,i], p, average='binary')\n",
    "        f1s[j,i] = scoref1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Individual F1-scores for each class:')\n",
    "print(np.max(f1s, axis=0))\n",
    "print('Macro F1-score CV =', np.mean(np.max(f1s, axis=0)))\n",
    "plt.plot(rng, f1s)\n",
    "T = np.empty(28)\n",
    "for i in range(28):\n",
    "    T[i] = rng[np.where(f1s[:,i] == np.max(f1s[:,i]))[0][0]]\n",
    "print('Probability threshold maximizing CV F1-score for each class:')\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathsTest, labelsTest = getTestDataset()\n",
    "\n",
    "testg = ProteinDataGenerator(pathsTest, labelsTest, batch_size, SHAPE, channels)\n",
    "submit = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "P = np.zeros((pathsTest.shape[0], 28))\n",
    "for i in tqdm(range(len(testg))):\n",
    "    images, labels = testg[i]\n",
    "    score = bestModel.predict(images)\n",
    "    P[i*batch_size:i*batch_size+score.shape[0]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP = np.array(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "\n",
    "for row in tqdm(range(submit.shape[0])):\n",
    "    \n",
    "    str_label = ''\n",
    "    \n",
    "    for col in range(PP.shape[1]):\n",
    "        if(PP[row, col] < T[col]):\n",
    "            str_label += ''\n",
    "        else:\n",
    "            str_label += str(col) + ' '\n",
    "    prediction.append(str_label.strip())\n",
    "    \n",
    "submit['Predicted'] = np.array(prediction)\n",
    "submit.to_csv('transfer_1x1conv_aug_focal_loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# pathsTest, labelsTest = getTestDataset()\n",
    "\n",
    "# testg = ProteinDataGenerator(pathsTest, labelsTest, batch_size, SHAPE)\n",
    "# submit = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "# P = np.zeros((pathsTest.shape[0], 28))\n",
    "# for i in tqdm(range(len(testg))):\n",
    "#     images, labels = testg[i]\n",
    "#     score = bestModel.predict(images)\n",
    "#     P[i*batch_size:i*batch_size+score.shape[0]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PP = np.array(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = []\n",
    "\n",
    "# for row in tqdm(range(submit.shape[0])):\n",
    "    \n",
    "#     str_label = ''\n",
    "    \n",
    "#     for col in range(PP.shape[1]):\n",
    "#         if(PP[row, col] < .2):   # to account for losing TP is more costly than decreasing FP\n",
    "#             #print(PP[row])\n",
    "#             str_label += ''\n",
    "#         else:\n",
    "#             str_label += str(col) + ' '\n",
    "#     prediction.append(str_label.strip())\n",
    "    \n",
    "# submit['Predicted'] = np.array(prediction)\n",
    "# submit.to_csv('datagenerator_model_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
