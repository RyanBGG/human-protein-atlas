{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "import keras\n",
    "import warnings\n",
    "from keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 299\n",
    "SEED = 777\n",
    "THRESHOLD = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info\n",
    "DIR = '../input/'\n",
    "data = pd.read_csv('../input/train.csv')\n",
    "\n",
    "# train_dataset_info = []\n",
    "# for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "#     train_dataset_info.append({\n",
    "#         'path':os.path.join(path_to_train, name),\n",
    "#         'labels':np.array([int(label) for label in labels])})\n",
    "# train_dataset_info = np.array(train_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainDataset():\n",
    "    \n",
    "    path_to_train = DIR + '/train/'\n",
    "    data = pd.read_csv(DIR + '/train.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name, lbl in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "        y = np.zeros(28)\n",
    "        for key in lbl:\n",
    "            y[int(key)] = 1\n",
    "        paths.append(os.path.join(path_to_train, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)\n",
    "\n",
    "def getTestDataset():\n",
    "    \n",
    "    path_to_test = DIR + '/test/'\n",
    "    data = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name in data['Id']:\n",
    "        y = np.ones(28)\n",
    "        paths.append(os.path.join(path_to_test, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)\n",
    "paths, labels = getTrainDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credits: https://github.com/keras-team/keras/blob/master/keras/utils/data_utils.py#L302\n",
    "# credits: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "from random import randint\n",
    "class ProteinDataGenerator(keras.utils.Sequence):\n",
    "            \n",
    "    def __init__(self, paths, labels, batch_size, shape, channels = [], shuffle = False, use_cache = False, augmentor = False):\n",
    "        self.paths, self.labels = paths, labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shape = shape\n",
    "        self.shuffle = shuffle\n",
    "        self.use_cache = use_cache\n",
    "        self.channels = channels\n",
    "        self.augmentor = augmentor\n",
    "        self.clahe = cv2.createCLAHE()\n",
    "        if use_cache == True:\n",
    "            self.cache = np.zeros((paths.shape[0], shape[0], shape[1], len(channels)))\n",
    "            self.is_cached = np.zeros((paths.shape[0]))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "\n",
    "        paths = self.paths[indexes]\n",
    "        X = np.zeros((paths.shape[0], self.shape[0], self.shape[1], self.shape[2]))\n",
    "        # Generate data\n",
    "        if self.use_cache == True:\n",
    "            X = self.cache[indexes]\n",
    "            for i, path in enumerate(paths[np.where(self.is_cached[indexes] == 0)]):\n",
    "                image = self.__load_image(path)\n",
    "                self.is_cached[indexes[i]] = 1\n",
    "                self.cache[indexes[i]] = image\n",
    "                X[i] = image\n",
    "        else:\n",
    "            for i, path in enumerate(paths):\n",
    "                X[i] = self.__load_image(path)\n",
    "        if self.augmentor == True:\n",
    "            for i, item in enumerate(X):\n",
    "                X[i] = self.augment(item)\n",
    "        y = self.labels[indexes]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        # Updates indexes after each epoch\n",
    "        self.indexes = np.arange(len(self.paths))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "        for item in (self[i] for i in range(len(self))):\n",
    "            yield item\n",
    "            \n",
    "    def __load_image(self, path):\n",
    "        images = []\n",
    "        for channel in self.channels:\n",
    "            im = np.array(Image.open(path + '_' + channel + '.png'))\n",
    "            \n",
    "#             im = clahe.apply(im)\n",
    "            images.append(im)\n",
    "            \n",
    "        if len(self.channels) >= 2:\n",
    "            im = np.stack((\n",
    "                images\n",
    "            ), -1)\n",
    "            im = cv2.resize(im, (SIZE,SIZE))\n",
    "            im = np.divide(im, 255)\n",
    "\n",
    "        else:\n",
    "            im = images[0]\n",
    "            im = cv2.resize(im, (SIZE,SIZE))\n",
    "            im = np.divide(im, 255)\n",
    "            im = np.expand_dims(im, 2)\n",
    "        return im\n",
    "    def augment(self, image):\n",
    "        if randint(0,1) == 1:\n",
    "            augment_img = iaa.Sequential([\n",
    "                iaa.OneOf([\n",
    "                    iaa.Fliplr(0.5), # horizontal flips\n",
    "                    iaa.Flipud(0.5), # horizontal flips\n",
    "                    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "                    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "                    # But we only blur about 50% of all images.\n",
    "                    iaa.Sometimes(0.5,\n",
    "                        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "                    ),\n",
    "                    # Make some images brighter and some darker.\n",
    "                    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "                    # which can end up changing the color of the images.\n",
    "                    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "                    # Apply affine transformations to each image.\n",
    "                    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "                    iaa.Affine(\n",
    "                        scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)},\n",
    "                        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
    "                        rotate=(-180, 180),\n",
    "                        shear=(-4, 4)\n",
    "                    )\n",
    "                ])], random_order=True)\n",
    "\n",
    "\n",
    "            image_aug = augment_img.augment_image(image)\n",
    "            return image_aug\n",
    "        else:\n",
    "            return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPE = (299, 299, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n"
     ]
    }
   ],
   "source": [
    "channels = [\"red\"]\n",
    "for path in paths[0:10]:\n",
    "    images = []\n",
    "    for channel in channels:\n",
    "        print(channel)\n",
    "        images.append(np.array(Image.open(path + '_' + channel + '.png')))\n",
    "\n",
    "    if len(channels) >= 2:\n",
    "        im = np.stack((\n",
    "            images\n",
    "        ), -1)\n",
    "        im = cv2.resize(im, (SIZE,SIZE))\n",
    "        im = np.divide(im, 255)\n",
    "        \n",
    "    else:\n",
    "        im = images[0]\n",
    "        im = cv2.resize(im, (SIZE,SIZE))\n",
    "        im = np.divide(im, 255)\n",
    "        im = np.expand_dims(im, 2)\n",
    "    print(im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class data_generator:\n",
    "    \n",
    "#     def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "#         assert shape[2] == 3\n",
    "#         while True:\n",
    "#             dataset_info = shuffle(dataset_info)\n",
    "#             for start in range(0, len(dataset_info), batch_size):\n",
    "#                 end = min(start + batch_size, len(dataset_info))\n",
    "#                 batch_images = []\n",
    "#                 X_train_batch = dataset_info[start:end]\n",
    "#                 batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "#                 for i in range(len(X_train_batch)):\n",
    "#                     image = data_generator.load_image(\n",
    "#                         X_train_batch[i]['path'], shape)   \n",
    "#                     if augument:\n",
    "#                         image = data_generator.augment(image)\n",
    "#                     batch_images.append(image/255.)\n",
    "#                     batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "#                 yield np.array(batch_images, np.float32), batch_labels\n",
    "\n",
    "#     def load_image(path, shape):\n",
    "#         image_red_ch = Image.open(path+'_red.png')\n",
    "#         image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "#         image_green_ch = Image.open(path+'_green.png')\n",
    "#         image_blue_ch = Image.open(path+'_blue.png')\n",
    "#         image = np.stack((\n",
    "#         np.array(image_red_ch), \n",
    "#         np.array(image_green_ch), \n",
    "#         np.array(image_blue_ch)), -1)\n",
    "#         image = cv2.resize(image, (shape[0], shape[1]))\n",
    "#         return image\n",
    "\n",
    "#     def augment(image):\n",
    "#         augment_img = iaa.Sequential([\n",
    "#             iaa.OneOf([\n",
    "#                 iaa.Affine(rotate=0),\n",
    "#                 iaa.Affine(rotate=90),\n",
    "#                 iaa.Affine(rotate=180),\n",
    "#                 iaa.Affine(rotate=270),\n",
    "#                 iaa.Fliplr(0.5),\n",
    "#                 iaa.Flipud(0.5),\n",
    "#             ])], random_order=True)\n",
    "\n",
    "#         image_aug = augment_img.augment_image(image)\n",
    "#         return image_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D, MaxPooling2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    #y_pred = K.round(y_pred)\n",
    "    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    #y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1-K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# f = h5py.File(\"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\", 'r')\n",
    "# dset = f[\"conv2d_1\"]\n",
    "# list(dset.keys())\n",
    "# group = dset[\"conv2d_1\"]\n",
    "# list(group.keys())\n",
    "# first_lay = group['kernel:0'].value\n",
    "# print(first_lay.shape)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros = np.zeros(shape = (3,3,1,32))\n",
    "# print(zeros.shape)\n",
    "# first_lay1 = np.concatenate([zeros, first_lay], axis = 2)\n",
    "# first_lay.shape\n",
    "# f = h5py.File(\"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\", 'r+')\n",
    "# dset = f[\"conv2d_1\"]\n",
    "# list(dset.keys())\n",
    "# group = dset[\"conv2d_1\"]\n",
    "# list(group.keys())\n",
    "# del group['kernel:0']\n",
    "# group['kernel:0'] = first_lay1\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 299, 299, 4)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 149, 149, 32) 1152        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 21,803,072\n",
      "Trainable params: 21,768,640\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Inception V3 model for Keras.\n",
    "Note that the input image format for this model is different than for\n",
    "the VGG16 and ResNet models (299x299 instead of 224x224),\n",
    "and that the input preprocessing function is also different (same as Xception).\n",
    "# Reference\n",
    "- [Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567)\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "\n",
    "def conv2d_bn(x,\n",
    "              filters,\n",
    "              num_row,\n",
    "              num_col,\n",
    "              padding='same',\n",
    "              strides=(1, 1),\n",
    "              name=None):\n",
    "    \"\"\"Utility function to apply conv + BN.\n",
    "    Arguments:\n",
    "        x: input tensor.\n",
    "        filters: filters in `Conv2D`.\n",
    "        num_row: height of the convolution kernel.\n",
    "        num_col: width of the convolution kernel.\n",
    "        padding: padding mode in `Conv2D`.\n",
    "        strides: strides in `Conv2D`.\n",
    "        name: name of the ops; will become `name + '_conv'`\n",
    "            for the convolution and `name + '_bn'` for the\n",
    "            batch norm layer.\n",
    "    Returns:\n",
    "        Output tensor after applying `Conv2D` and `BatchNormalization`.\n",
    "    \"\"\"\n",
    "    if name is not None:\n",
    "        bn_name = name + '_bn'\n",
    "        conv_name = name + '_conv'\n",
    "    else:\n",
    "        bn_name = None\n",
    "        conv_name = None\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        bn_axis = 1\n",
    "    else:\n",
    "        bn_axis = 3\n",
    "    x = Conv2D(\n",
    "        filters, (num_row, num_col),\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        use_bias=False,\n",
    "        name=conv_name)(x)\n",
    "    x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n",
    "    x = Activation('relu', name=name)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def InceptionV3(include_top=True,\n",
    "                weights='imagenet',\n",
    "                input_tensor=None,\n",
    "                input_shape=None,\n",
    "                pooling=None,\n",
    "                classes=1000):\n",
    "    \"\"\"Instantiates the Inception v3 architecture.\n",
    "    Optionally loads weights pre-trained\n",
    "    on ImageNet. Note that when using TensorFlow,\n",
    "    for best performance you should set\n",
    "    `image_data_format=\"channels_last\"` in your Keras config\n",
    "    at ~/.keras/keras.json.\n",
    "    The model and the weights are compatible with both\n",
    "    TensorFlow and Theano. The data format\n",
    "    convention used by the model is the one\n",
    "    specified in your Keras config file.\n",
    "    Note that the default input image size for this model is 299x299.\n",
    "    Arguments:\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization)\n",
    "            or \"imagenet\" (pre-training on ImageNet).\n",
    "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(299, 299, 3)` (with `channels_last` data format)\n",
    "            or `(3, 299, 299)` (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels,\n",
    "            and width and height should be no smaller than 139.\n",
    "            E.g. `(150, 150, 3)` would be one valid value.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    Returns:\n",
    "        A Keras model instance.\n",
    "    Raises:\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    if weights not in {'imagenet', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `imagenet` '\n",
    "                         '(pre-training on ImageNet).')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = (299,299,4)\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = 3\n",
    "\n",
    "    x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding='valid')\n",
    "    x = conv2d_bn(x, 32, 3, 3, padding='valid')\n",
    "    x = conv2d_bn(x, 64, 3, 3)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv2d_bn(x, 80, 1, 1, padding='valid')\n",
    "    x = conv2d_bn(x, 192, 3, 3, padding='valid')\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    # mixed 0, 1, 2: 35 x 35 x 256\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 32, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed0')\n",
    "\n",
    "    # mixed 1: 35 x 35 x 256\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed1')\n",
    "\n",
    "    # mixed 2: 35 x 35 x 256\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed2')\n",
    "\n",
    "    # mixed 3: 17 x 17 x 768\n",
    "    branch3x3 = conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(\n",
    "        branch3x3dbl, 96, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = layers.concatenate(\n",
    "        [branch3x3, branch3x3dbl, branch_pool], axis=channel_axis, name='mixed3')\n",
    "\n",
    "    # mixed 4: 17 x 17 x 768\n",
    "    branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch7x7 = conv2d_bn(x, 128, 1, 1)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 128, 1, 7)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "    branch7x7dbl = conv2d_bn(x, 128, 1, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 1, 7)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed4')\n",
    "\n",
    "    # mixed 5, 6: 17 x 17 x 768\n",
    "    for i in range(2):\n",
    "        branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "        branch7x7 = conv2d_bn(x, 160, 1, 1)\n",
    "        branch7x7 = conv2d_bn(branch7x7, 160, 1, 7)\n",
    "        branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "        branch7x7dbl = conv2d_bn(x, 160, 1, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 1, 7)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "        branch_pool = AveragePooling2D(\n",
    "            (3, 3), strides=(1, 1), padding='same')(x)\n",
    "        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "        x = layers.concatenate(\n",
    "            [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "            axis=channel_axis,\n",
    "            name='mixed' + str(5 + i))\n",
    "\n",
    "    # mixed 7: 17 x 17 x 768\n",
    "    branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch7x7 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 1, 7)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "    branch7x7dbl = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed7')\n",
    "\n",
    "    # mixed 8: 8 x 8 x 1280\n",
    "    branch3x3 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch3x3 = conv2d_bn(branch3x3, 320, 3, 3,\n",
    "                          strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch7x7x3 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)\n",
    "    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)\n",
    "    branch7x7x3 = conv2d_bn(\n",
    "        branch7x7x3, 192, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = layers.concatenate(\n",
    "        [branch3x3, branch7x7x3, branch_pool], axis=channel_axis, name='mixed8')\n",
    "\n",
    "    # mixed 9: 8 x 8 x 2048\n",
    "    for i in range(2):\n",
    "        branch1x1 = conv2d_bn(x, 320, 1, 1)\n",
    "\n",
    "        branch3x3 = conv2d_bn(x, 384, 1, 1)\n",
    "        branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)\n",
    "        branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)\n",
    "        branch3x3 = layers.concatenate(\n",
    "            [branch3x3_1, branch3x3_2], axis=channel_axis, name='mixed9_' + str(i))\n",
    "\n",
    "        branch3x3dbl = conv2d_bn(x, 448, 1, 1)\n",
    "        branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)\n",
    "        branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)\n",
    "        branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)\n",
    "        branch3x3dbl = layers.concatenate(\n",
    "            [branch3x3dbl_1, branch3x3dbl_2], axis=channel_axis)\n",
    "\n",
    "        branch_pool = AveragePooling2D(\n",
    "            (3, 3), strides=(1, 1), padding='same')(x)\n",
    "        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "        x = layers.concatenate(\n",
    "            [branch1x1, branch3x3, branch3x3dbl, branch_pool],\n",
    "            axis=channel_axis,\n",
    "            name='mixed' + str(9 + i))\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        x = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='inception_v3')\n",
    "\n",
    "    # load weights\n",
    "    if weights == 'imagenet':\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            if K.backend() == 'tensorflow':\n",
    "                warnings.warn('You are using the TensorFlow backend, yet you '\n",
    "                              'are using the Theano '\n",
    "                              'image data format convention '\n",
    "                              '(`image_data_format=\"channels_first\"`). '\n",
    "                              'For best performance, set '\n",
    "                              '`image_data_format=\"channels_last\"` in '\n",
    "                              'your Keras config '\n",
    "                              'at ~/.keras/keras.json.')\n",
    "        if include_top:\n",
    "            weights_path = get_file(\n",
    "                'inception_v3_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                WEIGHTS_PATH,\n",
    "                cache_subdir='models',\n",
    "                md5_hash='9a0d58056eeedaa3f26cb7ebd46da564')\n",
    "        else:\n",
    "            weights_path = get_file(\n",
    "                'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                WEIGHTS_PATH_NO_TOP,\n",
    "                cache_subdir='models',\n",
    "                md5_hash='bcbd6486424b2319ff4ef7d526e38f63')\n",
    "            weights_path = \"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "        model.load_weights(weights_path)\n",
    "        if K.backend() == 'theano':\n",
    "            convert_all_kernels_in_model(model)\n",
    "    return model\n",
    "if __name__ == '__main__':\n",
    "    model = InceptionV3(include_top=False, weights='imagenet')\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out, channels):\n",
    "    input_tensor = Input(shape=(299,299,len(channels)))\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    base_model = InceptionV3(include_top=False, weights='imagenet')\n",
    "    x = base_model(bn)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Conv2D(128, kernel_size=(3,3), activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 299, 299, 4)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 299, 299, 4)       16        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         (None, 8, 8, 2048)        21803072  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 2048)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_189 (Conv2D)          (None, 6, 6, 128)         2359424   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 28)                28700     \n",
      "=================================================================\n",
      "Total params: 28,910,828\n",
      "Trainable params: 28,876,388\n",
      "Non-trainable params: 34,440\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "channels = [\"green\", \"blue\", \"red\", \"yellow\"]\n",
    "# with tf.device('/cpu:0'):\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,4), \n",
    "    n_out=28, channels = channels)\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=Adam(1e-04),\n",
    "    metrics=['acc', f1])\n",
    "model.summary()\n",
    "\n",
    "# model.layers[3].layers[1] = Conv2D(32, kernel_size = (7,7), strides = (2,2), padding = \"same\", input_shape = (299, 299, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31072,) (31072, 28)\n",
      "(27964,) (27964, 28) (3108,) (3108, 28)\n"
     ]
    }
   ],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "epochs = 10; batch_size = 32;VAL_RATIO = .1;DEBUG = False\n",
    "# split data into train, valid\n",
    "paths, labels = getTrainDataset()\n",
    "\n",
    "# divide to \n",
    "keys = np.arange(paths.shape[0], dtype=np.int)  \n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(keys)\n",
    "lastTrainIndex = int((1-VAL_RATIO) * paths.shape[0])\n",
    "\n",
    "if DEBUG == True:  # use only small subset for debugging, Kaggle's RAM is limited\n",
    "    pathsTrain = paths[0:256]\n",
    "    labelsTrain = labels[0:256]\n",
    "    pathsVal = paths[lastTrainIndex:lastTrainIndex+256]\n",
    "    labelsVal = labels[lastTrainIndex:lastTrainIndex+256]\n",
    "    use_cache = True\n",
    "else:\n",
    "    pathsTrain = paths[0:lastTrainIndex]\n",
    "    labelsTrain = labels[0:lastTrainIndex]\n",
    "    pathsVal = paths[lastTrainIndex:]\n",
    "    labelsVal = labels[lastTrainIndex:]\n",
    "    use_cache = False\n",
    "\n",
    "print(paths.shape, labels.shape)\n",
    "print(pathsTrain.shape, labelsTrain.shape, pathsVal.shape, labelsVal.shape)\n",
    "use_cache = True\n",
    "tg = ProteinDataGenerator(pathsTrain, labelsTrain, batch_size, SHAPE, channels, use_cache=use_cache, augmentor = True)\n",
    "vg = ProteinDataGenerator(pathsVal, labelsVal, batch_size, SHAPE, channels, use_cache=use_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and valid datagens\n",
    "# train_generator = data_generator.create_train(\n",
    "#     train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "# validation_generator = data_generator.create_train(\n",
    "#     train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('../working/InceptionV3+branch.h5', monitor='val_f1', verbose=1, \n",
    "                             save_best_only=True, mode='max', save_weights_only = False)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_f1', factor=0.5, patience=10, \n",
    "                                   verbose=1, mode='max', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_f1\", \n",
    "                      mode=\"max\", \n",
    "                      patience=20)\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 299, 299, 4)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_190 (Bat (None, 299, 299, 4)       16        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         (None, 8, 8, 2048)        21803072  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 2048)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_284 (Conv2D)          (None, 6, 6, 128)         2359424   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 28)                28700     \n",
      "=================================================================\n",
      "Total params: 28,910,828\n",
      "Trainable params: 7,107,748\n",
      "Non-trainable params: 21,803,080\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# warm up model\n",
    "import tensorflow as tf\n",
    "channels = [\"green\", \"blue\", \"red\", \"yellow\"]\n",
    "# with tf.device('/cpu:0'):\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,4), \n",
    "    n_out=28, channels = channels)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "model.layers[0].trainable = True\n",
    "model.layers[1].trainable = True\n",
    "model.layers[-1].trainable = True\n",
    "model.layers[-2].trainable = True\n",
    "model.layers[-3].trainable = True\n",
    "model.layers[-4].trainable = True\n",
    "model.layers[-5].trainable = True\n",
    "model.layers[-6].trainable = True\n",
    "model.layers[-7].trainable = True\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tg = ProteinDataGenerator(pathsTrain, labelsTrain, batch_size, SHAPE, channels, use_cache=use_cache, augmentor = True)\n",
    "vg = ProteinDataGenerator(pathsVal, labelsVal, batch_size, SHAPE, channels, use_cache=use_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "437/437 [==============================] - 441s 1s/step - loss: 0.2119 - acc: 0.9325 - f1: 0.0584 - val_loss: 0.1922 - val_acc: 0.9379 - val_f1: 0.0524\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.05238, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 2/20\n",
      "437/437 [==============================] - 504s 1s/step - loss: 0.1887 - acc: 0.9389 - f1: 0.0547 - val_loss: 0.2143 - val_acc: 0.9403 - val_f1: 0.0823\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.05238 to 0.08229, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 3/20\n",
      "437/437 [==============================] - 132s 301ms/step - loss: 0.1849 - acc: 0.9395 - f1: 0.0526 - val_loss: 0.2209 - val_acc: 0.9408 - val_f1: 0.0854\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.08229 to 0.08535, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 4/20\n",
      "437/437 [==============================] - 204s 467ms/step - loss: 0.1823 - acc: 0.9403 - f1: 0.0502 - val_loss: 0.2019 - val_acc: 0.9373 - val_f1: 0.0720\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.08535\n",
      "Epoch 5/20\n",
      "437/437 [==============================] - 137s 312ms/step - loss: 0.1798 - acc: 0.9405 - f1: 0.0506 - val_loss: 0.2400 - val_acc: 0.9406 - val_f1: 0.0914\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.08535 to 0.09140, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 6/20\n",
      "437/437 [==============================] - 201s 460ms/step - loss: 0.1808 - acc: 0.9404 - f1: 0.0501 - val_loss: 0.1984 - val_acc: 0.9395 - val_f1: 0.0711\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.09140\n",
      "Epoch 7/20\n",
      "437/437 [==============================] - 137s 313ms/step - loss: 0.1786 - acc: 0.9407 - f1: 0.0504 - val_loss: 0.1989 - val_acc: 0.9335 - val_f1: 0.0684\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.09140\n",
      "Epoch 8/20\n",
      "437/437 [==============================] - 163s 373ms/step - loss: 0.1785 - acc: 0.9407 - f1: 0.0503 - val_loss: 0.2094 - val_acc: 0.9403 - val_f1: 0.0825\n",
      "\n",
      "Epoch 00008: val_f1 did not improve from 0.09140\n",
      "Epoch 9/20\n",
      "437/437 [==============================] - 138s 315ms/step - loss: 0.1772 - acc: 0.9409 - f1: 0.0508 - val_loss: 0.2035 - val_acc: 0.9316 - val_f1: 0.0721\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.09140\n",
      "Epoch 10/20\n",
      "437/437 [==============================] - 159s 363ms/step - loss: 0.1765 - acc: 0.9410 - f1: 0.0494 - val_loss: 0.2057 - val_acc: 0.9372 - val_f1: 0.0784\n",
      "\n",
      "Epoch 00010: val_f1 did not improve from 0.09140\n",
      "Epoch 11/20\n",
      "437/437 [==============================] - 136s 311ms/step - loss: 0.1755 - acc: 0.9412 - f1: 0.0506 - val_loss: 0.1951 - val_acc: 0.9346 - val_f1: 0.0686\n",
      "\n",
      "Epoch 00011: val_f1 did not improve from 0.09140\n",
      "Epoch 12/20\n",
      "437/437 [==============================] - 138s 317ms/step - loss: 0.1756 - acc: 0.9411 - f1: 0.0507 - val_loss: 0.2004 - val_acc: 0.9298 - val_f1: 0.0685\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.09140\n",
      "Epoch 13/20\n",
      "437/437 [==============================] - 153s 349ms/step - loss: 0.1749 - acc: 0.9414 - f1: 0.0514 - val_loss: 0.2096 - val_acc: 0.9291 - val_f1: 0.0755\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.09140\n",
      "Epoch 14/20\n",
      "437/437 [==============================] - 149s 340ms/step - loss: 0.1741 - acc: 0.9414 - f1: 0.0511 - val_loss: 0.1979 - val_acc: 0.9308 - val_f1: 0.0697\n",
      "\n",
      "Epoch 00014: val_f1 did not improve from 0.09140\n",
      "Epoch 15/20\n",
      "437/437 [==============================] - 136s 311ms/step - loss: 0.1731 - acc: 0.9414 - f1: 0.0523 - val_loss: 0.2199 - val_acc: 0.9388 - val_f1: 0.0859\n",
      "\n",
      "Epoch 00015: val_f1 did not improve from 0.09140\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 16/20\n",
      "437/437 [==============================] - 171s 392ms/step - loss: 0.1732 - acc: 0.9416 - f1: 0.0517 - val_loss: 0.2112 - val_acc: 0.9362 - val_f1: 0.0806\n",
      "\n",
      "Epoch 00016: val_f1 did not improve from 0.09140\n",
      "Epoch 17/20\n",
      "437/437 [==============================] - 134s 307ms/step - loss: 0.1722 - acc: 0.9415 - f1: 0.0526 - val_loss: 0.2052 - val_acc: 0.9335 - val_f1: 0.0729\n",
      "\n",
      "Epoch 00017: val_f1 did not improve from 0.09140\n",
      "Epoch 18/20\n",
      "437/437 [==============================] - 134s 307ms/step - loss: 0.1714 - acc: 0.9421 - f1: 0.0537 - val_loss: 0.2090 - val_acc: 0.9340 - val_f1: 0.0782\n",
      "\n",
      "Epoch 00018: val_f1 did not improve from 0.09140\n",
      "Epoch 19/20\n",
      "437/437 [==============================] - 135s 310ms/step - loss: 0.1712 - acc: 0.9419 - f1: 0.0547 - val_loss: 0.2143 - val_acc: 0.9285 - val_f1: 0.0790\n",
      "\n",
      "Epoch 00019: val_f1 did not improve from 0.09140\n",
      "Epoch 20/20\n",
      "437/437 [==============================] - 192s 440ms/step - loss: 0.1713 - acc: 0.9421 - f1: 0.0563 - val_loss: 0.2169 - val_acc: 0.9359 - val_f1: 0.0813\n",
      "\n",
      "Epoch 00020: val_f1 did not improve from 0.09140\n"
     ]
    }
   ],
   "source": [
    "hist =  model.fit_generator(\n",
    "        tg,\n",
    "        steps_per_epoch=np.ceil(float(len(pathsTrain)) / float(batch_size))/2,\n",
    "        validation_data=vg,\n",
    "        validation_steps=np.ceil(float(len(pathsVal)) / float(batch_size))/2,\n",
    "        epochs=20, \n",
    "        verbose=1,\n",
    "        callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x0000024E847270B8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000024E84727DD8>\n",
      "<keras.engine.training.Model object at 0x000002634968AE10>\n",
      "<keras.layers.core.Dropout object at 0x00000263496B7358>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002634F2B8F28>\n",
      "<keras.layers.core.Flatten object at 0x000002634E2E5898>\n",
      "<keras.layers.core.Dropout object at 0x0000026362876208>\n",
      "<keras.layers.core.Dense object at 0x000002636283BE48>\n",
      "<keras.layers.core.Dropout object at 0x000002636296FD30>\n",
      "<keras.layers.core.Dense object at 0x000002636296F908>\n",
      "Epoch 1/100\n",
      "1166/1165 [==============================] - 456s 391ms/step - loss: 0.1706 - acc: 0.9421 - f1: 0.0556 - val_loss: 0.2084 - val_acc: 0.9306 - val_f1: 0.0787\n",
      "\n",
      "Epoch 00001: val_f1 did not improve from 0.09140\n",
      "Epoch 2/100\n",
      "1166/1165 [==============================] - 406s 348ms/step - loss: 0.1698 - acc: 0.9422 - f1: 0.0577 - val_loss: 0.2168 - val_acc: 0.9315 - val_f1: 0.0878\n",
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.09140\n",
      "Epoch 3/100\n",
      "1166/1165 [==============================] - 430s 369ms/step - loss: 0.1694 - acc: 0.9422 - f1: 0.0590 - val_loss: 0.2076 - val_acc: 0.9318 - val_f1: 0.0786\n",
      "\n",
      "Epoch 00003: val_f1 did not improve from 0.09140\n",
      "Epoch 4/100\n",
      "1166/1165 [==============================] - 403s 346ms/step - loss: 0.1686 - acc: 0.9422 - f1: 0.0609 - val_loss: 0.2157 - val_acc: 0.9346 - val_f1: 0.0845\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.09140\n",
      "Epoch 5/100\n",
      "1166/1165 [==============================] - 410s 351ms/step - loss: 0.1674 - acc: 0.9426 - f1: 0.0639 - val_loss: 0.2184 - val_acc: 0.9286 - val_f1: 0.0771\n",
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.09140\n",
      "Epoch 6/100\n",
      "1166/1165 [==============================] - 466s 399ms/step - loss: 0.1671 - acc: 0.9426 - f1: 0.0646 - val_loss: 0.2201 - val_acc: 0.9357 - val_f1: 0.0868\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.09140\n",
      "Epoch 7/100\n",
      "1166/1165 [==============================] - 418s 358ms/step - loss: 0.1661 - acc: 0.9428 - f1: 0.0678 - val_loss: 0.2110 - val_acc: 0.9349 - val_f1: 0.0824\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.09140\n",
      "Epoch 8/100\n",
      "1166/1165 [==============================] - 387s 332ms/step - loss: 0.1655 - acc: 0.9428 - f1: 0.0715 - val_loss: 0.2237 - val_acc: 0.9355 - val_f1: 0.0876\n",
      "\n",
      "Epoch 00008: val_f1 did not improve from 0.09140\n",
      "Epoch 9/100\n",
      "1166/1165 [==============================] - 416s 357ms/step - loss: 0.1648 - acc: 0.9432 - f1: 0.0732 - val_loss: 0.2211 - val_acc: 0.9364 - val_f1: 0.0869\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.09140\n",
      "Epoch 10/100\n",
      "1166/1165 [==============================] - 397s 340ms/step - loss: 0.1638 - acc: 0.9432 - f1: 0.0777 - val_loss: 0.2134 - val_acc: 0.9334 - val_f1: 0.0801\n",
      "\n",
      "Epoch 00010: val_f1 did not improve from 0.09140\n",
      "Epoch 11/100\n",
      "1166/1165 [==============================] - 388s 333ms/step - loss: 0.1631 - acc: 0.9433 - f1: 0.0801 - val_loss: 0.2166 - val_acc: 0.9375 - val_f1: 0.0878\n",
      "\n",
      "Epoch 00011: val_f1 did not improve from 0.09140\n",
      "Epoch 12/100\n",
      "1166/1165 [==============================] - 403s 345ms/step - loss: 0.1624 - acc: 0.9433 - f1: 0.0844 - val_loss: 0.2144 - val_acc: 0.9342 - val_f1: 0.0827\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.09140\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 13/100\n",
      "1166/1165 [==============================] - 421s 361ms/step - loss: 0.1605 - acc: 0.9438 - f1: 0.0900 - val_loss: 0.2191 - val_acc: 0.9362 - val_f1: 0.0889\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.09140\n",
      "Epoch 14/100\n",
      "1166/1165 [==============================] - 398s 342ms/step - loss: 0.1598 - acc: 0.9439 - f1: 0.0938 - val_loss: 0.2188 - val_acc: 0.9363 - val_f1: 0.0895\n",
      "\n",
      "Epoch 00014: val_f1 did not improve from 0.09140\n",
      "Epoch 15/100\n",
      "1166/1165 [==============================] - 1370s 1s/step - loss: 0.1594 - acc: 0.9439 - f1: 0.0961 - val_loss: 0.2190 - val_acc: 0.9346 - val_f1: 0.0844\n",
      "\n",
      "Epoch 00015: val_f1 did not improve from 0.09140\n",
      "Epoch 16/100\n",
      "1166/1165 [==============================] - 420s 360ms/step - loss: 0.1586 - acc: 0.9442 - f1: 0.1000 - val_loss: 0.2153 - val_acc: 0.9352 - val_f1: 0.0833\n",
      "\n",
      "Epoch 00016: val_f1 did not improve from 0.09140\n",
      "Epoch 17/100\n",
      "1166/1165 [==============================] - 393s 337ms/step - loss: 0.1582 - acc: 0.9444 - f1: 0.1012 - val_loss: 0.2183 - val_acc: 0.9357 - val_f1: 0.0871\n",
      "\n",
      "Epoch 00017: val_f1 did not improve from 0.09140\n",
      "Epoch 18/100\n",
      "1166/1165 [==============================] - 430s 369ms/step - loss: 0.1578 - acc: 0.9444 - f1: 0.1029 - val_loss: 0.2181 - val_acc: 0.9346 - val_f1: 0.0824\n",
      "\n",
      "Epoch 00018: val_f1 did not improve from 0.09140\n",
      "Epoch 19/100\n",
      "1166/1165 [==============================] - 415s 356ms/step - loss: 0.1567 - acc: 0.9447 - f1: 0.1066 - val_loss: 0.2185 - val_acc: 0.9344 - val_f1: 0.0853\n",
      "\n",
      "Epoch 00019: val_f1 did not improve from 0.09140\n",
      "Epoch 20/100\n",
      "1166/1165 [==============================] - 383s 328ms/step - loss: 0.1564 - acc: 0.9446 - f1: 0.1096 - val_loss: 0.2158 - val_acc: 0.9331 - val_f1: 0.0851\n",
      "\n",
      "Epoch 00020: val_f1 did not improve from 0.09140\n",
      "Epoch 21/100\n",
      "1166/1165 [==============================] - 455s 390ms/step - loss: 0.1561 - acc: 0.9448 - f1: 0.1121 - val_loss: 0.2189 - val_acc: 0.9333 - val_f1: 0.0857\n",
      "\n",
      "Epoch 00021: val_f1 did not improve from 0.09140\n",
      "Epoch 22/100\n",
      "1166/1165 [==============================] - 409s 351ms/step - loss: 0.1553 - acc: 0.9450 - f1: 0.1166 - val_loss: 0.2159 - val_acc: 0.9320 - val_f1: 0.0817\n",
      "\n",
      "Epoch 00022: val_f1 did not improve from 0.09140\n",
      "Epoch 23/100\n",
      "1166/1165 [==============================] - 394s 338ms/step - loss: 0.1545 - acc: 0.9452 - f1: 0.1187 - val_loss: 0.2143 - val_acc: 0.9290 - val_f1: 0.0789\n",
      "\n",
      "Epoch 00023: val_f1 did not improve from 0.09140\n",
      "Epoch 24/100\n",
      "1166/1165 [==============================] - 413s 354ms/step - loss: 0.1541 - acc: 0.9453 - f1: 0.1203 - val_loss: 0.2132 - val_acc: 0.9327 - val_f1: 0.0830\n",
      "\n",
      "Epoch 00024: val_f1 did not improve from 0.09140\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 25/100\n",
      "1166/1165 [==============================] - 428s 367ms/step - loss: 0.1533 - acc: 0.9455 - f1: 0.1267 - val_loss: 0.2148 - val_acc: 0.9327 - val_f1: 0.0842\n",
      "\n",
      "Epoch 00025: val_f1 did not improve from 0.09140\n",
      "Epoch 26/100\n",
      "1166/1165 [==============================] - 374s 321ms/step - loss: 0.1522 - acc: 0.9458 - f1: 0.1286 - val_loss: 0.2164 - val_acc: 0.9330 - val_f1: 0.0849\n",
      "\n",
      "Epoch 00026: val_f1 did not improve from 0.09140\n",
      "Epoch 27/100\n",
      "1166/1165 [==============================] - 427s 366ms/step - loss: 0.1522 - acc: 0.9456 - f1: 0.1289 - val_loss: 0.2160 - val_acc: 0.9330 - val_f1: 0.0846\n",
      "\n",
      "Epoch 00027: val_f1 did not improve from 0.09140\n",
      "Epoch 28/100\n",
      "1166/1165 [==============================] - 434s 373ms/step - loss: 0.1517 - acc: 0.9459 - f1: 0.1306 - val_loss: 0.2161 - val_acc: 0.9330 - val_f1: 0.0865\n",
      "\n",
      "Epoch 00028: val_f1 did not improve from 0.09140\n",
      "Epoch 29/100\n",
      "1166/1165 [==============================] - 414s 355ms/step - loss: 0.1514 - acc: 0.9459 - f1: 0.1325 - val_loss: 0.2167 - val_acc: 0.9329 - val_f1: 0.0839\n",
      "\n",
      "Epoch 00029: val_f1 did not improve from 0.09140\n",
      "Epoch 30/100\n",
      "1166/1165 [==============================] - 432s 371ms/step - loss: 0.1515 - acc: 0.9461 - f1: 0.1329 - val_loss: 0.2160 - val_acc: 0.9333 - val_f1: 0.0857\n",
      "\n",
      "Epoch 00030: val_f1 did not improve from 0.09140\n",
      "Epoch 31/100\n",
      "1166/1165 [==============================] - 455s 390ms/step - loss: 0.1508 - acc: 0.9461 - f1: 0.1355 - val_loss: 0.2170 - val_acc: 0.9328 - val_f1: 0.0854\n",
      "\n",
      "Epoch 00031: val_f1 did not improve from 0.09140\n",
      "Epoch 32/100\n",
      "1166/1165 [==============================] - 397s 341ms/step - loss: 0.1505 - acc: 0.9462 - f1: 0.1357 - val_loss: 0.2176 - val_acc: 0.9314 - val_f1: 0.0848\n",
      "\n",
      "Epoch 00032: val_f1 did not improve from 0.09140\n",
      "Epoch 33/100\n",
      "1166/1165 [==============================] - 429s 368ms/step - loss: 0.1504 - acc: 0.9462 - f1: 0.1384 - val_loss: 0.2194 - val_acc: 0.9318 - val_f1: 0.0857\n",
      "\n",
      "Epoch 00033: val_f1 did not improve from 0.09140\n",
      "Epoch 34/100\n",
      "1166/1165 [==============================] - 432s 371ms/step - loss: 0.1500 - acc: 0.9463 - f1: 0.1408 - val_loss: 0.2157 - val_acc: 0.9314 - val_f1: 0.0845\n",
      "\n",
      "Epoch 00034: val_f1 did not improve from 0.09140\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    layer.trainable = False\n",
    "model.layers[2].layers[1].trainable = True\n",
    "batch_size = 12\n",
    "hist =  model.fit_generator(\n",
    "        tg,\n",
    "        steps_per_epoch=np.ceil(float(len(pathsTrain)) / float(batch_size))/2,\n",
    "        validation_data=vg,\n",
    "        validation_steps=np.ceil(float(len(pathsVal)) / float(batch_size))/2,\n",
    "        epochs=100, \n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x0000024E847270B8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x0000024E84727DD8>\n",
      "<keras.engine.training.Model object at 0x000002634968AE10>\n",
      "<keras.layers.core.Dropout object at 0x00000263496B7358>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002634F2B8F28>\n",
      "<keras.layers.core.Flatten object at 0x000002634E2E5898>\n",
      "<keras.layers.core.Dropout object at 0x0000026362876208>\n",
      "<keras.layers.core.Dense object at 0x000002636283BE48>\n",
      "<keras.layers.core.Dropout object at 0x000002636296FD30>\n",
      "<keras.layers.core.Dense object at 0x000002636296F908>\n",
      "Epoch 1/100\n",
      "1166/1165 [==============================] - 566s 486ms/step - loss: 0.1728 - acc: 0.9419 - f1: 0.0585 - val_loss: 0.1714 - val_acc: 0.9425 - val_f1: 0.0829\n",
      "\n",
      "Epoch 00001: val_f1 did not improve from 0.09140\n",
      "Epoch 2/100\n",
      "1166/1165 [==============================] - 523s 448ms/step - loss: 0.1510 - acc: 0.9492 - f1: 0.1260 - val_loss: 0.1394 - val_acc: 0.9534 - val_f1: 0.2022\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.09140 to 0.20218, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 3/100\n",
      "1166/1165 [==============================] - 542s 465ms/step - loss: 0.1257 - acc: 0.9567 - f1: 0.2243 - val_loss: 0.1261 - val_acc: 0.9567 - val_f1: 0.2337\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.20218 to 0.23368, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 4/100\n",
      "1166/1165 [==============================] - 519s 445ms/step - loss: 0.1073 - acc: 0.9626 - f1: 0.2805 - val_loss: 0.1223 - val_acc: 0.9568 - val_f1: 0.2568\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.23368 to 0.25676, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 5/100\n",
      "1166/1165 [==============================] - 526s 452ms/step - loss: 0.0944 - acc: 0.9666 - f1: 0.3181 - val_loss: 0.1091 - val_acc: 0.9622 - val_f1: 0.2920\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.25676 to 0.29204, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 6/100\n",
      "1166/1165 [==============================] - 530s 455ms/step - loss: 0.0830 - acc: 0.9705 - f1: 0.3497 - val_loss: 0.1131 - val_acc: 0.9612 - val_f1: 0.2941\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.29204 to 0.29414, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 7/100\n",
      "1166/1165 [==============================] - 609s 522ms/step - loss: 0.0693 - acc: 0.9753 - f1: 0.3853 - val_loss: 0.1196 - val_acc: 0.9618 - val_f1: 0.3000\n",
      "\n",
      "Epoch 00007: val_f1 improved from 0.29414 to 0.29999, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 8/100\n",
      "1166/1165 [==============================] - 531s 455ms/step - loss: 0.0607 - acc: 0.9787 - f1: 0.4077 - val_loss: 0.1244 - val_acc: 0.9606 - val_f1: 0.3124\n",
      "\n",
      "Epoch 00008: val_f1 improved from 0.29999 to 0.31237, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 9/100\n",
      "1166/1165 [==============================] - 524s 449ms/step - loss: 0.0539 - acc: 0.9811 - f1: 0.4268 - val_loss: 0.1240 - val_acc: 0.9618 - val_f1: 0.3096\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.31237\n",
      "Epoch 10/100\n",
      "1166/1165 [==============================] - 528s 453ms/step - loss: 0.0462 - acc: 0.9839 - f1: 0.4472 - val_loss: 0.1263 - val_acc: 0.9627 - val_f1: 0.3127\n",
      "\n",
      "Epoch 00010: val_f1 improved from 0.31237 to 0.31271, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 11/100\n",
      "1166/1165 [==============================] - 527s 452ms/step - loss: 0.0422 - acc: 0.9853 - f1: 0.4563 - val_loss: 0.1329 - val_acc: 0.9591 - val_f1: 0.3108\n",
      "\n",
      "Epoch 00011: val_f1 did not improve from 0.31271\n",
      "Epoch 12/100\n",
      "1166/1165 [==============================] - 550s 472ms/step - loss: 0.0382 - acc: 0.9868 - f1: 0.4682 - val_loss: 0.1465 - val_acc: 0.9609 - val_f1: 0.2903\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.31271\n",
      "Epoch 13/100\n",
      "1166/1165 [==============================] - 531s 455ms/step - loss: 0.0346 - acc: 0.9881 - f1: 0.4763 - val_loss: 0.1342 - val_acc: 0.9632 - val_f1: 0.3169\n",
      "\n",
      "Epoch 00013: val_f1 improved from 0.31271 to 0.31691, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 14/100\n",
      "1166/1165 [==============================] - 513s 440ms/step - loss: 0.0331 - acc: 0.9886 - f1: 0.4850 - val_loss: 0.1444 - val_acc: 0.9610 - val_f1: 0.3138\n",
      "\n",
      "Epoch 00014: val_f1 did not improve from 0.31691\n",
      "Epoch 15/100\n",
      "1166/1165 [==============================] - 541s 464ms/step - loss: 0.0314 - acc: 0.9892 - f1: 0.4838 - val_loss: 0.1379 - val_acc: 0.9605 - val_f1: 0.3130\n",
      "\n",
      "Epoch 00015: val_f1 did not improve from 0.31691\n",
      "Epoch 16/100\n",
      "1166/1165 [==============================] - 543s 466ms/step - loss: 0.0286 - acc: 0.9903 - f1: 0.4942 - val_loss: 0.1310 - val_acc: 0.9637 - val_f1: 0.3313\n",
      "\n",
      "Epoch 00016: val_f1 improved from 0.31691 to 0.33130, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 17/100\n",
      "1166/1165 [==============================] - 535s 459ms/step - loss: 0.0276 - acc: 0.9906 - f1: 0.4936 - val_loss: 0.1361 - val_acc: 0.9624 - val_f1: 0.3280\n",
      "\n",
      "Epoch 00017: val_f1 did not improve from 0.33130\n",
      "Epoch 18/100\n",
      "1166/1165 [==============================] - 534s 458ms/step - loss: 0.0271 - acc: 0.9908 - f1: 0.4963 - val_loss: 0.1466 - val_acc: 0.9625 - val_f1: 0.3084\n",
      "\n",
      "Epoch 00018: val_f1 did not improve from 0.33130\n",
      "Epoch 19/100\n",
      "1166/1165 [==============================] - 531s 455ms/step - loss: 0.0245 - acc: 0.9917 - f1: 0.5032 - val_loss: 0.1409 - val_acc: 0.9636 - val_f1: 0.3257\n",
      "\n",
      "Epoch 00019: val_f1 did not improve from 0.33130\n",
      "Epoch 20/100\n",
      "1166/1165 [==============================] - 535s 459ms/step - loss: 0.0240 - acc: 0.9919 - f1: 0.5045 - val_loss: 0.1405 - val_acc: 0.9611 - val_f1: 0.3200\n",
      "\n",
      "Epoch 00020: val_f1 did not improve from 0.33130\n",
      "Epoch 21/100\n",
      "1166/1165 [==============================] - 533s 457ms/step - loss: 0.0228 - acc: 0.9924 - f1: 0.5074 - val_loss: 0.1488 - val_acc: 0.9618 - val_f1: 0.3117\n",
      "\n",
      "Epoch 00021: val_f1 did not improve from 0.33130\n",
      "Epoch 22/100\n",
      "1166/1165 [==============================] - 523s 449ms/step - loss: 0.0214 - acc: 0.9928 - f1: 0.5088 - val_loss: 0.1358 - val_acc: 0.9631 - val_f1: 0.3269\n",
      "\n",
      "Epoch 00022: val_f1 did not improve from 0.33130\n",
      "Epoch 23/100\n",
      "1166/1165 [==============================] - 528s 453ms/step - loss: 0.0213 - acc: 0.9929 - f1: 0.5128 - val_loss: 0.1524 - val_acc: 0.9614 - val_f1: 0.3268\n",
      "\n",
      "Epoch 00023: val_f1 did not improve from 0.33130\n",
      "Epoch 24/100\n",
      "1166/1165 [==============================] - 536s 460ms/step - loss: 0.0204 - acc: 0.9931 - f1: 0.5120 - val_loss: 0.1491 - val_acc: 0.9616 - val_f1: 0.3290\n",
      "\n",
      "Epoch 00024: val_f1 did not improve from 0.33130\n",
      "Epoch 25/100\n",
      "1166/1165 [==============================] - 526s 451ms/step - loss: 0.0195 - acc: 0.9935 - f1: 0.5152 - val_loss: 0.1439 - val_acc: 0.9637 - val_f1: 0.3326\n",
      "\n",
      "Epoch 00025: val_f1 improved from 0.33130 to 0.33256, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 26/100\n",
      "1166/1165 [==============================] - 523s 449ms/step - loss: 0.0185 - acc: 0.9938 - f1: 0.5189 - val_loss: 0.1428 - val_acc: 0.9644 - val_f1: 0.3347\n",
      "\n",
      "Epoch 00026: val_f1 improved from 0.33256 to 0.33473, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 27/100\n",
      "1166/1165 [==============================] - 533s 457ms/step - loss: 0.0188 - acc: 0.9937 - f1: 0.5171 - val_loss: 0.1443 - val_acc: 0.9646 - val_f1: 0.3378\n",
      "\n",
      "Epoch 00027: val_f1 improved from 0.33473 to 0.33779, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 28/100\n",
      "1166/1165 [==============================] - 694s 595ms/step - loss: 0.0181 - acc: 0.9940 - f1: 0.5202 - val_loss: 0.1487 - val_acc: 0.9645 - val_f1: 0.3239\n",
      "\n",
      "Epoch 00028: val_f1 did not improve from 0.33779\n",
      "Epoch 29/100\n",
      "1166/1165 [==============================] - 500s 429ms/step - loss: 0.0175 - acc: 0.9942 - f1: 0.5214 - val_loss: 0.1511 - val_acc: 0.9648 - val_f1: 0.3323\n",
      "\n",
      "Epoch 00029: val_f1 did not improve from 0.33779\n",
      "Epoch 30/100\n",
      "1166/1165 [==============================] - 628s 538ms/step - loss: 0.0172 - acc: 0.9943 - f1: 0.5208 - val_loss: 0.1537 - val_acc: 0.9626 - val_f1: 0.3145\n",
      "\n",
      "Epoch 00030: val_f1 did not improve from 0.33779\n",
      "Epoch 31/100\n",
      "1166/1165 [==============================] - 524s 449ms/step - loss: 0.0159 - acc: 0.9947 - f1: 0.5238 - val_loss: 0.1510 - val_acc: 0.9636 - val_f1: 0.3305\n",
      "\n",
      "Epoch 00031: val_f1 did not improve from 0.33779\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1165 [==============================] - 520s 446ms/step - loss: 0.0161 - acc: 0.9946 - f1: 0.5235 - val_loss: 0.1421 - val_acc: 0.9644 - val_f1: 0.3429\n",
      "\n",
      "Epoch 00032: val_f1 improved from 0.33779 to 0.34292, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 33/100\n",
      "1166/1165 [==============================] - 557s 477ms/step - loss: 0.0161 - acc: 0.9946 - f1: 0.5249 - val_loss: 0.1502 - val_acc: 0.9646 - val_f1: 0.3307\n",
      "\n",
      "Epoch 00033: val_f1 did not improve from 0.34292\n",
      "Epoch 34/100\n",
      "1166/1165 [==============================] - 601s 515ms/step - loss: 0.0152 - acc: 0.9949 - f1: 0.5257 - val_loss: 0.1542 - val_acc: 0.9637 - val_f1: 0.3359\n",
      "\n",
      "Epoch 00034: val_f1 did not improve from 0.34292\n",
      "Epoch 35/100\n",
      "1166/1165 [==============================] - 521s 447ms/step - loss: 0.0145 - acc: 0.9951 - f1: 0.5266 - val_loss: 0.1609 - val_acc: 0.9629 - val_f1: 0.3313\n",
      "\n",
      "Epoch 00035: val_f1 did not improve from 0.34292\n",
      "Epoch 36/100\n",
      "1166/1165 [==============================] - 569s 488ms/step - loss: 0.0144 - acc: 0.9953 - f1: 0.5293 - val_loss: 0.1538 - val_acc: 0.9659 - val_f1: 0.3333\n",
      "\n",
      "Epoch 00036: val_f1 did not improve from 0.34292\n",
      "Epoch 37/100\n",
      "1166/1165 [==============================] - 538s 461ms/step - loss: 0.0139 - acc: 0.9954 - f1: 0.5272 - val_loss: 0.1516 - val_acc: 0.9650 - val_f1: 0.3349\n",
      "\n",
      "Epoch 00037: val_f1 did not improve from 0.34292\n",
      "Epoch 38/100\n",
      "1166/1165 [==============================] - 513s 440ms/step - loss: 0.0138 - acc: 0.9953 - f1: 0.5304 - val_loss: 0.1484 - val_acc: 0.9628 - val_f1: 0.3292\n",
      "\n",
      "Epoch 00038: val_f1 did not improve from 0.34292\n",
      "Epoch 39/100\n",
      "1166/1165 [==============================] - 546s 468ms/step - loss: 0.0131 - acc: 0.9956 - f1: 0.5308 - val_loss: 0.1502 - val_acc: 0.9649 - val_f1: 0.3388\n",
      "\n",
      "Epoch 00039: val_f1 did not improve from 0.34292\n",
      "Epoch 40/100\n",
      "1166/1165 [==============================] - 589s 505ms/step - loss: 0.0130 - acc: 0.9957 - f1: 0.5306 - val_loss: 0.1627 - val_acc: 0.9634 - val_f1: 0.3200\n",
      "\n",
      "Epoch 00040: val_f1 did not improve from 0.34292\n",
      "Epoch 41/100\n",
      "1166/1165 [==============================] - 521s 447ms/step - loss: 0.0131 - acc: 0.9956 - f1: 0.5329 - val_loss: 0.1398 - val_acc: 0.9656 - val_f1: 0.3442\n",
      "\n",
      "Epoch 00041: val_f1 improved from 0.34292 to 0.34423, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 42/100\n",
      "1166/1165 [==============================] - 572s 491ms/step - loss: 0.0125 - acc: 0.9959 - f1: 0.5310 - val_loss: 0.1513 - val_acc: 0.9646 - val_f1: 0.3417\n",
      "\n",
      "Epoch 00042: val_f1 did not improve from 0.34423\n",
      "Epoch 43/100\n",
      "1166/1165 [==============================] - 773s 663ms/step - loss: 0.0119 - acc: 0.9961 - f1: 0.5338 - val_loss: 0.1437 - val_acc: 0.9666 - val_f1: 0.3487\n",
      "\n",
      "Epoch 00043: val_f1 improved from 0.34423 to 0.34871, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 44/100\n",
      "1166/1165 [==============================] - 541s 464ms/step - loss: 0.0120 - acc: 0.9960 - f1: 0.5362 - val_loss: 0.1415 - val_acc: 0.9650 - val_f1: 0.3448\n",
      "\n",
      "Epoch 00044: val_f1 did not improve from 0.34871\n",
      "Epoch 45/100\n",
      "1166/1165 [==============================] - 538s 461ms/step - loss: 0.0118 - acc: 0.9961 - f1: 0.5337 - val_loss: 0.1453 - val_acc: 0.9677 - val_f1: 0.3469\n",
      "\n",
      "Epoch 00045: val_f1 did not improve from 0.34871\n",
      "Epoch 46/100\n",
      "1166/1165 [==============================] - 529s 454ms/step - loss: 0.0112 - acc: 0.9963 - f1: 0.5360 - val_loss: 0.1497 - val_acc: 0.9659 - val_f1: 0.3391\n",
      "\n",
      "Epoch 00046: val_f1 did not improve from 0.34871\n",
      "Epoch 47/100\n",
      "1166/1165 [==============================] - 509s 437ms/step - loss: 0.0114 - acc: 0.9963 - f1: 0.5354 - val_loss: 0.1426 - val_acc: 0.9672 - val_f1: 0.3456\n",
      "\n",
      "Epoch 00047: val_f1 did not improve from 0.34871\n",
      "Epoch 48/100\n",
      "1166/1165 [==============================] - 532s 456ms/step - loss: 0.0109 - acc: 0.9964 - f1: 0.5361 - val_loss: 0.1539 - val_acc: 0.9659 - val_f1: 0.3464\n",
      "\n",
      "Epoch 00048: val_f1 did not improve from 0.34871\n",
      "Epoch 49/100\n",
      "1166/1165 [==============================] - 680s 583ms/step - loss: 0.0108 - acc: 0.9965 - f1: 0.5341 - val_loss: 0.1565 - val_acc: 0.9639 - val_f1: 0.3374\n",
      "\n",
      "Epoch 00049: val_f1 did not improve from 0.34871\n",
      "Epoch 50/100\n",
      "1166/1165 [==============================] - 511s 438ms/step - loss: 0.0104 - acc: 0.9966 - f1: 0.5383 - val_loss: 0.1375 - val_acc: 0.9658 - val_f1: 0.3459\n",
      "\n",
      "Epoch 00050: val_f1 did not improve from 0.34871\n",
      "Epoch 51/100\n",
      "1166/1165 [==============================] - 531s 456ms/step - loss: 0.0108 - acc: 0.9964 - f1: 0.5378 - val_loss: 0.1444 - val_acc: 0.9655 - val_f1: 0.3462\n",
      "\n",
      "Epoch 00051: val_f1 did not improve from 0.34871\n",
      "Epoch 52/100\n",
      "1166/1165 [==============================] - 515s 442ms/step - loss: 0.0102 - acc: 0.9966 - f1: 0.5386 - val_loss: 0.1547 - val_acc: 0.9660 - val_f1: 0.3427\n",
      "\n",
      "Epoch 00052: val_f1 did not improve from 0.34871\n",
      "Epoch 53/100\n",
      "1166/1165 [==============================] - 522s 448ms/step - loss: 0.0098 - acc: 0.9967 - f1: 0.5376 - val_loss: 0.1447 - val_acc: 0.9668 - val_f1: 0.3527\n",
      "\n",
      "Epoch 00053: val_f1 improved from 0.34871 to 0.35271, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 54/100\n",
      "1166/1165 [==============================] - 532s 457ms/step - loss: 0.0099 - acc: 0.9967 - f1: 0.5395 - val_loss: 0.1505 - val_acc: 0.9669 - val_f1: 0.3523\n",
      "\n",
      "Epoch 00054: val_f1 did not improve from 0.35271\n",
      "Epoch 55/100\n",
      "1166/1165 [==============================] - 529s 453ms/step - loss: 0.0098 - acc: 0.9967 - f1: 0.5388 - val_loss: 0.1465 - val_acc: 0.9660 - val_f1: 0.3512\n",
      "\n",
      "Epoch 00055: val_f1 did not improve from 0.35271\n",
      "Epoch 56/100\n",
      "1166/1165 [==============================] - 528s 453ms/step - loss: 0.0093 - acc: 0.9969 - f1: 0.5401 - val_loss: 0.1527 - val_acc: 0.9651 - val_f1: 0.3453\n",
      "\n",
      "Epoch 00056: val_f1 did not improve from 0.35271\n",
      "Epoch 57/100\n",
      "1166/1165 [==============================] - 537s 461ms/step - loss: 0.0092 - acc: 0.9970 - f1: 0.5411 - val_loss: 0.1704 - val_acc: 0.9629 - val_f1: 0.3212\n",
      "\n",
      "Epoch 00057: val_f1 did not improve from 0.35271\n",
      "Epoch 58/100\n",
      "1166/1165 [==============================] - 520s 446ms/step - loss: 0.0091 - acc: 0.9970 - f1: 0.5432 - val_loss: 0.1481 - val_acc: 0.9665 - val_f1: 0.3499\n",
      "\n",
      "Epoch 00058: val_f1 did not improve from 0.35271\n",
      "Epoch 59/100\n",
      "1166/1165 [==============================] - 522s 448ms/step - loss: 0.0088 - acc: 0.9970 - f1: 0.5392 - val_loss: 0.1522 - val_acc: 0.9669 - val_f1: 0.3599\n",
      "\n",
      "Epoch 00059: val_f1 improved from 0.35271 to 0.35989, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 60/100\n",
      "1166/1165 [==============================] - 554s 475ms/step - loss: 0.0092 - acc: 0.9970 - f1: 0.5406 - val_loss: 0.1445 - val_acc: 0.9653 - val_f1: 0.3491\n",
      "\n",
      "Epoch 00060: val_f1 did not improve from 0.35989\n",
      "Epoch 61/100\n",
      "1166/1165 [==============================] - 526s 451ms/step - loss: 0.0084 - acc: 0.9973 - f1: 0.5437 - val_loss: 0.1584 - val_acc: 0.9655 - val_f1: 0.3428\n",
      "\n",
      "Epoch 00061: val_f1 did not improve from 0.35989\n",
      "Epoch 62/100\n",
      "1166/1165 [==============================] - 500s 429ms/step - loss: 0.0086 - acc: 0.9971 - f1: 0.5408 - val_loss: 0.1532 - val_acc: 0.9658 - val_f1: 0.3421\n",
      "\n",
      "Epoch 00062: val_f1 did not improve from 0.35989\n",
      "Epoch 63/100\n",
      "1166/1165 [==============================] - 555s 476ms/step - loss: 0.0083 - acc: 0.9973 - f1: 0.5429 - val_loss: 0.1540 - val_acc: 0.9661 - val_f1: 0.3510\n",
      "\n",
      "Epoch 00063: val_f1 did not improve from 0.35989\n",
      "Epoch 64/100\n",
      "1166/1165 [==============================] - 516s 443ms/step - loss: 0.0083 - acc: 0.9972 - f1: 0.5426 - val_loss: 0.1633 - val_acc: 0.9666 - val_f1: 0.3512\n",
      "\n",
      "Epoch 00064: val_f1 did not improve from 0.35989\n",
      "Epoch 65/100\n",
      "1166/1165 [==============================] - 504s 433ms/step - loss: 0.0084 - acc: 0.9972 - f1: 0.5423 - val_loss: 0.1653 - val_acc: 0.9658 - val_f1: 0.3433\n",
      "\n",
      "Epoch 00065: val_f1 did not improve from 0.35989\n",
      "Epoch 66/100\n",
      "1166/1165 [==============================] - 514s 441ms/step - loss: 0.0083 - acc: 0.9973 - f1: 0.5439 - val_loss: 0.1586 - val_acc: 0.9672 - val_f1: 0.3453\n",
      "\n",
      "Epoch 00066: val_f1 did not improve from 0.35989\n",
      "Epoch 67/100\n",
      "1166/1165 [==============================] - 523s 449ms/step - loss: 0.0078 - acc: 0.9975 - f1: 0.5426 - val_loss: 0.1570 - val_acc: 0.9663 - val_f1: 0.3492\n",
      "\n",
      "Epoch 00067: val_f1 did not improve from 0.35989\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1165 [==============================] - 502s 430ms/step - loss: 0.0077 - acc: 0.9976 - f1: 0.5437 - val_loss: 0.1854 - val_acc: 0.9656 - val_f1: 0.3417\n",
      "\n",
      "Epoch 00068: val_f1 did not improve from 0.35989\n",
      "Epoch 69/100\n",
      "1166/1165 [==============================] - 939s 805ms/step - loss: 0.0077 - acc: 0.9975 - f1: 0.5439 - val_loss: 0.1515 - val_acc: 0.9667 - val_f1: 0.3513\n",
      "\n",
      "Epoch 00069: val_f1 did not improve from 0.35989\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 70/100\n",
      "1166/1165 [==============================] - 534s 458ms/step - loss: 0.0048 - acc: 0.9984 - f1: 0.5492 - val_loss: 0.1485 - val_acc: 0.9698 - val_f1: 0.3704\n",
      "\n",
      "Epoch 00070: val_f1 improved from 0.35989 to 0.37042, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 71/100\n",
      "1166/1165 [==============================] - 506s 434ms/step - loss: 0.0038 - acc: 0.9987 - f1: 0.5545 - val_loss: 0.1616 - val_acc: 0.9681 - val_f1: 0.3581\n",
      "\n",
      "Epoch 00071: val_f1 did not improve from 0.37042\n",
      "Epoch 72/100\n",
      "1166/1165 [==============================] - 783s 672ms/step - loss: 0.0037 - acc: 0.9988 - f1: 0.5523 - val_loss: 0.1589 - val_acc: 0.9687 - val_f1: 0.3655\n",
      "\n",
      "Epoch 00072: val_f1 did not improve from 0.37042\n",
      "Epoch 73/100\n",
      "1166/1165 [==============================] - 652s 559ms/step - loss: 0.0038 - acc: 0.9988 - f1: 0.5533 - val_loss: 0.1626 - val_acc: 0.9692 - val_f1: 0.3625\n",
      "\n",
      "Epoch 00073: val_f1 did not improve from 0.37042\n",
      "Epoch 74/100\n",
      "1166/1165 [==============================] - 518s 444ms/step - loss: 0.0038 - acc: 0.9988 - f1: 0.5519 - val_loss: 0.1536 - val_acc: 0.9699 - val_f1: 0.3704\n",
      "\n",
      "Epoch 00074: val_f1 improved from 0.37042 to 0.37044, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 75/100\n",
      "1166/1165 [==============================] - 547s 469ms/step - loss: 0.0033 - acc: 0.9989 - f1: 0.5535 - val_loss: 0.1547 - val_acc: 0.9694 - val_f1: 0.3625\n",
      "\n",
      "Epoch 00075: val_f1 did not improve from 0.37044\n",
      "Epoch 76/100\n",
      "1166/1165 [==============================] - 537s 461ms/step - loss: 0.0032 - acc: 0.9989 - f1: 0.5517 - val_loss: 0.1668 - val_acc: 0.9696 - val_f1: 0.3722\n",
      "\n",
      "Epoch 00076: val_f1 improved from 0.37044 to 0.37222, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 77/100\n",
      "1166/1165 [==============================] - 510s 437ms/step - loss: 0.0034 - acc: 0.9989 - f1: 0.5539 - val_loss: 0.1673 - val_acc: 0.9695 - val_f1: 0.3668\n",
      "\n",
      "Epoch 00077: val_f1 did not improve from 0.37222\n",
      "Epoch 78/100\n",
      "1166/1165 [==============================] - 568s 487ms/step - loss: 0.0034 - acc: 0.9989 - f1: 0.5539 - val_loss: 0.1581 - val_acc: 0.9700 - val_f1: 0.3716\n",
      "\n",
      "Epoch 00078: val_f1 did not improve from 0.37222\n",
      "Epoch 79/100\n",
      "1166/1165 [==============================] - 553s 475ms/step - loss: 0.0031 - acc: 0.9990 - f1: 0.5526 - val_loss: 0.1729 - val_acc: 0.9677 - val_f1: 0.3554\n",
      "\n",
      "Epoch 00079: val_f1 did not improve from 0.37222\n",
      "Epoch 80/100\n",
      "1166/1165 [==============================] - 516s 442ms/step - loss: 0.0031 - acc: 0.9989 - f1: 0.5534 - val_loss: 0.1632 - val_acc: 0.9702 - val_f1: 0.3698\n",
      "\n",
      "Epoch 00080: val_f1 did not improve from 0.37222\n",
      "Epoch 81/100\n",
      "1166/1165 [==============================] - 638s 547ms/step - loss: 0.0033 - acc: 0.9989 - f1: 0.5551 - val_loss: 0.1535 - val_acc: 0.9699 - val_f1: 0.3688\n",
      "\n",
      "Epoch 00081: val_f1 did not improve from 0.37222\n",
      "Epoch 82/100\n",
      "1166/1165 [==============================] - 519s 445ms/step - loss: 0.0030 - acc: 0.9990 - f1: 0.5549 - val_loss: 0.1568 - val_acc: 0.9702 - val_f1: 0.3682\n",
      "\n",
      "Epoch 00082: val_f1 did not improve from 0.37222\n",
      "Epoch 83/100\n",
      "1166/1165 [==============================] - 511s 438ms/step - loss: 0.0029 - acc: 0.9990 - f1: 0.5537 - val_loss: 0.1711 - val_acc: 0.9699 - val_f1: 0.3640\n",
      "\n",
      "Epoch 00083: val_f1 did not improve from 0.37222\n",
      "Epoch 84/100\n",
      "1166/1165 [==============================] - 549s 471ms/step - loss: 0.0029 - acc: 0.9990 - f1: 0.5549 - val_loss: 0.1784 - val_acc: 0.9698 - val_f1: 0.3593\n",
      "\n",
      "Epoch 00084: val_f1 did not improve from 0.37222\n",
      "Epoch 85/100\n",
      "1166/1165 [==============================] - 533s 457ms/step - loss: 0.0030 - acc: 0.9990 - f1: 0.5547 - val_loss: 0.1724 - val_acc: 0.9694 - val_f1: 0.3629\n",
      "\n",
      "Epoch 00085: val_f1 did not improve from 0.37222\n",
      "Epoch 86/100\n",
      "1166/1165 [==============================] - 503s 431ms/step - loss: 0.0029 - acc: 0.9990 - f1: 0.5541 - val_loss: 0.1740 - val_acc: 0.9697 - val_f1: 0.3650\n",
      "\n",
      "Epoch 00086: val_f1 did not improve from 0.37222\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 87/100\n",
      "1166/1165 [==============================] - 633s 543ms/step - loss: 0.0021 - acc: 0.9993 - f1: 0.5558 - val_loss: 0.1649 - val_acc: 0.9708 - val_f1: 0.3769\n",
      "\n",
      "Epoch 00087: val_f1 improved from 0.37222 to 0.37693, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 88/100\n",
      "1166/1165 [==============================] - 502s 430ms/step - loss: 0.0018 - acc: 0.9994 - f1: 0.5560 - val_loss: 0.1702 - val_acc: 0.9709 - val_f1: 0.3721\n",
      "\n",
      "Epoch 00088: val_f1 did not improve from 0.37693\n",
      "Epoch 89/100\n",
      "1166/1165 [==============================] - 629s 539ms/step - loss: 0.0017 - acc: 0.9995 - f1: 0.5573 - val_loss: 0.1730 - val_acc: 0.9712 - val_f1: 0.3714\n",
      "\n",
      "Epoch 00089: val_f1 did not improve from 0.37693\n",
      "Epoch 90/100\n",
      "1166/1165 [==============================] - 545s 467ms/step - loss: 0.0016 - acc: 0.9995 - f1: 0.5579 - val_loss: 0.1769 - val_acc: 0.9708 - val_f1: 0.3704\n",
      "\n",
      "Epoch 00090: val_f1 did not improve from 0.37693\n",
      "Epoch 91/100\n",
      "1166/1165 [==============================] - 518s 444ms/step - loss: 0.0017 - acc: 0.9995 - f1: 0.5566 - val_loss: 0.1760 - val_acc: 0.9708 - val_f1: 0.3718\n",
      "\n",
      "Epoch 00091: val_f1 did not improve from 0.37693\n",
      "Epoch 92/100\n",
      "1166/1165 [==============================] - 512s 439ms/step - loss: 0.0015 - acc: 0.9995 - f1: 0.5566 - val_loss: 0.1765 - val_acc: 0.9704 - val_f1: 0.3700\n",
      "\n",
      "Epoch 00092: val_f1 did not improve from 0.37693\n",
      "Epoch 93/100\n",
      "1166/1165 [==============================] - 536s 460ms/step - loss: 0.0015 - acc: 0.9995 - f1: 0.5590 - val_loss: 0.1776 - val_acc: 0.9703 - val_f1: 0.3733\n",
      "\n",
      "Epoch 00093: val_f1 did not improve from 0.37693\n",
      "Epoch 94/100\n",
      "1166/1165 [==============================] - 515s 442ms/step - loss: 0.0013 - acc: 0.9996 - f1: 0.5561 - val_loss: 0.1793 - val_acc: 0.9704 - val_f1: 0.3707\n",
      "\n",
      "Epoch 00094: val_f1 did not improve from 0.37693\n",
      "Epoch 95/100\n",
      "1166/1165 [==============================] - 520s 446ms/step - loss: 0.0014 - acc: 0.9995 - f1: 0.5586 - val_loss: 0.1814 - val_acc: 0.9706 - val_f1: 0.3686\n",
      "\n",
      "Epoch 00095: val_f1 did not improve from 0.37693\n",
      "Epoch 96/100\n",
      "1166/1165 [==============================] - 518s 445ms/step - loss: 0.0013 - acc: 0.9996 - f1: 0.5573 - val_loss: 0.1875 - val_acc: 0.9705 - val_f1: 0.3685\n",
      "\n",
      "Epoch 00096: val_f1 did not improve from 0.37693\n",
      "Epoch 97/100\n",
      "1166/1165 [==============================] - 521s 447ms/step - loss: 0.0014 - acc: 0.9995 - f1: 0.5582 - val_loss: 0.1878 - val_acc: 0.9707 - val_f1: 0.3679\n",
      "\n",
      "Epoch 00097: val_f1 did not improve from 0.37693\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 98/100\n",
      "1166/1165 [==============================] - 521s 447ms/step - loss: 0.0011 - acc: 0.9996 - f1: 0.5563 - val_loss: 0.1834 - val_acc: 0.9709 - val_f1: 0.3740\n",
      "\n",
      "Epoch 00098: val_f1 did not improve from 0.37693\n",
      "Epoch 99/100\n",
      "1166/1165 [==============================] - 550s 472ms/step - loss: 0.0010 - acc: 0.9997 - f1: 0.5596 - val_loss: 0.1821 - val_acc: 0.9709 - val_f1: 0.3735\n",
      "\n",
      "Epoch 00099: val_f1 did not improve from 0.37693\n",
      "Epoch 100/100\n",
      "1166/1165 [==============================] - 519s 445ms/step - loss: 9.4409e-04 - acc: 0.9997 - f1: 0.5584 - val_loss: 0.1850 - val_acc: 0.9709 - val_f1: 0.3737\n",
      "\n",
      "Epoch 00100: val_f1 did not improve from 0.37693\n"
     ]
    }
   ],
   "source": [
    "# train all layers\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    layer.trainable = True\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=['accuracy', f1])\n",
    "batch_size = 12\n",
    "hist =  model.fit_generator(\n",
    "        tg,\n",
    "        steps_per_epoch=np.ceil(float(len(pathsTrain)) / float(batch_size))/2,\n",
    "        validation_data=vg,\n",
    "        validation_steps=np.ceil(float(len(pathsVal)) / float(batch_size))/2,\n",
    "        epochs=100, \n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=f1_loss,\n",
    "#             optimizer=Adam(lr=1e-4),\n",
    "#             metrics=['accuracy', f1])\n",
    "# hist =  model.fit_generator(\n",
    "#         tg,\n",
    "#         steps_per_epoch=np.ceil(float(len(pathsTrain)) / float(batch_size))/2,\n",
    "#         validation_data=vg,\n",
    "#         validation_steps=np.ceil(float(len(pathsVal)) / float(batch_size))/2,\n",
    "#         epochs=200, \n",
    "#         verbose=1,\n",
    "#         callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x250a7bd6860>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].set_title('loss')\n",
    "ax[0].plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\n",
    "ax[0].plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax[1].set_title('acc')\n",
    "ax[1].plot(hist.epoch, hist.history[\"f1\"], label=\"Train F1\")\n",
    "ax[1].plot(hist.epoch, hist.history[\"val_f1\"], label=\"Validation F1\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "bestModel = load_model('../working/InceptionV3.h5', custom_objects={'f1': f1, 'f1_loss': f1_loss, 'focal_loss_fixed':focal_loss()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "lastFullValPred = np.empty((0, 28))\n",
    "lastFullValLabels = np.empty((0, 28))\n",
    "for i in tqdm(range(len(vg))): \n",
    "    im, lbl = vg[i]\n",
    "    scores = bestModel.predict(im)\n",
    "    lastFullValPred = np.append(lastFullValPred, scores, axis=0)\n",
    "    lastFullValLabels = np.append(lastFullValLabels, lbl, axis=0)\n",
    "print(lastFullValPred.shape, lastFullValLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score as off1\n",
    "rng = np.arange(0, 1, 0.001)\n",
    "f1s = np.zeros((rng.shape[0], 28))\n",
    "for j,t in enumerate(tqdm(rng)):\n",
    "    for i in range(28):\n",
    "        p = np.array(lastFullValPred[:,i]>t, dtype=np.int8)\n",
    "        scoref1 = off1(lastFullValLabels[:,i], p, average='binary')\n",
    "        f1s[j,i] = scoref1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Individual F1-scores for each class:')\n",
    "print(np.max(f1s, axis=0))\n",
    "print('Macro F1-score CV =', np.mean(np.max(f1s, axis=0)))\n",
    "plt.plot(rng, f1s)\n",
    "T = np.empty(28)\n",
    "for i in range(28):\n",
    "    T[i] = rng[np.where(f1s[:,i] == np.max(f1s[:,i]))[0][0]]\n",
    "print('Probability threshold maximizing CV F1-score for each class:')\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathsTest, labelsTest = getTestDataset()\n",
    "\n",
    "testg = ProteinDataGenerator(pathsTest, labelsTest, batch_size, SHAPE, channels)\n",
    "submit = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "P = np.zeros((pathsTest.shape[0], 28))\n",
    "for i in tqdm(range(len(testg))):\n",
    "    images, labels = testg[i]\n",
    "    score = bestModel.predict(images)\n",
    "    P[i*batch_size:i*batch_size+score.shape[0]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP = np.array(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "\n",
    "for row in tqdm(range(submit.shape[0])):\n",
    "    \n",
    "    str_label = ''\n",
    "    \n",
    "    for col in range(PP.shape[1]):\n",
    "        if(PP[row, col] < T[col]):\n",
    "            str_label += ''\n",
    "        else:\n",
    "            str_label += str(col) + ' '\n",
    "    prediction.append(str_label.strip())\n",
    "    \n",
    "submit['Predicted'] = np.array(prediction)\n",
    "submit.to_csv('transfer_1x1conv_aug_focal_loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# pathsTest, labelsTest = getTestDataset()\n",
    "\n",
    "# testg = ProteinDataGenerator(pathsTest, labelsTest, batch_size, SHAPE)\n",
    "# submit = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "# P = np.zeros((pathsTest.shape[0], 28))\n",
    "# for i in tqdm(range(len(testg))):\n",
    "#     images, labels = testg[i]\n",
    "#     score = bestModel.predict(images)\n",
    "#     P[i*batch_size:i*batch_size+score.shape[0]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PP = np.array(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = []\n",
    "\n",
    "# for row in tqdm(range(submit.shape[0])):\n",
    "    \n",
    "#     str_label = ''\n",
    "    \n",
    "#     for col in range(PP.shape[1]):\n",
    "#         if(PP[row, col] < .2):   # to account for losing TP is more costly than decreasing FP\n",
    "#             #print(PP[row])\n",
    "#             str_label += ''\n",
    "#         else:\n",
    "#             str_label += str(col) + ' '\n",
    "#     prediction.append(str_label.strip())\n",
    "    \n",
    "# submit['Predicted'] = np.array(prediction)\n",
    "# submit.to_csv('datagenerator_model_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
