{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "import keras\n",
    "import warnings\n",
    "from keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SIZE = 299\n",
    "SEED = 777\n",
    "THRESHOLD = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset info\n",
    "DIR = '../input/'\n",
    "data = pd.read_csv('../input/train.csv')\n",
    "\n",
    "# train_dataset_info = []\n",
    "# for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "#     train_dataset_info.append({\n",
    "#         'path':os.path.join(path_to_train, name),\n",
    "#         'labels':np.array([int(label) for label in labels])})\n",
    "# train_dataset_info = np.array(train_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainDataset():\n",
    "    \n",
    "    path_to_train = DIR + '/train/'\n",
    "    data = pd.read_csv(DIR + '/train.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name, lbl in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "        y = np.zeros(28)\n",
    "        for key in lbl:\n",
    "            y[int(key)] = 1\n",
    "        paths.append(os.path.join(path_to_train, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)\n",
    "\n",
    "def getTestDataset():\n",
    "    \n",
    "    path_to_test = DIR + '/test/'\n",
    "    data = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name in data['Id']:\n",
    "        y = np.ones(28)\n",
    "        paths.append(os.path.join(path_to_test, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)\n",
    "paths, labels = getTrainDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credits: https://github.com/keras-team/keras/blob/master/keras/utils/data_utils.py#L302\n",
    "# credits: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "from random import randint\n",
    "class ProteinDataGenerator(keras.utils.Sequence):\n",
    "            \n",
    "    def __init__(self, paths, labels, batch_size, shape, channels = [], shuffle = False, use_cache = False, augmentor = False):\n",
    "        self.paths, self.labels = paths, labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shape = shape\n",
    "        self.shuffle = shuffle\n",
    "        self.use_cache = use_cache\n",
    "        self.channels = channels\n",
    "        self.augmentor = augmentor\n",
    "        self.clahe = cv2.createCLAHE()\n",
    "        if use_cache == True:\n",
    "            self.cache = np.zeros((paths.shape[0], shape[0], shape[1], len(channels)))\n",
    "            self.is_cached = np.zeros((paths.shape[0]))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "\n",
    "        paths = self.paths[indexes]\n",
    "        X = np.zeros((paths.shape[0], self.shape[0], self.shape[1], self.shape[2]))\n",
    "        # Generate data\n",
    "        if self.use_cache == True:\n",
    "            X = self.cache[indexes]\n",
    "            for i, path in enumerate(paths[np.where(self.is_cached[indexes] == 0)]):\n",
    "                image = self.__load_image(path)\n",
    "                self.is_cached[indexes[i]] = 1\n",
    "                self.cache[indexes[i]] = image\n",
    "                X[i] = image\n",
    "        else:\n",
    "            for i, path in enumerate(paths):\n",
    "                X[i] = self.__load_image(path)\n",
    "        if self.augmentor == True:\n",
    "            for i, item in enumerate(X):\n",
    "                X[i] = self.augment(item)\n",
    "        y = self.labels[indexes]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        # Updates indexes after each epoch\n",
    "        self.indexes = np.arange(len(self.paths))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "        for item in (self[i] for i in range(len(self))):\n",
    "            yield item\n",
    "            \n",
    "    def __load_image(self, path):\n",
    "        images = []\n",
    "        for channel in self.channels:\n",
    "            im = np.array(Image.open(path + '_' + channel + '.png'))\n",
    "            \n",
    "#             im = clahe.apply(im)\n",
    "            images.append(im)\n",
    "            \n",
    "        if len(self.channels) >= 2:\n",
    "            im = np.stack((\n",
    "                images\n",
    "            ), -1)\n",
    "            im = cv2.resize(im, (SIZE,SIZE))\n",
    "            im = np.divide(im, 255)\n",
    "\n",
    "        else:\n",
    "            im = images[0]\n",
    "            im = cv2.resize(im, (SIZE,SIZE))\n",
    "            im = np.divide(im, 255)\n",
    "            im = np.expand_dims(im, 2)\n",
    "        return im\n",
    "    def augment(self, image):\n",
    "        if randint(0,1) == 1:\n",
    "            augment_img = iaa.Sequential([\n",
    "                iaa.OneOf([\n",
    "                    iaa.Fliplr(0.5), # horizontal flips\n",
    "                    iaa.Flipud(0.5), # horizontal flips\n",
    "                    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "                    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "                    # But we only blur about 50% of all images.\n",
    "                    iaa.Sometimes(0.5,\n",
    "                        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "                    ),\n",
    "                    # Make some images brighter and some darker.\n",
    "                    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "                    # which can end up changing the color of the images.\n",
    "                    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "                    # Apply affine transformations to each image.\n",
    "                    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "                    iaa.Affine(\n",
    "                        scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)},\n",
    "                        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
    "                        rotate=(-180, 180),\n",
    "                        shear=(-4, 4)\n",
    "                    )\n",
    "                ])], random_order=True)\n",
    "\n",
    "\n",
    "            image_aug = augment_img.augment_image(image)\n",
    "            return image_aug\n",
    "        else:\n",
    "            return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPE = (299, 299, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n",
      "red\n",
      "(299, 299, 1)\n"
     ]
    }
   ],
   "source": [
    "channels = [\"red\"]\n",
    "for path in paths[0:10]:\n",
    "    images = []\n",
    "    for channel in channels:\n",
    "        print(channel)\n",
    "        images.append(np.array(Image.open(path + '_' + channel + '.png')))\n",
    "\n",
    "    if len(channels) >= 2:\n",
    "        im = np.stack((\n",
    "            images\n",
    "        ), -1)\n",
    "        im = cv2.resize(im, (SIZE,SIZE))\n",
    "        im = np.divide(im, 255)\n",
    "        \n",
    "    else:\n",
    "        im = images[0]\n",
    "        im = cv2.resize(im, (SIZE,SIZE))\n",
    "        im = np.divide(im, 255)\n",
    "        im = np.expand_dims(im, 2)\n",
    "    print(im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class data_generator:\n",
    "    \n",
    "#     def create_train(dataset_info, batch_size, shape, augument=True):\n",
    "#         assert shape[2] == 3\n",
    "#         while True:\n",
    "#             dataset_info = shuffle(dataset_info)\n",
    "#             for start in range(0, len(dataset_info), batch_size):\n",
    "#                 end = min(start + batch_size, len(dataset_info))\n",
    "#                 batch_images = []\n",
    "#                 X_train_batch = dataset_info[start:end]\n",
    "#                 batch_labels = np.zeros((len(X_train_batch), 28))\n",
    "#                 for i in range(len(X_train_batch)):\n",
    "#                     image = data_generator.load_image(\n",
    "#                         X_train_batch[i]['path'], shape)   \n",
    "#                     if augument:\n",
    "#                         image = data_generator.augment(image)\n",
    "#                     batch_images.append(image/255.)\n",
    "#                     batch_labels[i][X_train_batch[i]['labels']] = 1\n",
    "#                 yield np.array(batch_images, np.float32), batch_labels\n",
    "\n",
    "#     def load_image(path, shape):\n",
    "#         image_red_ch = Image.open(path+'_red.png')\n",
    "#         image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "#         image_green_ch = Image.open(path+'_green.png')\n",
    "#         image_blue_ch = Image.open(path+'_blue.png')\n",
    "#         image = np.stack((\n",
    "#         np.array(image_red_ch), \n",
    "#         np.array(image_green_ch), \n",
    "#         np.array(image_blue_ch)), -1)\n",
    "#         image = cv2.resize(image, (shape[0], shape[1]))\n",
    "#         return image\n",
    "\n",
    "#     def augment(image):\n",
    "#         augment_img = iaa.Sequential([\n",
    "#             iaa.OneOf([\n",
    "#                 iaa.Affine(rotate=0),\n",
    "#                 iaa.Affine(rotate=90),\n",
    "#                 iaa.Affine(rotate=180),\n",
    "#                 iaa.Affine(rotate=270),\n",
    "#                 iaa.Fliplr(0.5),\n",
    "#                 iaa.Flipud(0.5),\n",
    "#             ])], random_order=True)\n",
    "\n",
    "#         image_aug = augment_img.augment_image(image)\n",
    "#         return image_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D, MaxPooling2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    #y_pred = K.round(y_pred)\n",
    "    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    #y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1-K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# f = h5py.File(\"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\", 'r')\n",
    "# dset = f[\"conv2d_1\"]\n",
    "# list(dset.keys())\n",
    "# group = dset[\"conv2d_1\"]\n",
    "# list(group.keys())\n",
    "# first_lay = group['kernel:0'].value\n",
    "# print(first_lay.shape)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros = np.zeros(shape = (3,3,1,32))\n",
    "# print(zeros.shape)\n",
    "# first_lay1 = np.concatenate([zeros, first_lay], axis = 2)\n",
    "# first_lay.shape\n",
    "# f = h5py.File(\"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\", 'r+')\n",
    "# dset = f[\"conv2d_1\"]\n",
    "# list(dset.keys())\n",
    "# group = dset[\"conv2d_1\"]\n",
    "# list(group.keys())\n",
    "# del group['kernel:0']\n",
    "# group['kernel:0'] = first_lay1\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 299, 299, 4)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 149, 149, 32) 1152        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 21,803,072\n",
      "Trainable params: 21,768,640\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Inception V3 model for Keras.\n",
    "Note that the input image format for this model is different than for\n",
    "the VGG16 and ResNet models (299x299 instead of 224x224),\n",
    "and that the input preprocessing function is also different (same as Xception).\n",
    "# Reference\n",
    "- [Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567)\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "\n",
    "def conv2d_bn(x,\n",
    "              filters,\n",
    "              num_row,\n",
    "              num_col,\n",
    "              padding='same',\n",
    "              strides=(1, 1),\n",
    "              name=None):\n",
    "    \"\"\"Utility function to apply conv + BN.\n",
    "    Arguments:\n",
    "        x: input tensor.\n",
    "        filters: filters in `Conv2D`.\n",
    "        num_row: height of the convolution kernel.\n",
    "        num_col: width of the convolution kernel.\n",
    "        padding: padding mode in `Conv2D`.\n",
    "        strides: strides in `Conv2D`.\n",
    "        name: name of the ops; will become `name + '_conv'`\n",
    "            for the convolution and `name + '_bn'` for the\n",
    "            batch norm layer.\n",
    "    Returns:\n",
    "        Output tensor after applying `Conv2D` and `BatchNormalization`.\n",
    "    \"\"\"\n",
    "    if name is not None:\n",
    "        bn_name = name + '_bn'\n",
    "        conv_name = name + '_conv'\n",
    "    else:\n",
    "        bn_name = None\n",
    "        conv_name = None\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        bn_axis = 1\n",
    "    else:\n",
    "        bn_axis = 3\n",
    "    x = Conv2D(\n",
    "        filters, (num_row, num_col),\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        use_bias=False,\n",
    "        name=conv_name)(x)\n",
    "    x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n",
    "    x = Activation('relu', name=name)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def InceptionV3(include_top=True,\n",
    "                weights='imagenet',\n",
    "                input_tensor=None,\n",
    "                input_shape=None,\n",
    "                pooling=None,\n",
    "                classes=1000):\n",
    "    \"\"\"Instantiates the Inception v3 architecture.\n",
    "    Optionally loads weights pre-trained\n",
    "    on ImageNet. Note that when using TensorFlow,\n",
    "    for best performance you should set\n",
    "    `image_data_format=\"channels_last\"` in your Keras config\n",
    "    at ~/.keras/keras.json.\n",
    "    The model and the weights are compatible with both\n",
    "    TensorFlow and Theano. The data format\n",
    "    convention used by the model is the one\n",
    "    specified in your Keras config file.\n",
    "    Note that the default input image size for this model is 299x299.\n",
    "    Arguments:\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization)\n",
    "            or \"imagenet\" (pre-training on ImageNet).\n",
    "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(299, 299, 3)` (with `channels_last` data format)\n",
    "            or `(3, 299, 299)` (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels,\n",
    "            and width and height should be no smaller than 139.\n",
    "            E.g. `(150, 150, 3)` would be one valid value.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    Returns:\n",
    "        A Keras model instance.\n",
    "    Raises:\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    if weights not in {'imagenet', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `imagenet` '\n",
    "                         '(pre-training on ImageNet).')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = (299,299,4)\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = 3\n",
    "\n",
    "    x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding='valid')\n",
    "    x = conv2d_bn(x, 32, 3, 3, padding='valid')\n",
    "    x = conv2d_bn(x, 64, 3, 3)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv2d_bn(x, 80, 1, 1, padding='valid')\n",
    "    x = conv2d_bn(x, 192, 3, 3, padding='valid')\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    # mixed 0, 1, 2: 35 x 35 x 256\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 32, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed0')\n",
    "\n",
    "    # mixed 1: 35 x 35 x 256\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed1')\n",
    "\n",
    "    # mixed 2: 35 x 35 x 256\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed2')\n",
    "\n",
    "    # mixed 3: 17 x 17 x 768\n",
    "    branch3x3 = conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(\n",
    "        branch3x3dbl, 96, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = layers.concatenate(\n",
    "        [branch3x3, branch3x3dbl, branch_pool], axis=channel_axis, name='mixed3')\n",
    "\n",
    "    # mixed 4: 17 x 17 x 768\n",
    "    branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch7x7 = conv2d_bn(x, 128, 1, 1)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 128, 1, 7)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "    branch7x7dbl = conv2d_bn(x, 128, 1, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 1, 7)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed4')\n",
    "\n",
    "    # mixed 5, 6: 17 x 17 x 768\n",
    "    for i in range(2):\n",
    "        branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "        branch7x7 = conv2d_bn(x, 160, 1, 1)\n",
    "        branch7x7 = conv2d_bn(branch7x7, 160, 1, 7)\n",
    "        branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "        branch7x7dbl = conv2d_bn(x, 160, 1, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 1, 7)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n",
    "        branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "        branch_pool = AveragePooling2D(\n",
    "            (3, 3), strides=(1, 1), padding='same')(x)\n",
    "        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "        x = layers.concatenate(\n",
    "            [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "            axis=channel_axis,\n",
    "            name='mixed' + str(5 + i))\n",
    "\n",
    "    # mixed 7: 17 x 17 x 768\n",
    "    branch1x1 = conv2d_bn(x, 192, 1, 1)\n",
    "\n",
    "    branch7x7 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 1, 7)\n",
    "    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n",
    "\n",
    "    branch7x7dbl = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n",
    "    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed7')\n",
    "\n",
    "    # mixed 8: 8 x 8 x 1280\n",
    "    branch3x3 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch3x3 = conv2d_bn(branch3x3, 320, 3, 3,\n",
    "                          strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch7x7x3 = conv2d_bn(x, 192, 1, 1)\n",
    "    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)\n",
    "    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)\n",
    "    branch7x7x3 = conv2d_bn(\n",
    "        branch7x7x3, 192, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    x = layers.concatenate(\n",
    "        [branch3x3, branch7x7x3, branch_pool], axis=channel_axis, name='mixed8')\n",
    "\n",
    "    # mixed 9: 8 x 8 x 2048\n",
    "    for i in range(2):\n",
    "        branch1x1 = conv2d_bn(x, 320, 1, 1)\n",
    "\n",
    "        branch3x3 = conv2d_bn(x, 384, 1, 1)\n",
    "        branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)\n",
    "        branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)\n",
    "        branch3x3 = layers.concatenate(\n",
    "            [branch3x3_1, branch3x3_2], axis=channel_axis, name='mixed9_' + str(i))\n",
    "\n",
    "        branch3x3dbl = conv2d_bn(x, 448, 1, 1)\n",
    "        branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)\n",
    "        branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)\n",
    "        branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)\n",
    "        branch3x3dbl = layers.concatenate(\n",
    "            [branch3x3dbl_1, branch3x3dbl_2], axis=channel_axis)\n",
    "\n",
    "        branch_pool = AveragePooling2D(\n",
    "            (3, 3), strides=(1, 1), padding='same')(x)\n",
    "        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n",
    "        x = layers.concatenate(\n",
    "            [branch1x1, branch3x3, branch3x3dbl, branch_pool],\n",
    "            axis=channel_axis,\n",
    "            name='mixed' + str(9 + i))\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        x = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='inception_v3')\n",
    "\n",
    "    # load weights\n",
    "    if weights == 'imagenet':\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            if K.backend() == 'tensorflow':\n",
    "                warnings.warn('You are using the TensorFlow backend, yet you '\n",
    "                              'are using the Theano '\n",
    "                              'image data format convention '\n",
    "                              '(`image_data_format=\"channels_first\"`). '\n",
    "                              'For best performance, set '\n",
    "                              '`image_data_format=\"channels_last\"` in '\n",
    "                              'your Keras config '\n",
    "                              'at ~/.keras/keras.json.')\n",
    "        if include_top:\n",
    "            weights_path = get_file(\n",
    "                'inception_v3_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                WEIGHTS_PATH,\n",
    "                cache_subdir='models',\n",
    "                md5_hash='9a0d58056eeedaa3f26cb7ebd46da564')\n",
    "        else:\n",
    "            weights_path = get_file(\n",
    "                'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                WEIGHTS_PATH_NO_TOP,\n",
    "                cache_subdir='models',\n",
    "                md5_hash='bcbd6486424b2319ff4ef7d526e38f63')\n",
    "            weights_path = \"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "        model.load_weights(weights_path)\n",
    "        if K.backend() == 'theano':\n",
    "            convert_all_kernels_in_model(model)\n",
    "    return model\n",
    "if __name__ == '__main__':\n",
    "    model = InceptionV3(include_top=False, weights='imagenet')\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out, channels):\n",
    "    input_tensor = Input(shape=(299,299,len(channels)))\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    base_model = InceptionV3(include_top=False, weights='imagenet')\n",
    "    x = base_model(bn)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Conv2D(128, kernel_size=(3,3), activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 299, 299, 4)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 299, 299, 4)       16        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         (None, 8, 8, 2048)        21803072  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 2048)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_189 (Conv2D)          (None, 6, 6, 128)         2359424   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 28)                28700     \n",
      "=================================================================\n",
      "Total params: 28,910,828\n",
      "Trainable params: 28,876,388\n",
      "Non-trainable params: 34,440\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "channels = [\"green\", \"blue\", \"red\", \"yellow\"]\n",
    "# with tf.device('/cpu:0'):\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,4), \n",
    "    n_out=28, channels = channels)\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=Adam(1e-04),\n",
    "    metrics=['acc', f1])\n",
    "model.summary()\n",
    "\n",
    "# model.layers[3].layers[1] = Conv2D(32, kernel_size = (7,7), strides = (2,2), padding = \"same\", input_shape = (299, 299, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31072,) (31072, 28)\n",
      "(27964,) (27964, 28) (3108,) (3108, 28)\n"
     ]
    }
   ],
   "source": [
    "# create callbacks list\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "epochs = 10; batch_size = 32;VAL_RATIO = .1;DEBUG = False\n",
    "# split data into train, valid\n",
    "paths, labels = getTrainDataset()\n",
    "\n",
    "# divide to \n",
    "keys = np.arange(paths.shape[0], dtype=np.int)  \n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(keys)\n",
    "lastTrainIndex = int((1-VAL_RATIO) * paths.shape[0])\n",
    "\n",
    "if DEBUG == True:  # use only small subset for debugging, Kaggle's RAM is limited\n",
    "    pathsTrain = paths[0:256]\n",
    "    labelsTrain = labels[0:256]\n",
    "    pathsVal = paths[lastTrainIndex:lastTrainIndex+256]\n",
    "    labelsVal = labels[lastTrainIndex:lastTrainIndex+256]\n",
    "    use_cache = True\n",
    "else:\n",
    "    pathsTrain = paths[0:lastTrainIndex]\n",
    "    labelsTrain = labels[0:lastTrainIndex]\n",
    "    pathsVal = paths[lastTrainIndex:]\n",
    "    labelsVal = labels[lastTrainIndex:]\n",
    "    use_cache = False\n",
    "\n",
    "print(paths.shape, labels.shape)\n",
    "print(pathsTrain.shape, labelsTrain.shape, pathsVal.shape, labelsVal.shape)\n",
    "use_cache = True\n",
    "tg = ProteinDataGenerator(pathsTrain, labelsTrain, batch_size, SHAPE, channels, use_cache=use_cache)\n",
    "vg = ProteinDataGenerator(pathsVal, labelsVal, batch_size, SHAPE, channels, use_cache=use_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and valid datagens\n",
    "# train_generator = data_generator.create_train(\n",
    "#     train_dataset_info[train_indexes], batch_size, (SIZE,SIZE,3), augument=True)\n",
    "# validation_generator = data_generator.create_train(\n",
    "#     train_dataset_info[valid_indexes], 32, (SIZE,SIZE,3), augument=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('../working/InceptionV3+branch.h5', monitor='val_f1', verbose=1, \n",
    "                             save_best_only=True, mode='max', save_weights_only = False)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_f1', factor=0.5, patience=10, \n",
    "                                   verbose=1, mode='max', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_f1\", \n",
    "                      mode=\"max\", \n",
    "                      patience=20)\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 299, 299, 4)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_285 (Bat (None, 299, 299, 4)       16        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         (None, 8, 8, 2048)        21803072  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8, 8, 2048)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_379 (Conv2D)          (None, 6, 6, 128)         2359424   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 28)                28700     \n",
      "=================================================================\n",
      "Total params: 28,910,828\n",
      "Trainable params: 7,107,748\n",
      "Non-trainable params: 21,803,080\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# warm up model\n",
    "import tensorflow as tf\n",
    "channels = [\"green\", \"blue\", \"red\", \"yellow\"]\n",
    "# with tf.device('/cpu:0'):\n",
    "model = create_model(\n",
    "    input_shape=(SIZE,SIZE,4), \n",
    "    n_out=28, channels = channels)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "model.layers[0].trainable = True\n",
    "model.layers[1].trainable = True\n",
    "model.layers[-1].trainable = True\n",
    "model.layers[-2].trainable = True\n",
    "model.layers[-3].trainable = True\n",
    "model.layers[-4].trainable = True\n",
    "model.layers[-5].trainable = True\n",
    "model.layers[-6].trainable = True\n",
    "model.layers[-7].trainable = True\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-854d7f30fcde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProteinDataGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpathsTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelsTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSHAPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProteinDataGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpathsVal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelsVal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSHAPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-6fac07e64128>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, paths, labels, batch_size, shape, channels, shuffle, use_cache, augmentor)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclahe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateCLAHE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muse_cache\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_cached\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "tg = ProteinDataGenerator(pathsTrain, labelsTrain, batch_size, SHAPE, channels, use_cache=use_cache)\n",
    "vg = ProteinDataGenerator(pathsVal, labelsVal, batch_size, SHAPE, channels, use_cache=use_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "437/437 [==============================] - 739s 2s/step - loss: 0.2138 - acc: 0.9321 - f1: 0.0587 - val_loss: 0.1960 - val_acc: 0.9400 - val_f1: 0.0583\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.05826, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 2/20\n",
      "437/437 [==============================] - 709s 2s/step - loss: 0.1882 - acc: 0.9387 - f1: 0.0532 - val_loss: 0.2411 - val_acc: 0.9401 - val_f1: 0.0892\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.05826 to 0.08916, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 3/20\n",
      "437/437 [==============================] - 152s 347ms/step - loss: 0.1839 - acc: 0.9399 - f1: 0.0528 - val_loss: 0.2327 - val_acc: 0.9399 - val_f1: 0.0874\n",
      "\n",
      "Epoch 00003: val_f1 did not improve from 0.08916\n",
      "Epoch 4/20\n",
      "437/437 [==============================] - 207s 473ms/step - loss: 0.1827 - acc: 0.9398 - f1: 0.0518 - val_loss: 0.2059 - val_acc: 0.9385 - val_f1: 0.0745\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.08916\n",
      "Epoch 5/20\n",
      "437/437 [==============================] - 139s 318ms/step - loss: 0.1799 - acc: 0.9401 - f1: 0.0533 - val_loss: 0.2071 - val_acc: 0.9397 - val_f1: 0.0731\n",
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.08916\n",
      "Epoch 6/20\n",
      "437/437 [==============================] - 218s 498ms/step - loss: 0.1792 - acc: 0.9412 - f1: 0.0515 - val_loss: 0.2157 - val_acc: 0.9398 - val_f1: 0.0772\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.08916\n",
      "Epoch 7/20\n",
      "437/437 [==============================] - 142s 324ms/step - loss: 0.1769 - acc: 0.9412 - f1: 0.0514 - val_loss: 0.1900 - val_acc: 0.9367 - val_f1: 0.0635\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.08916\n",
      "Epoch 8/20\n",
      "437/437 [==============================] - 208s 476ms/step - loss: 0.1773 - acc: 0.9407 - f1: 0.0529 - val_loss: 0.2050 - val_acc: 0.9374 - val_f1: 0.0740\n",
      "\n",
      "Epoch 00008: val_f1 did not improve from 0.08916\n",
      "Epoch 9/20\n",
      "437/437 [==============================] - 138s 317ms/step - loss: 0.1751 - acc: 0.9415 - f1: 0.0548 - val_loss: 0.2119 - val_acc: 0.9383 - val_f1: 0.0774\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.08916\n",
      "Epoch 10/20\n",
      "437/437 [==============================] - 174s 399ms/step - loss: 0.1750 - acc: 0.9410 - f1: 0.0544 - val_loss: 0.2076 - val_acc: 0.9326 - val_f1: 0.0795\n",
      "\n",
      "Epoch 00010: val_f1 did not improve from 0.08916\n",
      "Epoch 11/20\n",
      "437/437 [==============================] - 145s 332ms/step - loss: 0.1731 - acc: 0.9414 - f1: 0.0587 - val_loss: 0.2286 - val_acc: 0.9385 - val_f1: 0.0887\n",
      "\n",
      "Epoch 00011: val_f1 did not improve from 0.08916\n",
      "Epoch 12/20\n",
      "437/437 [==============================] - 199s 454ms/step - loss: 0.1737 - acc: 0.9418 - f1: 0.0564 - val_loss: 0.2089 - val_acc: 0.9333 - val_f1: 0.0788\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.08916\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 13/20\n",
      "437/437 [==============================] - 147s 336ms/step - loss: 0.1709 - acc: 0.9419 - f1: 0.0591 - val_loss: 0.2329 - val_acc: 0.9339 - val_f1: 0.0904\n",
      "\n",
      "Epoch 00013: val_f1 improved from 0.08916 to 0.09042, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 14/20\n",
      "437/437 [==============================] - 204s 466ms/step - loss: 0.1702 - acc: 0.9421 - f1: 0.0602 - val_loss: 0.2138 - val_acc: 0.9289 - val_f1: 0.0806\n",
      "\n",
      "Epoch 00014: val_f1 did not improve from 0.09042\n",
      "Epoch 15/20\n",
      "437/437 [==============================] - 149s 342ms/step - loss: 0.1693 - acc: 0.9423 - f1: 0.0624 - val_loss: 0.2079 - val_acc: 0.9289 - val_f1: 0.0781\n",
      "\n",
      "Epoch 00015: val_f1 did not improve from 0.09042\n",
      "Epoch 16/20\n",
      "437/437 [==============================] - 188s 430ms/step - loss: 0.1692 - acc: 0.9421 - f1: 0.0635 - val_loss: 0.2156 - val_acc: 0.9296 - val_f1: 0.0813\n",
      "\n",
      "Epoch 00016: val_f1 did not improve from 0.09042\n",
      "Epoch 17/20\n",
      "437/437 [==============================] - 142s 325ms/step - loss: 0.1675 - acc: 0.9425 - f1: 0.0672 - val_loss: 0.2188 - val_acc: 0.9309 - val_f1: 0.0831\n",
      "\n",
      "Epoch 00017: val_f1 did not improve from 0.09042\n",
      "Epoch 18/20\n",
      "437/437 [==============================] - 210s 480ms/step - loss: 0.1683 - acc: 0.9422 - f1: 0.0667 - val_loss: 0.2137 - val_acc: 0.9303 - val_f1: 0.0843\n",
      "\n",
      "Epoch 00018: val_f1 did not improve from 0.09042\n",
      "Epoch 19/20\n",
      "437/437 [==============================] - 141s 323ms/step - loss: 0.1666 - acc: 0.9426 - f1: 0.0694 - val_loss: 0.2094 - val_acc: 0.9278 - val_f1: 0.0774\n",
      "\n",
      "Epoch 00019: val_f1 did not improve from 0.09042\n",
      "Epoch 20/20\n",
      "437/437 [==============================] - 238s 545ms/step - loss: 0.1672 - acc: 0.9425 - f1: 0.0691 - val_loss: 0.2099 - val_acc: 0.9290 - val_f1: 0.0761\n",
      "\n",
      "Epoch 00020: val_f1 did not improve from 0.09042\n"
     ]
    }
   ],
   "source": [
    "hist =  model.fit_generator(\n",
    "        tg,\n",
    "        steps_per_epoch=np.ceil(float(len(pathsTrain)) / float(batch_size))/2,\n",
    "        validation_data=vg,\n",
    "        validation_steps=np.ceil(float(len(pathsVal)) / float(batch_size))/2,\n",
    "        epochs=20, \n",
    "        verbose=1,\n",
    "        callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x000001F94DF24CF8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001F94DF24588>\n",
      "<keras.engine.training.Model object at 0x000001E6B9E45F28>\n",
      "<keras.layers.core.Dropout object at 0x000001E6B9E45EF0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001E6CCD87080>\n",
      "<keras.layers.core.Flatten object at 0x000001E6CCD80A58>\n",
      "<keras.layers.core.Dropout object at 0x000001E6E55B8898>\n",
      "<keras.layers.core.Dense object at 0x000001E6E56A5E80>\n",
      "<keras.layers.core.Dropout object at 0x000001E6E56BE470>\n",
      "<keras.layers.core.Dense object at 0x000001E6E56BE518>\n",
      "Epoch 1/100\n",
      "1166/1165 [==============================] - 528s 452ms/step - loss: 0.1651 - acc: 0.9428 - f1: 0.0750 - val_loss: 0.2087 - val_acc: 0.9234 - val_f1: 0.0739\n",
      "\n",
      "Epoch 00001: val_f1 did not improve from 0.09042\n",
      "Epoch 2/100\n",
      "1166/1165 [==============================] - 450s 386ms/step - loss: 0.1631 - acc: 0.9434 - f1: 0.0808 - val_loss: 0.2173 - val_acc: 0.9320 - val_f1: 0.0794\n",
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.09042\n",
      "Epoch 3/100\n",
      "1166/1165 [==============================] - 409s 351ms/step - loss: 0.1618 - acc: 0.9436 - f1: 0.0869 - val_loss: 0.2148 - val_acc: 0.9297 - val_f1: 0.0788\n",
      "\n",
      "Epoch 00003: val_f1 did not improve from 0.09042\n",
      "Epoch 4/100\n",
      "1166/1165 [==============================] - 462s 396ms/step - loss: 0.1593 - acc: 0.9440 - f1: 0.0950 - val_loss: 0.2137 - val_acc: 0.9281 - val_f1: 0.0785\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.09042\n",
      "Epoch 5/100\n",
      "1166/1165 [==============================] - 435s 373ms/step - loss: 0.1573 - acc: 0.9446 - f1: 0.1044 - val_loss: 0.2141 - val_acc: 0.9267 - val_f1: 0.0772\n",
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.09042\n",
      "Epoch 6/100\n",
      "1166/1165 [==============================] - 424s 363ms/step - loss: 0.1551 - acc: 0.9451 - f1: 0.1138 - val_loss: 0.2186 - val_acc: 0.9281 - val_f1: 0.0779\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.09042\n",
      "Epoch 7/100\n",
      "1166/1165 [==============================] - 414s 355ms/step - loss: 0.1522 - acc: 0.9456 - f1: 0.1242 - val_loss: 0.2229 - val_acc: 0.9240 - val_f1: 0.0788\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.09042\n",
      "Epoch 8/100\n",
      "1166/1165 [==============================] - 443s 380ms/step - loss: 0.1496 - acc: 0.9464 - f1: 0.1337 - val_loss: 0.2250 - val_acc: 0.9235 - val_f1: 0.0760\n",
      "\n",
      "Epoch 00008: val_f1 did not improve from 0.09042\n",
      "Epoch 9/100\n",
      "1166/1165 [==============================] - 459s 393ms/step - loss: 0.1473 - acc: 0.9469 - f1: 0.1440 - val_loss: 0.2233 - val_acc: 0.9254 - val_f1: 0.0755\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.09042\n",
      "Epoch 10/100\n",
      "1166/1165 [==============================] - 459s 393ms/step - loss: 0.1435 - acc: 0.9478 - f1: 0.1576 - val_loss: 0.2225 - val_acc: 0.9253 - val_f1: 0.0769\n",
      "\n",
      "Epoch 00010: val_f1 did not improve from 0.09042\n",
      "Epoch 11/100\n",
      "1166/1165 [==============================] - 428s 367ms/step - loss: 0.1415 - acc: 0.9484 - f1: 0.1652 - val_loss: 0.2253 - val_acc: 0.9256 - val_f1: 0.0805\n",
      "\n",
      "Epoch 00011: val_f1 did not improve from 0.09042\n",
      "Epoch 12/100\n",
      "1166/1165 [==============================] - 455s 390ms/step - loss: 0.1389 - acc: 0.9491 - f1: 0.1761 - val_loss: 0.2277 - val_acc: 0.9253 - val_f1: 0.0801\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.09042\n",
      "Epoch 13/100\n",
      "1166/1165 [==============================] - 414s 355ms/step - loss: 0.1350 - acc: 0.9502 - f1: 0.1892 - val_loss: 0.2325 - val_acc: 0.9227 - val_f1: 0.0775\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.09042\n",
      "Epoch 14/100\n",
      "1166/1165 [==============================] - 454s 389ms/step - loss: 0.1333 - acc: 0.9506 - f1: 0.1974 - val_loss: 0.2430 - val_acc: 0.9221 - val_f1: 0.0741\n",
      "\n",
      "Epoch 00014: val_f1 did not improve from 0.09042\n",
      "Epoch 15/100\n",
      "1166/1165 [==============================] - 514s 441ms/step - loss: 0.1303 - acc: 0.9515 - f1: 0.2058 - val_loss: 0.2478 - val_acc: 0.9180 - val_f1: 0.0760\n",
      "\n",
      "Epoch 00015: val_f1 did not improve from 0.09042\n",
      "Epoch 16/100\n",
      "1166/1165 [==============================] - 434s 372ms/step - loss: 0.1269 - acc: 0.9523 - f1: 0.2214 - val_loss: 0.2620 - val_acc: 0.9184 - val_f1: 0.0756\n",
      "\n",
      "Epoch 00016: val_f1 did not improve from 0.09042\n",
      "Epoch 17/100\n",
      "1166/1165 [==============================] - 421s 361ms/step - loss: 0.1239 - acc: 0.9536 - f1: 0.2272 - val_loss: 0.2465 - val_acc: 0.9215 - val_f1: 0.0751\n",
      "\n",
      "Epoch 00017: val_f1 did not improve from 0.09042\n",
      "Epoch 18/100\n",
      "1166/1165 [==============================] - 450s 386ms/step - loss: 0.1220 - acc: 0.9542 - f1: 0.2326 - val_loss: 0.2419 - val_acc: 0.9234 - val_f1: 0.0767\n",
      "\n",
      "Epoch 00018: val_f1 did not improve from 0.09042\n",
      "Epoch 19/100\n",
      "1166/1165 [==============================] - 457s 392ms/step - loss: 0.1183 - acc: 0.9551 - f1: 0.2474 - val_loss: 0.2668 - val_acc: 0.9195 - val_f1: 0.0681\n",
      "\n",
      "Epoch 00019: val_f1 did not improve from 0.09042\n",
      "Epoch 20/100\n",
      "1166/1165 [==============================] - 426s 365ms/step - loss: 0.1163 - acc: 0.9558 - f1: 0.2513 - val_loss: 0.2606 - val_acc: 0.9225 - val_f1: 0.0749\n",
      "\n",
      "Epoch 00020: val_f1 did not improve from 0.09042\n",
      "Epoch 21/100\n",
      "1159/1165 [============================>.] - ETA: 2s - loss: 0.1138 - acc: 0.9566 - f1: 0.2598"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-df7ee5993739>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1426\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1220\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1221\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\magic\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    layer.trainable = False\n",
    "model.layers[2].layers[1].trainable = True\n",
    "batch_size = 12\n",
    "hist =  model.fit_generator(\n",
    "        tg,\n",
    "        steps_per_epoch=np.ceil(float(len(pathsTrain)) / float(batch_size))/2,\n",
    "        validation_data=vg,\n",
    "        validation_steps=np.ceil(float(len(pathsVal)) / float(batch_size))/2,\n",
    "        epochs=100, \n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x000001F94DF24CF8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x000001F94DF24588>\n",
      "<keras.engine.training.Model object at 0x000001E6B9E45F28>\n",
      "<keras.layers.core.Dropout object at 0x000001E6B9E45EF0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x000001E6CCD87080>\n",
      "<keras.layers.core.Flatten object at 0x000001E6CCD80A58>\n",
      "<keras.layers.core.Dropout object at 0x000001E6E55B8898>\n",
      "<keras.layers.core.Dense object at 0x000001E6E56A5E80>\n",
      "<keras.layers.core.Dropout object at 0x000001E6E56BE470>\n",
      "<keras.layers.core.Dense object at 0x000001E6E56BE518>\n",
      "Epoch 1/100\n",
      "1166/1165 [==============================] - 577s 495ms/step - loss: 0.1749 - acc: 0.9413 - f1: 0.0726 - val_loss: 0.1802 - val_acc: 0.9408 - val_f1: 0.0840\n",
      "\n",
      "Epoch 00001: val_f1 did not improve from 0.09042\n",
      "Epoch 2/100\n",
      "1166/1165 [==============================] - 551s 473ms/step - loss: 0.1668 - acc: 0.9432 - f1: 0.0729 - val_loss: 0.1647 - val_acc: 0.9448 - val_f1: 0.1035\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.09042 to 0.10347, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 3/100\n",
      "1166/1165 [==============================] - 568s 487ms/step - loss: 0.1504 - acc: 0.9485 - f1: 0.1277 - val_loss: 0.1490 - val_acc: 0.9501 - val_f1: 0.1639\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.10347 to 0.16387, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 4/100\n",
      "1166/1165 [==============================] - 578s 495ms/step - loss: 0.1308 - acc: 0.9543 - f1: 0.1984 - val_loss: 0.1434 - val_acc: 0.9507 - val_f1: 0.1986\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.16387 to 0.19855, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 5/100\n",
      "1166/1165 [==============================] - 540s 463ms/step - loss: 0.1129 - acc: 0.9600 - f1: 0.2559 - val_loss: 0.1371 - val_acc: 0.9531 - val_f1: 0.2247\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.19855 to 0.22465, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 6/100\n",
      "1166/1165 [==============================] - 572s 491ms/step - loss: 0.0930 - acc: 0.9669 - f1: 0.3140 - val_loss: 0.1486 - val_acc: 0.9503 - val_f1: 0.2017\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.22465\n",
      "Epoch 7/100\n",
      "1166/1165 [==============================] - 538s 461ms/step - loss: 0.0671 - acc: 0.9759 - f1: 0.3807 - val_loss: 0.1615 - val_acc: 0.9494 - val_f1: 0.2192\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.22465\n",
      "Epoch 8/100\n",
      "1166/1165 [==============================] - 515s 442ms/step - loss: 0.0521 - acc: 0.9812 - f1: 0.4241 - val_loss: 0.1613 - val_acc: 0.9500 - val_f1: 0.2382\n",
      "\n",
      "Epoch 00008: val_f1 improved from 0.22465 to 0.23821, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 9/100\n",
      "1166/1165 [==============================] - 580s 497ms/step - loss: 0.0418 - acc: 0.9851 - f1: 0.4528 - val_loss: 0.1724 - val_acc: 0.9486 - val_f1: 0.2246\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.23821\n",
      "Epoch 10/100\n",
      "1166/1165 [==============================] - 547s 469ms/step - loss: 0.0319 - acc: 0.9888 - f1: 0.4807 - val_loss: 0.1958 - val_acc: 0.9497 - val_f1: 0.2385\n",
      "\n",
      "Epoch 00010: val_f1 improved from 0.23821 to 0.23848, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 11/100\n",
      "1166/1165 [==============================] - 540s 463ms/step - loss: 0.0279 - acc: 0.9903 - f1: 0.4892 - val_loss: 0.1869 - val_acc: 0.9515 - val_f1: 0.2427\n",
      "\n",
      "Epoch 00011: val_f1 improved from 0.23848 to 0.24270, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 12/100\n",
      "1166/1165 [==============================] - 559s 479ms/step - loss: 0.0249 - acc: 0.9914 - f1: 0.4997 - val_loss: 0.1906 - val_acc: 0.9488 - val_f1: 0.2316\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.24270\n",
      "Epoch 13/100\n",
      "1166/1165 [==============================] - 542s 465ms/step - loss: 0.0215 - acc: 0.9926 - f1: 0.5082 - val_loss: 0.2196 - val_acc: 0.9498 - val_f1: 0.2279\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.24270\n",
      "Epoch 14/100\n",
      "1166/1165 [==============================] - 529s 453ms/step - loss: 0.0203 - acc: 0.9931 - f1: 0.5145 - val_loss: 0.2197 - val_acc: 0.9485 - val_f1: 0.2258\n",
      "\n",
      "Epoch 00014: val_f1 did not improve from 0.24270\n",
      "Epoch 15/100\n",
      "1166/1165 [==============================] - 566s 485ms/step - loss: 0.0183 - acc: 0.9937 - f1: 0.5135 - val_loss: 0.2085 - val_acc: 0.9500 - val_f1: 0.2380\n",
      "\n",
      "Epoch 00015: val_f1 did not improve from 0.24270\n",
      "Epoch 16/100\n",
      "1166/1165 [==============================] - 537s 460ms/step - loss: 0.0170 - acc: 0.9942 - f1: 0.5211 - val_loss: 0.2085 - val_acc: 0.9512 - val_f1: 0.2347\n",
      "\n",
      "Epoch 00016: val_f1 did not improve from 0.24270\n",
      "Epoch 17/100\n",
      "1166/1165 [==============================] - 507s 435ms/step - loss: 0.0156 - acc: 0.9947 - f1: 0.5209 - val_loss: 0.2348 - val_acc: 0.9509 - val_f1: 0.2320\n",
      "\n",
      "Epoch 00017: val_f1 did not improve from 0.24270\n",
      "Epoch 18/100\n",
      "1166/1165 [==============================] - 888s 762ms/step - loss: 0.0151 - acc: 0.9948 - f1: 0.5273 - val_loss: 0.2155 - val_acc: 0.9523 - val_f1: 0.2375\n",
      "\n",
      "Epoch 00018: val_f1 did not improve from 0.24270\n",
      "Epoch 19/100\n",
      "1166/1165 [==============================] - 550s 472ms/step - loss: 0.0134 - acc: 0.9955 - f1: 0.5308 - val_loss: 0.2417 - val_acc: 0.9513 - val_f1: 0.2287\n",
      "\n",
      "Epoch 00019: val_f1 did not improve from 0.24270\n",
      "Epoch 20/100\n",
      "1166/1165 [==============================] - 532s 456ms/step - loss: 0.0130 - acc: 0.9957 - f1: 0.5280 - val_loss: 0.2433 - val_acc: 0.9517 - val_f1: 0.2252\n",
      "\n",
      "Epoch 00020: val_f1 did not improve from 0.24270\n",
      "Epoch 21/100\n",
      "1166/1165 [==============================] - 544s 467ms/step - loss: 0.0127 - acc: 0.9957 - f1: 0.5322 - val_loss: 0.2156 - val_acc: 0.9516 - val_f1: 0.2400\n",
      "\n",
      "Epoch 00021: val_f1 did not improve from 0.24270\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 22/100\n",
      "1166/1165 [==============================] - 529s 454ms/step - loss: 0.0061 - acc: 0.9980 - f1: 0.5460 - val_loss: 0.2527 - val_acc: 0.9551 - val_f1: 0.2551\n",
      "\n",
      "Epoch 00022: val_f1 improved from 0.24270 to 0.25512, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 23/100\n",
      "1166/1165 [==============================] - 554s 475ms/step - loss: 0.0037 - acc: 0.9988 - f1: 0.5505 - val_loss: 0.2568 - val_acc: 0.9543 - val_f1: 0.2465\n",
      "\n",
      "Epoch 00023: val_f1 did not improve from 0.25512\n",
      "Epoch 24/100\n",
      "1166/1165 [==============================] - 551s 472ms/step - loss: 0.0039 - acc: 0.9988 - f1: 0.5531 - val_loss: 0.2658 - val_acc: 0.9548 - val_f1: 0.2453\n",
      "\n",
      "Epoch 00024: val_f1 did not improve from 0.25512\n",
      "Epoch 25/100\n",
      "1166/1165 [==============================] - 552s 473ms/step - loss: 0.0038 - acc: 0.9988 - f1: 0.5518 - val_loss: 0.2800 - val_acc: 0.9555 - val_f1: 0.2504\n",
      "\n",
      "Epoch 00025: val_f1 did not improve from 0.25512\n",
      "Epoch 26/100\n",
      "1166/1165 [==============================] - 534s 458ms/step - loss: 0.0040 - acc: 0.9988 - f1: 0.5522 - val_loss: 0.2650 - val_acc: 0.9538 - val_f1: 0.2516\n",
      "\n",
      "Epoch 00026: val_f1 did not improve from 0.25512\n",
      "Epoch 27/100\n",
      "1166/1165 [==============================] - 509s 437ms/step - loss: 0.0036 - acc: 0.9989 - f1: 0.5532 - val_loss: 0.2892 - val_acc: 0.9545 - val_f1: 0.2467\n",
      "\n",
      "Epoch 00027: val_f1 did not improve from 0.25512\n",
      "Epoch 28/100\n",
      "1166/1165 [==============================] - 535s 459ms/step - loss: 0.0035 - acc: 0.9990 - f1: 0.5526 - val_loss: 0.2772 - val_acc: 0.9571 - val_f1: 0.2544\n",
      "\n",
      "Epoch 00028: val_f1 did not improve from 0.25512\n",
      "Epoch 29/100\n",
      "1166/1165 [==============================] - 522s 447ms/step - loss: 0.0034 - acc: 0.9990 - f1: 0.5525 - val_loss: 0.2852 - val_acc: 0.9551 - val_f1: 0.2495\n",
      "\n",
      "Epoch 00029: val_f1 did not improve from 0.25512\n",
      "Epoch 30/100\n",
      "1166/1165 [==============================] - 590s 506ms/step - loss: 0.0035 - acc: 0.9989 - f1: 0.5540 - val_loss: 0.2705 - val_acc: 0.9542 - val_f1: 0.2545\n",
      "\n",
      "Epoch 00030: val_f1 did not improve from 0.25512\n",
      "Epoch 31/100\n",
      "1166/1165 [==============================] - 556s 477ms/step - loss: 0.0032 - acc: 0.9990 - f1: 0.5536 - val_loss: 0.2751 - val_acc: 0.9552 - val_f1: 0.2458\n",
      "\n",
      "Epoch 00031: val_f1 did not improve from 0.25512\n",
      "Epoch 32/100\n",
      "1166/1165 [==============================] - 514s 440ms/step - loss: 0.0031 - acc: 0.9990 - f1: 0.5539 - val_loss: 0.2789 - val_acc: 0.9550 - val_f1: 0.2499\n",
      "\n",
      "Epoch 00032: val_f1 did not improve from 0.25512\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1165 [==============================] - 597s 512ms/step - loss: 0.0019 - acc: 0.9995 - f1: 0.5555 - val_loss: 0.2803 - val_acc: 0.9573 - val_f1: 0.2590\n",
      "\n",
      "Epoch 00033: val_f1 improved from 0.25512 to 0.25899, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 34/100\n",
      "1166/1165 [==============================] - 527s 452ms/step - loss: 0.0011 - acc: 0.9997 - f1: 0.5586 - val_loss: 0.3091 - val_acc: 0.9569 - val_f1: 0.2572\n",
      "\n",
      "Epoch 00034: val_f1 did not improve from 0.25899\n",
      "Epoch 35/100\n",
      "1166/1165 [==============================] - 524s 449ms/step - loss: 0.0011 - acc: 0.9997 - f1: 0.5575 - val_loss: 0.3154 - val_acc: 0.9572 - val_f1: 0.2572\n",
      "\n",
      "Epoch 00035: val_f1 did not improve from 0.25899\n",
      "Epoch 36/100\n",
      "1166/1165 [==============================] - 563s 483ms/step - loss: 0.0012 - acc: 0.9997 - f1: 0.5583 - val_loss: 0.3158 - val_acc: 0.9570 - val_f1: 0.2565\n",
      "\n",
      "Epoch 00036: val_f1 did not improve from 0.25899\n",
      "Epoch 37/100\n",
      "1166/1165 [==============================] - 525s 450ms/step - loss: 0.0010 - acc: 0.9997 - f1: 0.5574 - val_loss: 0.3025 - val_acc: 0.9566 - val_f1: 0.2614\n",
      "\n",
      "Epoch 00037: val_f1 improved from 0.25899 to 0.26144, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 38/100\n",
      "1166/1165 [==============================] - 524s 450ms/step - loss: 9.2950e-04 - acc: 0.9997 - f1: 0.5589 - val_loss: 0.3211 - val_acc: 0.9567 - val_f1: 0.2529\n",
      "\n",
      "Epoch 00038: val_f1 did not improve from 0.26144\n",
      "Epoch 39/100\n",
      "1166/1165 [==============================] - 572s 490ms/step - loss: 0.0011 - acc: 0.9997 - f1: 0.5584 - val_loss: 0.3117 - val_acc: 0.9574 - val_f1: 0.2575\n",
      "\n",
      "Epoch 00039: val_f1 did not improve from 0.26144\n",
      "Epoch 40/100\n",
      "1166/1165 [==============================] - 537s 461ms/step - loss: 0.0010 - acc: 0.9997 - f1: 0.5576 - val_loss: 0.3218 - val_acc: 0.9570 - val_f1: 0.2530\n",
      "\n",
      "Epoch 00040: val_f1 did not improve from 0.26144\n",
      "Epoch 41/100\n",
      "1166/1165 [==============================] - 528s 453ms/step - loss: 0.0010 - acc: 0.9997 - f1: 0.5597 - val_loss: 0.3217 - val_acc: 0.9576 - val_f1: 0.2557\n",
      "\n",
      "Epoch 00041: val_f1 did not improve from 0.26144\n",
      "Epoch 42/100\n",
      "1166/1165 [==============================] - 561s 481ms/step - loss: 9.4133e-04 - acc: 0.9998 - f1: 0.5583 - val_loss: 0.3293 - val_acc: 0.9567 - val_f1: 0.2573\n",
      "\n",
      "Epoch 00042: val_f1 did not improve from 0.26144\n",
      "Epoch 43/100\n",
      "1166/1165 [==============================] - 547s 469ms/step - loss: 9.9223e-04 - acc: 0.9997 - f1: 0.5604 - val_loss: 0.3256 - val_acc: 0.9561 - val_f1: 0.2522\n",
      "\n",
      "Epoch 00043: val_f1 did not improve from 0.26144\n",
      "Epoch 44/100\n",
      "1166/1165 [==============================] - 537s 460ms/step - loss: 8.9704e-04 - acc: 0.9997 - f1: 0.5576 - val_loss: 0.3204 - val_acc: 0.9561 - val_f1: 0.2594\n",
      "\n",
      "Epoch 00044: val_f1 did not improve from 0.26144\n",
      "Epoch 45/100\n",
      "1166/1165 [==============================] - 547s 469ms/step - loss: 0.0011 - acc: 0.9997 - f1: 0.5577 - val_loss: 0.3178 - val_acc: 0.9563 - val_f1: 0.2549\n",
      "\n",
      "Epoch 00045: val_f1 did not improve from 0.26144\n",
      "Epoch 46/100\n",
      "1166/1165 [==============================] - 557s 477ms/step - loss: 9.8302e-04 - acc: 0.9997 - f1: 0.5579 - val_loss: 0.3168 - val_acc: 0.9569 - val_f1: 0.2595\n",
      "\n",
      "Epoch 00046: val_f1 did not improve from 0.26144\n",
      "Epoch 47/100\n",
      "1166/1165 [==============================] - 507s 435ms/step - loss: 8.7185e-04 - acc: 0.9998 - f1: 0.5588 - val_loss: 0.3178 - val_acc: 0.9553 - val_f1: 0.2552\n",
      "\n",
      "Epoch 00047: val_f1 did not improve from 0.26144\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 48/100\n",
      "1166/1165 [==============================] - 649s 557ms/step - loss: 5.7315e-04 - acc: 0.9998 - f1: 0.5594 - val_loss: 0.3234 - val_acc: 0.9574 - val_f1: 0.2624\n",
      "\n",
      "Epoch 00048: val_f1 improved from 0.26144 to 0.26237, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 49/100\n",
      "1166/1165 [==============================] - 551s 472ms/step - loss: 3.0474e-04 - acc: 0.9999 - f1: 0.5598 - val_loss: 0.3315 - val_acc: 0.9576 - val_f1: 0.2609\n",
      "\n",
      "Epoch 00049: val_f1 did not improve from 0.26237\n",
      "Epoch 50/100\n",
      "1166/1165 [==============================] - 512s 439ms/step - loss: 3.1645e-04 - acc: 0.9999 - f1: 0.5577 - val_loss: 0.3461 - val_acc: 0.9572 - val_f1: 0.2598\n",
      "\n",
      "Epoch 00050: val_f1 did not improve from 0.26237\n",
      "Epoch 51/100\n",
      "1166/1165 [==============================] - 985s 845ms/step - loss: 5.1106e-04 - acc: 0.9999 - f1: 0.5614 - val_loss: 0.3474 - val_acc: 0.9573 - val_f1: 0.2603\n",
      "\n",
      "Epoch 00051: val_f1 did not improve from 0.26237\n",
      "Epoch 52/100\n",
      "1166/1165 [==============================] - 521s 447ms/step - loss: 3.6763e-04 - acc: 0.9999 - f1: 0.5604 - val_loss: 0.3545 - val_acc: 0.9575 - val_f1: 0.2584\n",
      "\n",
      "Epoch 00052: val_f1 did not improve from 0.26237\n",
      "Epoch 53/100\n",
      "1166/1165 [==============================] - 542s 465ms/step - loss: 3.6096e-04 - acc: 0.9999 - f1: 0.5598 - val_loss: 0.3546 - val_acc: 0.9569 - val_f1: 0.2606\n",
      "\n",
      "Epoch 00053: val_f1 did not improve from 0.26237\n",
      "Epoch 54/100\n",
      "1166/1165 [==============================] - 542s 465ms/step - loss: 3.8222e-04 - acc: 0.9999 - f1: 0.5598 - val_loss: 0.3474 - val_acc: 0.9565 - val_f1: 0.2616\n",
      "\n",
      "Epoch 00054: val_f1 did not improve from 0.26237\n",
      "Epoch 55/100\n",
      "1166/1165 [==============================] - 534s 458ms/step - loss: 3.7793e-04 - acc: 0.9999 - f1: 0.5584 - val_loss: 0.3577 - val_acc: 0.9571 - val_f1: 0.2644\n",
      "\n",
      "Epoch 00055: val_f1 improved from 0.26237 to 0.26444, saving model to ../working/InceptionV3+branch.h5\n",
      "Epoch 56/100\n",
      "1166/1165 [==============================] - 527s 452ms/step - loss: 3.8283e-04 - acc: 0.9999 - f1: 0.5610 - val_loss: 0.3665 - val_acc: 0.9573 - val_f1: 0.2539\n",
      "\n",
      "Epoch 00056: val_f1 did not improve from 0.26444\n",
      "Epoch 57/100\n",
      "1166/1165 [==============================] - 539s 462ms/step - loss: 4.6441e-04 - acc: 0.9999 - f1: 0.5594 - val_loss: 0.3516 - val_acc: 0.9562 - val_f1: 0.2590\n",
      "\n",
      "Epoch 00057: val_f1 did not improve from 0.26444\n",
      "Epoch 58/100\n",
      "1166/1165 [==============================] - 560s 480ms/step - loss: 3.6501e-04 - acc: 0.9999 - f1: 0.5610 - val_loss: 0.3558 - val_acc: 0.9573 - val_f1: 0.2574\n",
      "\n",
      "Epoch 00058: val_f1 did not improve from 0.26444\n",
      "Epoch 59/100\n",
      "1166/1165 [==============================] - 545s 468ms/step - loss: 3.7256e-04 - acc: 0.9999 - f1: 0.5586 - val_loss: 0.3503 - val_acc: 0.9571 - val_f1: 0.2573\n",
      "\n",
      "Epoch 00059: val_f1 did not improve from 0.26444\n",
      "Epoch 60/100\n",
      "1166/1165 [==============================] - 562s 482ms/step - loss: 3.9016e-04 - acc: 0.9999 - f1: 0.5597 - val_loss: 0.3594 - val_acc: 0.9575 - val_f1: 0.2557\n",
      "\n",
      "Epoch 00060: val_f1 did not improve from 0.26444\n",
      "Epoch 61/100\n",
      "1166/1165 [==============================] - 532s 456ms/step - loss: 3.5146e-04 - acc: 0.9999 - f1: 0.5595 - val_loss: 0.3585 - val_acc: 0.9574 - val_f1: 0.2581\n",
      "\n",
      "Epoch 00061: val_f1 did not improve from 0.26444\n",
      "Epoch 62/100\n",
      "1166/1165 [==============================] - 539s 463ms/step - loss: 3.5153e-04 - acc: 0.9999 - f1: 0.5594 - val_loss: 0.3595 - val_acc: 0.9576 - val_f1: 0.2549\n",
      "\n",
      "Epoch 00062: val_f1 did not improve from 0.26444\n",
      "Epoch 63/100\n",
      "1166/1165 [==============================] - 558s 479ms/step - loss: 3.7492e-04 - acc: 0.9999 - f1: 0.5606 - val_loss: 0.3554 - val_acc: 0.9570 - val_f1: 0.2537\n",
      "\n",
      "Epoch 00063: val_f1 did not improve from 0.26444\n",
      "Epoch 64/100\n",
      "1166/1165 [==============================] - 566s 486ms/step - loss: 3.2361e-04 - acc: 0.9999 - f1: 0.5589 - val_loss: 0.3581 - val_acc: 0.9576 - val_f1: 0.2514\n",
      "\n",
      "Epoch 00064: val_f1 did not improve from 0.26444\n",
      "Epoch 65/100\n",
      "1166/1165 [==============================] - 506s 434ms/step - loss: 4.6085e-04 - acc: 0.9999 - f1: 0.5610 - val_loss: 0.3539 - val_acc: 0.9571 - val_f1: 0.2557\n",
      "\n",
      "Epoch 00065: val_f1 did not improve from 0.26444\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 66/100\n",
      "1166/1165 [==============================] - 708s 607ms/step - loss: 2.5042e-04 - acc: 1.0000 - f1: 0.5600 - val_loss: 0.3547 - val_acc: 0.9574 - val_f1: 0.2555\n",
      "\n",
      "Epoch 00066: val_f1 did not improve from 0.26444\n",
      "Epoch 67/100\n",
      "1166/1165 [==============================] - 546s 468ms/step - loss: 1.6082e-04 - acc: 1.0000 - f1: 0.5606 - val_loss: 0.3618 - val_acc: 0.9574 - val_f1: 0.2570\n",
      "\n",
      "Epoch 00067: val_f1 did not improve from 0.26444\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166/1165 [==============================] - 540s 463ms/step - loss: 1.2304e-04 - acc: 1.0000 - f1: 0.5586 - val_loss: 0.3647 - val_acc: 0.9575 - val_f1: 0.2566\n",
      "\n",
      "Epoch 00068: val_f1 did not improve from 0.26444\n",
      "Epoch 69/100\n",
      "1166/1165 [==============================] - 601s 516ms/step - loss: 8.8763e-05 - acc: 1.0000 - f1: 0.5609 - val_loss: 0.3674 - val_acc: 0.9575 - val_f1: 0.2558\n",
      "\n",
      "Epoch 00069: val_f1 did not improve from 0.26444\n",
      "Epoch 70/100\n",
      "1166/1165 [==============================] - 562s 482ms/step - loss: 1.0476e-04 - acc: 1.0000 - f1: 0.5602 - val_loss: 0.3748 - val_acc: 0.9579 - val_f1: 0.2600\n",
      "\n",
      "Epoch 00070: val_f1 did not improve from 0.26444\n",
      "Epoch 71/100\n",
      "1166/1165 [==============================] - 524s 450ms/step - loss: 1.2735e-04 - acc: 1.0000 - f1: 0.5612 - val_loss: 0.3717 - val_acc: 0.9571 - val_f1: 0.2612\n",
      "\n",
      "Epoch 00071: val_f1 did not improve from 0.26444\n",
      "Epoch 72/100\n",
      "1166/1165 [==============================] - 837s 718ms/step - loss: 1.2002e-04 - acc: 1.0000 - f1: 0.5592 - val_loss: 0.3739 - val_acc: 0.9572 - val_f1: 0.2602\n",
      "\n",
      "Epoch 00072: val_f1 did not improve from 0.26444\n",
      "Epoch 73/100\n",
      "1166/1165 [==============================] - 537s 461ms/step - loss: 9.9398e-05 - acc: 1.0000 - f1: 0.5612 - val_loss: 0.3786 - val_acc: 0.9575 - val_f1: 0.2550\n",
      "\n",
      "Epoch 00073: val_f1 did not improve from 0.26444\n",
      "Epoch 74/100\n",
      "1166/1165 [==============================] - 538s 462ms/step - loss: 9.6306e-05 - acc: 1.0000 - f1: 0.5599 - val_loss: 0.3718 - val_acc: 0.9571 - val_f1: 0.2571\n",
      "\n",
      "Epoch 00074: val_f1 did not improve from 0.26444\n",
      "Epoch 75/100\n",
      "1166/1165 [==============================] - 784s 672ms/step - loss: 9.1648e-05 - acc: 1.0000 - f1: 0.5600 - val_loss: 0.3736 - val_acc: 0.9576 - val_f1: 0.2623\n",
      "\n",
      "Epoch 00075: val_f1 did not improve from 0.26444\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n"
     ]
    }
   ],
   "source": [
    "# train all layers\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer)\n",
    "    layer.trainable = True\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            metrics=['accuracy', f1])\n",
    "batch_size = 12\n",
    "hist =  model.fit_generator(\n",
    "        tg,\n",
    "        steps_per_epoch=np.ceil(float(len(pathsTrain)) / float(batch_size))/2,\n",
    "        validation_data=vg,\n",
    "        validation_steps=np.ceil(float(len(pathsVal)) / float(batch_size))/2,\n",
    "        epochs=100, \n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=f1_loss,\n",
    "#             optimizer=Adam(lr=1e-4),\n",
    "#             metrics=['accuracy', f1])\n",
    "# hist =  model.fit_generator(\n",
    "#         tg,\n",
    "#         steps_per_epoch=np.ceil(float(len(pathsTrain)) / float(batch_size))/2,\n",
    "#         validation_data=vg,\n",
    "#         validation_steps=np.ceil(float(len(pathsVal)) / float(batch_size))/2,\n",
    "#         epochs=200, \n",
    "#         verbose=1,\n",
    "#         callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1e6b6814198>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].set_title('loss')\n",
    "ax[0].plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\n",
    "ax[0].plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax[1].set_title('acc')\n",
    "ax[1].plot(hist.epoch, hist.history[\"f1\"], label=\"Train F1\")\n",
    "ax[1].plot(hist.epoch, hist.history[\"val_f1\"], label=\"Validation F1\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "bestModel = load_model('../working/InceptionV3.h5', custom_objects={'f1': f1, 'f1_loss': f1_loss, 'focal_loss_fixed':focal_loss()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "lastFullValPred = np.empty((0, 28))\n",
    "lastFullValLabels = np.empty((0, 28))\n",
    "for i in tqdm(range(len(vg))): \n",
    "    im, lbl = vg[i]\n",
    "    scores = bestModel.predict(im)\n",
    "    lastFullValPred = np.append(lastFullValPred, scores, axis=0)\n",
    "    lastFullValLabels = np.append(lastFullValLabels, lbl, axis=0)\n",
    "print(lastFullValPred.shape, lastFullValLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score as off1\n",
    "rng = np.arange(0, 1, 0.001)\n",
    "f1s = np.zeros((rng.shape[0], 28))\n",
    "for j,t in enumerate(tqdm(rng)):\n",
    "    for i in range(28):\n",
    "        p = np.array(lastFullValPred[:,i]>t, dtype=np.int8)\n",
    "        scoref1 = off1(lastFullValLabels[:,i], p, average='binary')\n",
    "        f1s[j,i] = scoref1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Individual F1-scores for each class:')\n",
    "print(np.max(f1s, axis=0))\n",
    "print('Macro F1-score CV =', np.mean(np.max(f1s, axis=0)))\n",
    "plt.plot(rng, f1s)\n",
    "T = np.empty(28)\n",
    "for i in range(28):\n",
    "    T[i] = rng[np.where(f1s[:,i] == np.max(f1s[:,i]))[0][0]]\n",
    "print('Probability threshold maximizing CV F1-score for each class:')\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathsTest, labelsTest = getTestDataset()\n",
    "\n",
    "testg = ProteinDataGenerator(pathsTest, labelsTest, batch_size, SHAPE, channels)\n",
    "submit = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "P = np.zeros((pathsTest.shape[0], 28))\n",
    "for i in tqdm(range(len(testg))):\n",
    "    images, labels = testg[i]\n",
    "    score = bestModel.predict(images)\n",
    "    P[i*batch_size:i*batch_size+score.shape[0]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP = np.array(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "\n",
    "for row in tqdm(range(submit.shape[0])):\n",
    "    \n",
    "    str_label = ''\n",
    "    \n",
    "    for col in range(PP.shape[1]):\n",
    "        if(PP[row, col] < T[col]):\n",
    "            str_label += ''\n",
    "        else:\n",
    "            str_label += str(col) + ' '\n",
    "    prediction.append(str_label.strip())\n",
    "    \n",
    "submit['Predicted'] = np.array(prediction)\n",
    "submit.to_csv('transfer_1x1conv_aug_focal_loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# pathsTest, labelsTest = getTestDataset()\n",
    "\n",
    "# testg = ProteinDataGenerator(pathsTest, labelsTest, batch_size, SHAPE)\n",
    "# submit = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "# P = np.zeros((pathsTest.shape[0], 28))\n",
    "# for i in tqdm(range(len(testg))):\n",
    "#     images, labels = testg[i]\n",
    "#     score = bestModel.predict(images)\n",
    "#     P[i*batch_size:i*batch_size+score.shape[0]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PP = np.array(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = []\n",
    "\n",
    "# for row in tqdm(range(submit.shape[0])):\n",
    "    \n",
    "#     str_label = ''\n",
    "    \n",
    "#     for col in range(PP.shape[1]):\n",
    "#         if(PP[row, col] < .2):   # to account for losing TP is more costly than decreasing FP\n",
    "#             #print(PP[row])\n",
    "#             str_label += ''\n",
    "#         else:\n",
    "#             str_label += str(col) + ' '\n",
    "#     prediction.append(str_label.strip())\n",
    "    \n",
    "# submit['Predicted'] = np.array(prediction)\n",
    "# submit.to_csv('datagenerator_model_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
